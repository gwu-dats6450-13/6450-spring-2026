<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-bb1cd4028f63a369ab283e072cb7d572.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <title>DATS 6450.13 – Lecture 2</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #767676;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #767676;  padding-left: 4px; }
    div.sourceCode
      { color: #545454; background-color: #fefefe; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #545454; } /* Normal */
    code span.al { color: #7928a1; } /* Alert */
    code span.an { color: #696969; } /* Annotation */
    code span.at { color: #a55a00; } /* Attribute */
    code span.bn { color: #7928a1; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #d91e18; } /* ControlFlow */
    code span.ch { color: #008000; } /* Char */
    code span.cn { color: #d91e18; } /* Constant */
    code span.co { color: #696969; } /* Comment */
    code span.cv { color: #696969; font-style: italic; } /* CommentVar */
    code span.do { color: #696969; font-style: italic; } /* Documentation */
    code span.dt { color: #7928a1; } /* DataType */
    code span.dv { color: #7928a1; } /* DecVal */
    code span.er { color: #7928a1; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #a55a00; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #696969; } /* Information */
    code span.kw { color: #d91e18; } /* Keyword */
    code span.op { color: #00769e; } /* Operator */
    code span.ot { color: #d91e18; } /* Other */
    code span.pp { color: #7928a1; } /* Preprocessor */
    code span.sc { color: #00769e; } /* SpecialChar */
    code span.ss { color: #008000; } /* SpecialString */
    code span.st { color: #008000; } /* String */
    code span.va { color: #a55a00; } /* Variable */
    code span.vs { color: #008000; } /* VerbatimString */
    code span.wa { color: #696969; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-857c1e38c7f8756f363ea65e7ad620aa.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
  <link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Lecture 2</h1>
  <p class="subtitle">Parallelization</p>

<div class="quarto-title-authors">
</div>

</section>
<section id="agenda-and-goals-for-today" class="slide level2">
<h2>Agenda and Goals for Today</h2>
<ul>
<li>Scaling up and scaling out for AI data pipelines</li>
<li>Parallelization fundamentals for data processing</li>
<li>Map and Reduce functions</li>
<li>Parallel data preparation for AI/ML workloads</li>
<li>Distributed data processing frameworks (Spark, Ray, Dask)</li>
<li>Lab Preview: Parallelization with Python
<ul>
<li>Use the <code>multiprocessing</code> module</li>
<li>Implement synchronous and asynchronous processing</li>
<li>Parallel data preprocessing examples</li>
</ul></li>
<li>Homework Preview: Parallelization with Python
<ul>
<li>Parallel data processing</li>
<li>Building scalable data pipelines</li>
</ul></li>
</ul>
</section>
<section id="looking-back" class="slide level2">
<h2>Looking back</h2>
<ul>
<li>Due date reminders:
<ul>
<li>Assignment 2: January 30</li>
<li>Lab 3: January 30</li>
</ul></li>
</ul>
</section>
<section id="glossary" class="slide level2 smaller">
<h2>Glossary</h2>
<table class="caption-top">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Local</strong></td>
<td>Your current workstation (laptop, desktop, etc.), wherever you start the terminal/console application.</td>
</tr>
<tr class="even">
<td><strong>Remote</strong></td>
<td>Any machine you connect to via ssh or other means.</td>
</tr>
<tr class="odd">
<td><strong>EC2</strong></td>
<td>Single virtual machine in the cloud where you can run computation (ephemeral)</td>
</tr>
<tr class="even">
<td><strong>SageMaker</strong></td>
<td>Integrated Developer Environment where you can conduct data science on single machines or distributed training</td>
</tr>
<tr class="odd">
<td><strong>GPU</strong></td>
<td>Graphics Processing Unit - specialized hardware for parallel computation, essential for AI/ML</td>
</tr>
<tr class="even">
<td><strong>TPU</strong></td>
<td>Tensor Processing Unit - Google’s custom AI accelerator chips</td>
</tr>
<tr class="odd">
<td><strong>Ephemeral</strong></td>
<td>Lasting for a short time - any machine that will get turned off or place you will lose data</td>
</tr>
<tr class="even">
<td><strong>Persistent</strong></td>
<td>Lasting for a long time - any environment where your work is NOT lost when the timer goes off</td>
</tr>
</tbody>
</table>
</section>
<section>
<section id="parallelization" class="title-slide slide level1 section center">
<h1>Parallelization</h1>

</section>
<section id="typical-real-world-scenarios" class="slide level2">
<h2>Typical real world scenarios</h2>
<ul>
<li class="fragment">You need to <strong>prepare training data</strong> for LLMs by cleaning and deduplicating 100TB of web-scraped text</li>
<li class="fragment">You are building a <strong>RAG system</strong> that requires embedding and indexing millions of documents in parallel</li>
<li class="fragment">You need to <strong>extract structured data</strong> from millions of PDFs using vision models for document AI</li>
<li class="fragment">You are <strong>preprocessing multimodal datasets</strong> with billions of image-text pairs for foundation model training</li>
<li class="fragment">You need to run <strong>quality filtering</strong> on petabytes of Common Crawl data for training dataset curation</li>
<li class="fragment">You are <strong>generating synthetic training data</strong> using LLMs to augment limited real-world datasets</li>
<li class="fragment">You need to <strong>transform and tokenize</strong> text corpora across 100+ languages for multilingual AI</li>
<li class="fragment">You are building <strong>real-time data pipelines</strong> that process streaming data for online learning systems</li>
</ul>
</section>
<section id="data-for-ai-parallel-scenarios" class="slide level2">
<h2>Data-for-AI Parallel Scenarios</h2>
<ul>
<li class="fragment"><strong>Web-scale data processing</strong>: Filtering 45TB of Common Crawl data for high-quality training examples</li>
<li class="fragment"><strong>Embedding generation</strong>: Creating vector embeddings for 100M documents using sentence transformers</li>
<li class="fragment"><strong>Data deduplication</strong>: Finding near-duplicates in billion-scale datasets using MinHash and LSH</li>
<li class="fragment"><strong>Synthetic data generation</strong>: Using LLMs to generate millions of instruction-following examples</li>
<li class="fragment"><strong>Data quality assessment</strong>: Running parallel quality checks on training data (toxicity, bias, factuality)</li>
<li class="fragment"><strong>Feature extraction</strong>: Extracting features from millions of images, videos, or audio files for AI training</li>
</ul>
</section></section>
<section>
<section id="parallel-programming" class="title-slide slide level1 section center">
<h1>Parallel Programming</h1>

</section>
<section id="linear-vs.-parallel" class="slide level2">
<h2>Linear vs.&nbsp;Parallel</h2>
<div class="columns">
<div class="column" style="width:47%;">
<h3 id="linearsequential">Linear/Sequential</h3>
<ol type="1">
<li>A program starts to run</li>
<li>The program issues an instruction</li>
<li>The instruction is executed</li>
<li>Steps 2 and 3 are repeated</li>
<li>The program finishes running</li>
</ol>
</div><div class="column" style="width:47%;">
<h3 id="parallel">Parallel</h3>
<ol type="1">
<li>A program starts to run</li>
<li>The program divides up the work into chunks of instructions and data</li>
<li>Each chunk of work is executed independently</li>
<li>The chunks of work are reassembled</li>
<li>The program finishes running</li>
</ol>
</div></div>
</section>
<section id="linear-vs.-parallel-1" class="slide level2">
<h2>Linear vs.&nbsp;Parallel</h2>
<div class="columns">
<div class="column" style="width:47%;">
<p><img src="img/linear.png" width="400"></p>
</div><div class="column" style="width:47%;">
<p><img src="img/parallel.png" width="400"></p>
</div></div>
</section>
<section id="linear-vs.-parallel-2" class="slide level2">
<h2>Linear vs.&nbsp;Parallel</h2>
<p>From a data engineering for AI perspective</p>
<div class="columns">
<div class="column" style="width:47%;">
<h3 id="linear">Linear</h3>
<ul>
<li>The data remains monolithic</li>
<li>Procedures act on the data sequentially
<ul>
<li>Each procedure has to complete before the next procedure can start</li>
</ul></li>
<li>You can think of this as a single pipeline</li>
</ul>
</div><div class="column" style="width:47%;">
<h3 id="parallel-1">Parallel</h3>
<ul>
<li>The data can be split up into chunks</li>
<li>The same procedures can be run on each chunk at the same time</li>
<li>Or, independent procedures can run on different chunks at the same time</li>
<li>Need to bring things back together at the end</li>
</ul>
</div></div>
<div class="fragment">
<h3 id="what-are-some-examples-of-linear-and-parallel-data-science-workflows">What are some examples of linear and parallel data science workflows?</h3>
</div>
</section>
<section id="embarrasingly-parallel-in-data-for-ai" class="slide level2">
<h2>Embarrasingly Parallel in Data for AI</h2>
<p>It’s <strong>easy</strong> to speed things up when:</p>
<ul>
<li>Processing millions of documents independently for text extraction</li>
<li>Generating embeddings for each document in a corpus</li>
<li>Applying the same preprocessing to each image in a dataset</li>
<li>Running quality filters on individual data samples</li>
<li>Tokenizing text files independently</li>
<li>Converting file formats (PDF to text, audio to features)</li>
<li>Validating and cleaning individual records</li>
</ul>
<p>Just run <strong>multiple data processing tasks at the same time</strong></p>
</section>
<section id="modern-data-processing-tools-for-ai" class="slide level2">
<h2>Modern Data Processing Tools for AI:</h2>
<ul>
<li><strong>Apache Spark</strong>: Distributed data processing at scale</li>
<li><strong>Ray Data</strong>: Scalable data preprocessing for ML</li>
<li><strong>Dask</strong>: Parallel computing with Python APIs</li>
<li><strong>Polars</strong>: Fast DataFrame library with parallel execution</li>
</ul>
</section>
<section id="embarrasingly-parallel" class="slide level2">
<h2>Embarrasingly Parallel</h2>
<p>The concept is based on the old middle/high school math problem:</p>
<blockquote>
<p>If 5 people can shovel a parking lot in 6 hours, how long will it take 100 people to shovel the same parking lot?</p>
</blockquote>
<p>Basic idea is that many hands (cores/instances) make lighter (faster/more efficient) work of the same problem, as long as the effort can be split up appropriately into nearly equal parcels</p>

<aside><div>
<p>The classical answer to the problem is 18 minutes</p>
</div></aside></section>
<section id="embarassingly-parallel" class="slide level2">
<h2>Embarassingly parallel</h2>
<ul>
<li class="fragment">If you can truly split up your problem into multiple <strong>independent</strong> parts, then you can often get linear speedups with the number of parallel components (to a limit)
<ul>
<li class="fragment">The more cores you use and the more you parallelize, the more you incur communication overhead and decrease available RAM, so the speedup is almost certainly sub-linear, i.e.&nbsp;for a 4-core machine you’ll probably get a 3-4x speedup, but rarely a full 4x speedup<sup>1</sup></li>
</ul></li>
<li class="fragment">The question often is, which part of your problem is embarassingly parallel?</li>
<li class="fragment">Amdahl’s law (which we’ll see in a few slides) shows how parallelization can benefit overall if a large proportion of the problem is parallelizable</li>
<li class="fragment">It’s not all milk and honey. Setting up, programming, evaluating, debugging parallel computations requires better infrastructure and more expertise.</li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn1"><p>Gorelick &amp; Ozsvald, 2020. <em>High Performance Python</em>, O’Reilly</p></li></ol></aside></section></section>
<section>
<section id="when-might-parallel-programming-not-be-more-efficient" class="title-slide slide level1 section center">
<h1>When might parallel programming not be more efficient?</h1>

</section>
<section id="some-limitations-in-data-processing-for-ai" class="slide level2 smaller">
<h2>Some limitations in Data Processing for AI</h2>
<h4 id="you-can-get-speedups-by-parallelizing-data-operations-but">You can get speedups by parallelizing data operations, but</h4>
<ul>
<li><strong>Data skew</strong>: Uneven distribution of data can create bottlenecks (e.g., one partition has 90% of data)</li>
<li><strong>I/O bound operations</strong>: Reading/writing to storage can limit parallelization benefits</li>
<li><strong>Memory overhead</strong>: Loading large models (like BERT) on each worker for feature extraction</li>
<li><strong>Deduplication challenges</strong>: Some operations like global deduplication require data shuffling</li>
<li><strong>Ordering dependencies</strong>: Maintaining sequence order in time-series or conversational data</li>
</ul>
<div class="fragment">
<h4 id="data-specific-challenges">Data-Specific Challenges</h4>
<ul>
<li><strong>Data quality issues</strong>: Bad data in parallel pipelines can be hard to trace</li>
<li><strong>Resource management</strong>: Large models (BERT, GPT) need significant memory per worker</li>
<li><strong>Consistency</strong>: Ensuring all workers use same preprocessing versions</li>
</ul>
</div>
<div class="fragment">
<h4 id="making-sure-that-we-can-get-back-all-the-pieces-needs-monitoring">Making sure that we can get back all the pieces needs monitoring</h4>
<ul>
<li>Failure tolerance and protections (Spark checkpointing)</li>
<li>Proper collection and aggregation of the processed data</li>
<li>Data validation after parallel processing</li>
</ul>
</div>
</section>
<section id="amdahls-law-in-data-processing" class="slide level2">
<h2>Amdahl’s Law in Data Processing</h2>
<div class="column" style="width:50%;">
<div class="imgcenter">
<p><a href="img/AmdahlsLaw.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img data-src="img/AmdahlsLaw.svg"></a></p>
</div>
</div><div class="column" style="width:45%;">
<p><span class="math display">\[
\lim_{s\rightarrow\infty} S_{latency} = \frac{1}{1-p}
\]</span></p>
<p><strong>Data Pipeline Examples:</strong></p>
<ul>
<li>If 80% is parallel (processing) and 20% is sequential (I/O), max speedup = 5x</li>
<li>If 95% is parallel (embedding generation), max speedup = 20x</li>
</ul>
</div><div class="fragment">
<h3 id="real-data-processing-speedups">Real Data Processing Speedups:</h3>
<ul>
<li><strong>Document processing</strong>: Near-linear up to 100s of workers</li>
<li><strong>Embedding generation</strong>: Linear with number of GPUs</li>
<li><strong>Data deduplication</strong>: Sub-linear due to shuffling overhead</li>
</ul>
</div>
<!-- ::: {.aside}
By <a href="https://en.wikipedia.org/wiki/User:Daniels220" class="extiw" title="wikipedia:User:Daniels220">Daniels220</a> at <a href="https://en.wikipedia.org/wiki/" class="extiw" title="wikipedia:">English Wikipedia</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=6678551">Link</a>
::: -->
</section>
<section id="pros-and-cons-of-parallelization-for-ai-data" class="slide level2 smaller">
<h2>Pros and cons of parallelization for AI Data</h2>
<div class="column" style="width:47%;">
<h3 id="yes---parallelize-these">Yes - Parallelize These</h3>
<p><strong>Data Preparation:</strong></p>
<ul>
<li>Text extraction from documents (PDFs, HTML)</li>
<li>Tokenization of text corpora</li>
<li>Image preprocessing and augmentation</li>
<li>Embedding generation for documents</li>
<li>Data quality filtering and validation</li>
<li>Format conversions (audio → features)</li>
<li>Web scraping and data collection</li>
<li>Synthetic data generation</li>
</ul>
<p><strong>Data Processing:</strong></p>
<ul>
<li>Batch inference on datasets</li>
<li>Feature extraction at scale</li>
<li>Data deduplication (local)</li>
<li>Parallel chunk processing</li>
</ul>
</div><div class="column" style="width:47%;">
<div class="fragment">
<h3 id="no---keep-sequential">No - Keep Sequential</h3>
<p><strong>Order-Dependent:</strong></p>
<ul>
<li>Conversation threading</li>
<li>Time-series preprocessing</li>
<li>Sequential data validation</li>
<li>Cumulative statistics</li>
</ul>
<p><strong>Global Operations:</strong></p>
<ul>
<li>Global deduplication</li>
<li>Cross-dataset joins</li>
<li>Sorting entire datasets</li>
<li>Computing exact quantiles</li>
</ul>
<p><strong>Small Data:</strong></p>
<ul>
<li>Config file processing</li>
<li>Metadata operations</li>
<li>Single document processing</li>
</ul>
</div>
</div><div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>For data operations in the “No” column, they often require global coordination or maintain strict ordering. However, many can be approximated with parallel algorithms (like approximate deduplication with locality-sensitive hashing )</p>
</div>
</div>
</div>
</div>
</section>
<section id="pros-and-cons-of-parallelization" class="slide level2 smaller">
<h2>Pros and cons of parallelization</h2>
<div class="columns">
<div class="column" style="width:47%;">
<h3 id="pros">Pros</h3>
<ul>
<li><p>Higher efficiency</p></li>
<li><p>Using modern infrastructure</p></li>
<li><p>Scalable to larger data, more complex procedures</p>
<ul>
<li><em>proviso</em> procedures are embarassingly parallel</li>
</ul></li>
</ul>
</div><div class="column right" style="width:47%;">
<div class="fragment">
<h3 id="cons">Cons</h3>
<ul>
<li>Higher programming complexity</li>
<li>Need proper software infrastructure (MPI, Hadoop, etc)</li>
<li>Need to ensure right packages/modules are distributed across processors</li>
<li>Need to account for a proportion of jobs failing, and recovering from them</li>
<li>Hence, Hadoop/Spark and other technologies</li>
<li>Higher setup cost in terms of time/expertise/money</li>
</ul>
</div>
</div></div>
<div class="fragment">
<p>There are good solutions today for most of the cons, so the pros have it and so this paradigm is widely accepted and implemented</p>
</div>
</section></section>
<section>
<section id="parallel-programming-models" class="title-slide slide level1 section center">
<h1>Parallel Programming Models</h1>

</section>
<section id="distributed-memory-message-passing-model" class="slide level2">
<h2>Distributed memory / Message Passing Model</h2>
<div style="text-align: center;">
<p><img src="parallelization-python/img/parallel1.png" width="600"></p>
</div>
</section>
<section id="data-parallel-model" class="slide level2">
<h2>Data parallel model</h2>
<div style="text-align: center;">
<p><img src="parallelization-python/img/parallel2.png" width="600"></p>
</div>
</section>
<section id="hybrid-model" class="slide level2">
<h2>Hybrid model</h2>
<div style="text-align: center;">
<p><img src="parallelization-python/img/parallel4.png" width="500"></p>
</div>
<div style="text-align: center;">
<p><img src="parallelization-python/img/parallel3.png" width="500"></p>
</div>
</section>
<section id="partitioning-data" class="slide level2">
<h2>Partitioning data</h2>
<div style="text-align: center;">
<p><img src="parallelization-python/img/parallel5.png" width="600"></p>
</div>
</section>
<section id="designing-parallel-programs" class="slide level2">
<h2>Designing parallel programs</h2>
<ul>
<li>Data partitioning</li>
<li>Communication</li>
<li>Synchronization / Orchestration</li>
<li>Data dependencies</li>
<li>Load balancing</li>
<li>Input and Output (I/O)</li>
<li>Debugging</li>
</ul>
<div class="fragment">
<p>A lot of these components are data engineering and DevOps issues</p>
</div>
<div class="fragment">
<p>Infrastructures have standardized many of these and have helped data scientists implement parallel programming much more easily</p>
</div>
<div class="fragment">
<p>We’ll see in the lab how the <code>multiprocessing</code> module in Python makes parallel processing on a machine quite easy to implement</p>
</div>
</section>
<section id="parallel-data-processing-for-ai" class="slide level2">
<h2>Parallel Data Processing for AI</h2>
<h3 id="common-ai-data-workloads">Common AI Data Workloads</h3>
<ul>
<li><strong>Text preprocessing</strong>: Tokenization, cleaning for LLM training</li>
<li><strong>Embedding generation</strong>: Converting documents to vectors</li>
<li><strong>Data quality filtering</strong>: Removing low-quality training samples</li>
<li><strong>Format conversion</strong>: PDF to text, audio to features</li>
<li><strong>Synthetic data generation</strong>: Using LLMs to create training data</li>
<li><strong>Deduplication</strong>: Finding and removing duplicate content</li>
</ul>
<div class="fragment">
<p>These operations are <strong>embarrassingly parallel</strong> - each document/sample can be processed independently</p>
</div>
<!-- {{< include parallelization-python/_module-parallel-computing.qmd >}}  -->
</section></section>
<section id="functional-programming" class="title-slide slide level1 section center">
<h1>Functional Programming</h1>

</section>

<section>
<section id="map-and-reduce" class="title-slide slide level1 center">
<h1>Map and Reduce</h1>

</section>
<section id="section" class="slide level2" data-background-image="parallelization-python/img/parallel-time.png" data-background-size="contain">
<h2></h2>
</section>
<section id="components-of-a-parallel-programming-workflow" class="slide level2">
<h2>Components of a parallel programming workflow</h2>
<ol type="1">
<li>Divide the work into chunks</li>
<li>Work on each chunk separately</li>
<li>Reassemble the work</li>
</ol>
<p>This paradigm is often referred to as a <strong>map-reduce framework</strong>, or, more descriptively, the <strong>split-apply-combine</strong> paradigm</p>
</section>
<section id="section-1" class="slide level2" data-background-image="parallelization-python/img/split-apply-combine.png" data-background-size="contain">
<h2></h2>
</section>
<section id="section-2" class="slide level2" data-background-image="parallelization-python/img/split-apply-combine-2.png" data-background-size="contain">
<h2></h2>
</section></section>
<section>
<section id="map" class="title-slide slide level1 section center">
<h1>Map</h1>

</section>
<section id="map-1" class="slide level2">
<h2>Map</h2>
<p>The <strong>map</strong> operation is a 1-1 operation that takes each split and processes it</p>
<p>The <strong>map</strong> operation keeps the same number of objects in its output that were present in its input</p>
<div style="text-align: center; width: 50%; height: 50%">
<p><a href="parallelization-python/img/map1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img data-src="parallelization-python/img/map1.png"></a></p>
</div>
</section>
<section id="map-2" class="slide level2">
<h2>Map</h2>
<div style="text-align: center; width: 80%, height: 80%">
<p><a href="parallelization-python/img/map2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img data-src="parallelization-python/img/map2.png"></a></p>
</div>
<p>The operations included in a particular <strong>map</strong> can be quite complex, involving multiple steps. In fact, you can implement a <em>pipeline</em> of procedures within the <strong>map</strong> step to process each data object.</p>
<p>The main point is that the <em>same</em> operations will be run on each data object in the <strong>map</strong> implementation</p>
</section>
<section id="map-3" class="slide level2">
<h2>Map</h2>
<p>Some examples of <strong>map</strong> operations are:</p>
<p><strong>Traditional Data Analytics:</strong></p>
<ol type="1">
<li>Extracting a standard table from online reports from multiple years</li>
<li>Extracting particular records from multiple JSON objects</li>
<li>Transforming data (as opposed to summarizing it)</li>
<li>Run a normalization script on each transcript in a GWAS dataset</li>
<li>Standardizing demographic data for each of the last 20 years against the 2000 US population</li>
</ol>
</section>
<section id="map-4" class="slide level2">
<h2>Map</h2>
<p>Some examples of <strong>map</strong> operations are:</p>
<p><strong>AI Data Processing (2025):</strong></p>
<ol type="1">
<li><strong>Text processing</strong>: Tokenizing millions of documents for LLM training</li>
<li><strong>Embedding generation</strong>: Converting each document to a vector representation</li>
<li><strong>Data extraction</strong>: Extracting text from millions of PDFs using OCR</li>
<li><strong>Quality filtering</strong>: Applying toxicity/bias filters to each text sample</li>
<li><strong>Image preprocessing</strong>: Resizing and normalizing images for vision models</li>
<li><strong>Synthetic data</strong>: Generating training examples from prompts using LLMs</li>
</ol>
</section>
<section id="map-5" class="slide level2">
<h2>Map</h2>
<div class="imgcenter imghalf">
<p><a href="parallelization-python/img/map3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img data-src="parallelization-python/img/map3.png"></a></p>
</div>
</section></section>
<section>
<section id="reduce" class="title-slide slide level1 section center">
<h1>Reduce</h1>

</section>
<section id="reduce-1" class="slide level2">
<h2>Reduce</h2>
<p>The <strong>reduce</strong> operation takes multiple objects and <em>reduces</em> them to a (perhaps) smaller number of objects using transformations that aren’t amenable to the <strong>map</strong> paradigm.</p>
<p>These transformations are often serial/linear in nature</p>
<p>The <strong>reduce</strong> transformation is usually the last, not-so-elegant transformation needed after most of the other transformations have been efficiently handled in a parallel fashion by <strong>map</strong></p>
<div class="imgcenter imghalf">
<p><a href="parallelization-python/img/reduce2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img data-src="parallelization-python/img/reduce2.png"></a></p>
</div>
</section>
<section id="reduce-2" class="slide level2">
<h2>Reduce</h2>
<p>The <strong>reduce</strong> operation requires</p>
<ol type="a">
<li>
An <i>accumulator</i> function, that will update serially as new data is fed into it
</li><li>
A sequence of objects to run through the accumulator function
</li><li>
A starting value from which the accumulator function starts
</li></ol>
<p>Programmatically, this can be written as</p>
<div class="imgcenter" style="width: 80%; height:80%;">
<p><a href="parallelization-python/img/reduce3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img data-src="parallelization-python/img/reduce3.png"></a></p>
</div>
</section>
<section id="reduce-3" class="slide level2">
<h2>Reduce</h2>
<p>The <strong>reduce</strong> operation works serially from “left” to “right”, passing each object successively through the accumulator function.</p>
<p>For example, if we were to add successive numbers with a function called <code>add</code>…</p>
<div class="imgcenter" style="width: 80%; height: 80%;">
<p><a href="parallelization-python/img/reduce4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img data-src="parallelization-python/img/reduce4.png"></a></p>
</div>
</section>
<section id="reduce-4" class="slide level2">
<h2>Reduce</h2>
<p>Some examples:</p>
<p><strong>Traditional Analytics:</strong></p>
<ol type="1">
<li>Finding the common elements (<em>intersection</em>) of a large number of sets</li>
<li>Computing a table of group-wise summaries</li>
<li>Filtering</li>
<li>Tabulating</li>
</ol>
<p><strong>AI Data Processing (2025):</strong></p>
<ol type="1">
<li><strong>Deduplication</strong>: Finding unique documents across billions of samples</li>
<li><strong>Aggregating statistics</strong>: Computing dataset quality metrics</li>
<li><strong>Vocabulary building</strong>: Creating token vocabularies from text corpus</li>
<li><strong>Index building</strong>: Merging vector embeddings into searchable indices</li>
<li><strong>Data validation</strong>: Aggregating quality scores across partitions</li>
</ol>
</section></section>
<section>
<section id="map-reduce" class="title-slide slide level1 section center">
<h1>Map &amp; Reduce</h1>

</section>
<section id="map-reduce-1" class="slide level2">
<h2>map-reduce</h2>
<p>Combining the <strong>map</strong> and <strong>reduce</strong> operations creates a powerful pipeline that can handle a diverse range of problems in the Big Data context</p>
<div class="imgcenter imghalf">
<p><a href="parallelization-python/img/mr1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img data-src="parallelization-python/img/mr1.png"></a></p>
</div>
</section></section>
<section>
<section id="parallelization-and-map-reduce" class="title-slide slide level1 section center">
<h1>Parallelization and map-reduce</h1>

</section>
<section id="parallelization-and-map-reduce-work-hand-in-hand" class="slide level2">
<h2>Parallelization and map-reduce work hand-in-hand</h2>
<div class="imgcenter" style="width: 80%; height: 80%;">
<p><a href="parallelization-python/img/pmr1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img data-src="parallelization-python/img/pmr1.png"></a></p>
</div>
<div class="fragment">
<p>One of the issues here is, how to split the data in a “good” manner so that the map-reduce framework works well</p>
</div>
</section></section>
<section>
<section id="the-python-multiprocessing-module" class="title-slide slide level1 section center">
<h1>The Python <code>multiprocessing</code> module</h1>

</section>
<section id="the-multiprocessing-module" class="slide level2">
<h2>The <code>multiprocessing</code> module</h2>
<ul>
<li>Focused on single-machine multicore parallelism</li>
<li>Facilitates:
<ul>
<li>process- and thread-based parallel processing</li>
<li>sharing work over queues</li>
<li>sharing data among processes</li>
</ul></li>
</ul>
</section>
<section id="processes-and-threads" class="slide level2">
<h2>Processes and threads</h2>
<ul>
<li class="fragment">A <strong>process</strong> is an executing program, that is self-contained and has dedicated runtime and memory</li>
<li class="fragment">A <strong>thread</strong> is the basic unit to which the operating system allocates processor time. It is an entity <em>within a process</em>. A thread can execute any part of the process code, including parts currently being executed by another thread.</li>
<li class="fragment">A thread will often be faster to spin up and terminate than a full process</li>
<li class="fragment">Threads can share memory and data with each other</li>
</ul>
<div class="fragment">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Python has the <strong>Global Interpretor Lock (GIL)</strong> which only allows only one thread to interact with Python objects at a time. So the way to parallel process in Python is to do <strong>multi-processor</strong> parallelization, where we run multiple Python interpretors across multiple processes, each with its own private memory space and GIL.</p>
</div>
</div>
</div>
</div>
</section>
<section id="some-concepts-in-multiprocessing" class="slide level2">
<h2>Some concepts in <code>multiprocessing</code><sup>1</sup></h2>
<h3 id="process">Process</h3>
<p>A forked copy of the current process; this creates a new process identifier, and the task runs as an independent child process in the operating system</p>
<h3 id="pool">Pool</h3>
<p>Wraps the <code>Process</code> into a convenient pool of workers that share a chunk of work and return an aggregated result</p>
<aside><ol class="aside-footnotes"><li id="fn2"><p>Gorelick &amp; Ozsvald, 2020. <em>High Performance Python</em>, O’Reilly</p></li></ol></aside></section>
<section id="other-methods-of-parallel-processing-in-python" class="slide level2">
<h2>Other methods of parallel processing in Python</h2>
<ul>
<li>The <code>joblib</code> module</li>
<li>Most <code>scikit-learn</code> functions have implicit parallelization baked in through the <code>n_jobs</code> parameter</li>
<li><strong>Ray</strong> for distributed AI/ML workloads</li>
<li><strong>Dask</strong> for scaling pandas/numpy operations</li>
<li><strong>Apache Beam</strong> for data pipelines</li>
</ul>
<p>For example</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">from</span> sklearn.ensembles <span class="im">import</span> RandomForestClassifier</span>
<span id="cb1-2"><a></a>clf <span class="op">=</span> RandomForestClassifier(n_estimators <span class="op">=</span> <span class="dv">100</span>, random_state <span class="op">=</span> <span class="dv">124</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>uses the <code>joblib</code> module to use all available processors (<code>n_jobs=-1</code>) to do the bootstrapping</p>

<aside><div>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>See <a href="https://scikit-learn.org/stable/computing/parallelism.html">here</a> for a description of parallel processing in the scikit-learn module</p>
</div>
</div>
</div>
</div></aside></section>
<section id="ai-data-processing-example" class="slide level2">
<h2>AI Data Processing Example</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="im">from</span> multiprocessing <span class="im">import</span> Pool</span>
<span id="cb2-2"><a></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb2-3"><a></a></span>
<span id="cb2-4"><a></a><span class="co"># Parallel tokenization for LLM training</span></span>
<span id="cb2-5"><a></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span>
<span id="cb2-6"><a></a></span>
<span id="cb2-7"><a></a><span class="kw">def</span> process_document(text):</span>
<span id="cb2-8"><a></a>    <span class="co"># Clean and tokenize text</span></span>
<span id="cb2-9"><a></a>    tokens <span class="op">=</span> tokenizer(text, truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb2-10"><a></a>    <span class="cf">return</span> tokens</span>
<span id="cb2-11"><a></a></span>
<span id="cb2-12"><a></a><span class="co"># Process millions of documents in parallel</span></span>
<span id="cb2-13"><a></a><span class="cf">with</span> Pool(processes<span class="op">=</span><span class="dv">32</span>) <span class="im">as</span> pool:</span>
<span id="cb2-14"><a></a>    tokenized_docs <span class="op">=</span> pool.<span class="bu">map</span>(process_document, documents)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section></section>
<section>
<section id="async-io-in-python" class="title-slide slide level1 section center">
<h1>Async I/O in Python</h1>

</section>
<section id="the-need-for-async-io-in-ai-data-processing" class="slide level2">
<h2>The need for <code>Async I/O</code> in AI Data Processing</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li class="fragment"><p>When talking to external systems (databases, APIs, LLM services) the bottleneck is not local CPU/memory but rather the time it takes to receive a response from the external system.</p></li>
<li class="fragment"><p>The Async I/O model addresses this by allowing to send multiple request in parallel without having to wait for a response.</p></li>
<li class="fragment"><p><strong>AI Use Cases</strong>:</p>
<ul>
<li class="fragment">Calling LLM APIs (OpenAI, Anthropic, Bedrock)</li>
<li class="fragment">Fetching data from multiple sources</li>
<li class="fragment">Parallel web scraping for training data</li>
<li class="fragment">Distributed vector database queries</li>
</ul></li>
<li class="fragment"><p>References: <a href="https://docs.python.org/3/library/asyncio.html">asyncio — Asynchronous I/O</a>, <a href="https://realpython.com/async-io-python/">Async IO in Python: A Complete Walkthrough</a></p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="parallelization-python/img/async_io.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Async I/O"><img data-src="parallelization-python/img/async_io.svg" alt="Async I/O"></a></p>
<figcaption><a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/asyncio/"><code>Async I/O</code></a></figcaption>
</figure>
</div>
</div></div>
</section>
<section id="concurrency-and-parallelism-in-python3" class="slide level2">
<h2>Concurrency and parallelism in Python3</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li class="fragment"><p><strong>Parallelism</strong>: multiple tasks are running in parallel, each on a different processors. This is done through the <code>multiprocessing</code> module.</p></li>
<li class="fragment"><p><strong>Concurrency</strong>: multiple tasks are taking turns to run on the same processor. Another task can be scheduled while the current one is blocked on I/O.</p></li>
<li class="fragment"><p><strong>Threading</strong>: multiple threads take turns executing tasks. One process can contain multiple threads. Similar to concurrency but within the context of a single process.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="parallelization-python/img/concurrency-python.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Concurrency in Python"><img data-src="parallelization-python/img/concurrency-python.png" alt="Concurrency in Python"></a></p>
<figcaption><a href="https://realpython.com/async-io-python/"><code>Concurrency in Python</code></a></figcaption>
</figure>
</div>
</div></div>
</section>
<section id="how-to-implement-concurrency-with-asyncio" class="slide level2">
<h2>How to implement concurrency with <code>asyncio</code></h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li class="fragment"><p><code>asyncio</code> is a library to write concurrent code using the async/await syntax.</p></li>
<li class="fragment"><p>Use of <code>async</code> and <code>await</code> keywords. You can call an <code>async</code> function multiple times while you <code>await</code> the result of a previous invocation.</p></li>
<li class="fragment"><p><code>await</code> the result of multiple <code>async</code> tasks using <code>gather</code>.</p></li>
<li class="fragment"><p>The <code>main</code> function in this example is called a <code>coroutine</code>. Multiple <code>coroutines</code> can be run <code>concurrnetly</code> as awaitable <code>tasks</code>.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="im">import</span> asyncio</span>
<span id="cb3-2"><a></a></span>
<span id="cb3-3"><a></a><span class="cf">async</span> <span class="kw">def</span> main():</span>
<span id="cb3-4"><a></a>    <span class="bu">print</span>(<span class="st">'Hello ...'</span>)</span>
<span id="cb3-5"><a></a>    <span class="cf">await</span> asyncio.sleep(<span class="dv">1</span>)</span>
<span id="cb3-6"><a></a>    <span class="bu">print</span>(<span class="st">'... World!'</span>)</span>
<span id="cb3-7"><a></a></span>
<span id="cb3-8"><a></a>asyncio.run(main())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div></div>
</section>
<section id="anatomy-of-an-asyncio-python-program" class="slide level2 smaller">
<h2>Anatomy of an <code>asyncio</code> Python program</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li class="fragment"><p>Write a regular Python function that makes a call to a database, an API or any other blocking functionality as you normally would.</p></li>
<li class="fragment"><p>Create a coroutine i.e.&nbsp;an <code>async</code> wrapper function to the blocking function using <code>async</code> and <code>await</code>, with the call to blocking function made using the <code>asyncio.to_thread</code> function. This enables the coroutine execution in a separate thread.</p></li>
<li class="fragment"><p>Create another coroutine that makes multiple calls (in a loop, list comprehension) to the <code>async</code> wrapper created in the previous step and awaits completion of all of the invocations using the <code>asyncio.gather</code> function.</p></li>
<li class="fragment"><p>Call the coroutine created in the previous step from another function using the <code>asyncio.run</code> function.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="im">import</span> time</span>
<span id="cb4-2"><a></a><span class="im">import</span> asyncio</span>
<span id="cb4-3"><a></a></span>
<span id="cb4-4"><a></a><span class="kw">def</span> my_blocking_func(i):</span>
<span id="cb4-5"><a></a>    <span class="co"># some blocking code such as an api call</span></span>
<span id="cb4-6"><a></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, entry"</span>)</span>
<span id="cb4-7"><a></a>    time.sleep(<span class="dv">1</span>)</span>
<span id="cb4-8"><a></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, exiting"</span>)</span>
<span id="cb4-9"><a></a>    <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb4-10"><a></a>  </span>
<span id="cb4-11"><a></a><span class="cf">async</span> <span class="kw">def</span> async_my_blocking_func(i: <span class="bu">int</span>):</span>
<span id="cb4-12"><a></a>    <span class="cf">return</span> <span class="cf">await</span> asyncio.to_thread(my_blocking_func, i)</span>
<span id="cb4-13"><a></a></span>
<span id="cb4-14"><a></a><span class="cf">async</span> <span class="kw">def</span> async_my_blocking_func_for_multiple(n: <span class="bu">int</span>):</span>
<span id="cb4-15"><a></a>    <span class="cf">return</span> <span class="cf">await</span> asyncio.gather(<span class="op">*</span>[async_my_blocking_func(i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)])</span>
<span id="cb4-16"><a></a></span>
<span id="cb4-17"><a></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb4-18"><a></a>    <span class="co"># async version</span></span>
<span id="cb4-19"><a></a>    s <span class="op">=</span> time.perf_counter()</span>
<span id="cb4-20"><a></a>    n <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb4-21"><a></a>    brewery_counts <span class="op">=</span> asyncio.run(async_my_blocking_func_for_multiple(n))</span>
<span id="cb4-22"><a></a>    elapsed_async <span class="op">=</span> time.perf_counter() <span class="op">-</span> s</span>
<span id="cb4-23"><a></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="va">__file__</span><span class="sc">}</span><span class="ss">, async_my_blocking_func_for_multiple finished in </span><span class="sc">{</span>elapsed_async<span class="sc">:0.2f}</span><span class="ss"> seconds"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div></div>
</section>
<section id="ai-example-parallel-llm-api-calls" class="slide level2">
<h2>AI Example: Parallel LLM API Calls</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a><span class="im">import</span> asyncio</span>
<span id="cb5-2"><a></a><span class="im">import</span> aiohttp</span>
<span id="cb5-3"><a></a></span>
<span id="cb5-4"><a></a><span class="cf">async</span> <span class="kw">def</span> call_llm_api(prompt, session):</span>
<span id="cb5-5"><a></a>    <span class="co">"""Make async call to LLM API"""</span></span>
<span id="cb5-6"><a></a>    <span class="cf">async</span> <span class="cf">with</span> session.post(</span>
<span id="cb5-7"><a></a>        <span class="st">"https://api.example.com/generate"</span>,</span>
<span id="cb5-8"><a></a>        json<span class="op">=</span>{<span class="st">"prompt"</span>: prompt, <span class="st">"max_tokens"</span>: <span class="dv">100</span>}</span>
<span id="cb5-9"><a></a>    ) <span class="im">as</span> response:</span>
<span id="cb5-10"><a></a>        <span class="cf">return</span> <span class="cf">await</span> response.json()</span>
<span id="cb5-11"><a></a></span>
<span id="cb5-12"><a></a><span class="cf">async</span> <span class="kw">def</span> process_prompts(prompts):</span>
<span id="cb5-13"><a></a>    <span class="co">"""Process multiple prompts in parallel"""</span></span>
<span id="cb5-14"><a></a>    <span class="cf">async</span> <span class="cf">with</span> aiohttp.ClientSession() <span class="im">as</span> session:</span>
<span id="cb5-15"><a></a>        tasks <span class="op">=</span> [call_llm_api(prompt, session) <span class="cf">for</span> prompt <span class="kw">in</span> prompts]</span>
<span id="cb5-16"><a></a>        results <span class="op">=</span> <span class="cf">await</span> asyncio.gather(<span class="op">*</span>tasks)</span>
<span id="cb5-17"><a></a>    <span class="cf">return</span> results</span>
<span id="cb5-18"><a></a></span>
<span id="cb5-19"><a></a><span class="co"># Generate synthetic data using parallel API calls</span></span>
<span id="cb5-20"><a></a>prompts <span class="op">=</span> [<span class="st">"Generate a customer review for..."</span>, </span>
<span id="cb5-21"><a></a>           <span class="st">"Create technical documentation for..."</span>,</span>
<span id="cb5-22"><a></a>           <span class="st">"Write a dialogue about..."</span>] <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb5-23"><a></a></span>
<span id="cb5-24"><a></a>results <span class="op">=</span> asyncio.run(process_prompts(prompts))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>This approach can speed up synthetic data generation by 10-100x compared to sequential API calls</p>
</div>
</div>
</div>
</section></section>
<section>
<section id="data-engineering-for-ai-at-scale" class="title-slide slide level1 section center">
<h1>Data Engineering for AI at Scale</h1>

</section>
<section id="the-ai-data-challenge" class="slide level2">
<h2>The AI Data Challenge</h2>
<ul>
<li class="fragment"><strong>Volume</strong>: LLMs trained on trillions of tokens (petabytes of text)</li>
<li class="fragment"><strong>Variety</strong>: Multimodal data (text, images, audio, video, structured data)</li>
<li class="fragment"><strong>Velocity</strong>: Real-time data pipelines for online learning</li>
<li class="fragment"><strong>Veracity</strong>: Data quality is crucial for model performance</li>
<li class="fragment"><strong>Value</strong>: High-quality data is more important than model architecture</li>
</ul>
<div class="fragment">
<blockquote>
<p>“Data is the new oil, but it needs refining” - especially for AI</p>
</blockquote>
</div>
</section>
<section id="parallel-data-processing-pipeline-for-ai" class="slide level2">
<h2>Parallel Data Processing Pipeline for AI</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a><span class="co"># Example: Parallel document processing for RAG system</span></span>
<span id="cb6-2"><a></a><span class="im">from</span> multiprocessing <span class="im">import</span> Pool</span>
<span id="cb6-3"><a></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-4"><a></a><span class="im">from</span> sentence_transformers <span class="im">import</span> SentenceTransformer</span>
<span id="cb6-5"><a></a></span>
<span id="cb6-6"><a></a><span class="kw">def</span> process_document(doc):</span>
<span id="cb6-7"><a></a>    <span class="co"># Extract text</span></span>
<span id="cb6-8"><a></a>    text <span class="op">=</span> extract_text(doc)</span>
<span id="cb6-9"><a></a>    <span class="co"># Clean and normalize</span></span>
<span id="cb6-10"><a></a>    cleaned <span class="op">=</span> clean_text(text)</span>
<span id="cb6-11"><a></a>    <span class="co"># Chunk into passages</span></span>
<span id="cb6-12"><a></a>    chunks <span class="op">=</span> chunk_text(cleaned, max_length<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb6-13"><a></a>    <span class="co"># Generate embeddings</span></span>
<span id="cb6-14"><a></a>    embeddings <span class="op">=</span> model.encode(chunks)</span>
<span id="cb6-15"><a></a>    <span class="cf">return</span> {<span class="st">'doc_id'</span>: doc.<span class="bu">id</span>, <span class="st">'chunks'</span>: chunks, <span class="st">'embeddings'</span>: embeddings}</span>
<span id="cb6-16"><a></a></span>
<span id="cb6-17"><a></a><span class="co"># Process millions of documents in parallel</span></span>
<span id="cb6-18"><a></a><span class="cf">with</span> Pool(processes<span class="op">=</span><span class="dv">32</span>) <span class="im">as</span> pool:</span>
<span id="cb6-19"><a></a>    results <span class="op">=</span> pool.<span class="bu">map</span>(process_document, documents)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="common-data-parallel-patterns-for-ai" class="slide level2">
<h2>Common Data Parallel Patterns for AI</h2>
<h3 id="embarrassingly-parallel">1. Embarrassingly Parallel</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a><span class="co"># Process each item independently</span></span>
<span id="cb7-2"><a></a>results <span class="op">=</span> parallel_map(transform_function, data_items)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<h3 id="map-reduce-pattern">2. Map-Reduce Pattern</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a><span class="co"># Map: Extract features from each document</span></span>
<span id="cb8-2"><a></a><span class="co"># Reduce: Aggregate statistics across corpus</span></span>
<span id="cb8-3"><a></a>word_counts <span class="op">=</span> data.<span class="bu">map</span>(tokenize).<span class="bu">reduce</span>(aggregate_counts)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<h3 id="pipeline-pattern">3. Pipeline Pattern</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a><span class="co"># Stage 1: Download → Stage 2: Extract → Stage 3: Transform → Stage 4: Load</span></span>
<span id="cb9-2"><a></a>pipeline <span class="op">=</span> Download() <span class="op">|</span> Extract() <span class="op">|</span> Transform() <span class="op">|</span> Load()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="data-quality-at-scale" class="slide level2">
<h2>Data Quality at Scale</h2>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="quality-checks-to-parallelize">Quality Checks to Parallelize</h3>
<ul>
<li>Duplicate detection (MinHash)</li>
<li>Language identification</li>
<li>Toxicity filtering</li>
<li>PII detection and removal</li>
<li>Format validation</li>
<li>Schema compliance</li>
<li>Statistical outlier detection</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="quality-metrics-for-ai-data">Quality Metrics for AI Data</h3>
<ul>
<li>Token distribution analysis</li>
<li>Vocabulary coverage</li>
<li>Domain representation</li>
<li>Bias measurement</li>
<li>Data freshness</li>
<li>Annotation agreement</li>
<li>Synthetic data detection</li>
</ul>
</div></div>
</section>
<section id="distributed-frameworks-for-ai-data" class="slide level2">
<h2>Distributed Frameworks for AI Data</h2>
<h3 id="apache-spark-for-large-scale-processing">Apache Spark for Large-Scale Processing</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a><span class="co"># Process TB-scale datasets</span></span>
<span id="cb10-2"><a></a>df <span class="op">=</span> spark.read.parquet(<span class="st">"s3://bucket/training_data/"</span>)</span>
<span id="cb10-3"><a></a>cleaned <span class="op">=</span> df.<span class="bu">filter</span>(col(<span class="st">"quality_score"</span>) <span class="op">&gt;</span> <span class="fl">0.8</span>) <span class="op">\</span></span>
<span id="cb10-4"><a></a>           .select(clean_text_udf(<span class="st">"text"</span>).alias(<span class="st">"cleaned_text"</span>))</span>
<span id="cb10-5"><a></a>cleaned.write.parquet(<span class="st">"s3://bucket/cleaned_data/"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<h3 id="ray-data-for-ml-pipelines">Ray Data for ML Pipelines</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a><span class="im">import</span> ray.data</span>
<span id="cb11-2"><a></a></span>
<span id="cb11-3"><a></a><span class="co"># Distributed preprocessing with Ray</span></span>
<span id="cb11-4"><a></a>ds <span class="op">=</span> ray.data.read_parquet(<span class="st">"s3://bucket/images/"</span>)</span>
<span id="cb11-5"><a></a>processed <span class="op">=</span> ds.map_batches(preprocess_images, batch_size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb11-6"><a></a>              .map_batches(extract_features, num_gpus<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="embedding-generation-at-scale" class="slide level2">
<h2>Embedding Generation at Scale</h2>
<ul>
<li class="fragment"><strong>Challenge</strong>: Generate embeddings for 100M+ documents</li>
<li class="fragment"><strong>Solution</strong>: Distributed GPU processing</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a><span class="co"># Distributed embedding generation</span></span>
<span id="cb12-2"><a></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel</span>
<span id="cb12-3"><a></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb12-4"><a></a></span>
<span id="cb12-5"><a></a><span class="kw">def</span> generate_embeddings_distributed(documents, rank, world_size):</span>
<span id="cb12-6"><a></a>    <span class="co"># Each GPU processes a subset</span></span>
<span id="cb12-7"><a></a>    subset <span class="op">=</span> documents[rank::world_size]</span>
<span id="cb12-8"><a></a>    model <span class="op">=</span> AutoModel.from_pretrained(<span class="st">"sentence-transformers/all-MiniLM-L6-v2"</span>)</span>
<span id="cb12-9"><a></a>    model <span class="op">=</span> model.to(<span class="ss">f"cuda:</span><span class="sc">{</span>rank<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-10"><a></a>    </span>
<span id="cb12-11"><a></a>    embeddings <span class="op">=</span> []</span>
<span id="cb12-12"><a></a>    <span class="cf">for</span> batch <span class="kw">in</span> batch_iterator(subset, batch_size<span class="op">=</span><span class="dv">256</span>):</span>
<span id="cb12-13"><a></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-14"><a></a>            emb <span class="op">=</span> model.encode(batch)</span>
<span id="cb12-15"><a></a>        embeddings.extend(emb)</span>
<span id="cb12-16"><a></a>    <span class="cf">return</span> embeddings</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="data-deduplication-for-ai" class="slide level2">
<h2>Data Deduplication for AI</h2>
<h3 id="why-deduplication-matters">Why Deduplication Matters</h3>
<ul>
<li>Training on duplicates wastes compute</li>
<li>Can cause overfitting and memorization</li>
<li>Critical for web-scale datasets</li>
</ul>
<h3 id="parallel-deduplication-strategies">Parallel Deduplication Strategies</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a><span class="co"># MinHash for approximate deduplication</span></span>
<span id="cb13-2"><a></a><span class="im">from</span> datasketch <span class="im">import</span> MinHash, MinHashLSH</span>
<span id="cb13-3"><a></a></span>
<span id="cb13-4"><a></a><span class="kw">def</span> parallel_dedup(documents):</span>
<span id="cb13-5"><a></a>    <span class="co"># Step 1: Generate MinHash signatures (parallel)</span></span>
<span id="cb13-6"><a></a>    signatures <span class="op">=</span> parallel_map(generate_minhash, documents)</span>
<span id="cb13-7"><a></a>    </span>
<span id="cb13-8"><a></a>    <span class="co"># Step 2: LSH for finding near-duplicates</span></span>
<span id="cb13-9"><a></a>    lsh <span class="op">=</span> MinHashLSH(threshold<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb13-10"><a></a>    <span class="cf">for</span> doc_id, sig <span class="kw">in</span> signatures:</span>
<span id="cb13-11"><a></a>        lsh.insert(doc_id, sig)</span>
<span id="cb13-12"><a></a>    </span>
<span id="cb13-13"><a></a>    <span class="co"># Step 3: Filter duplicates (parallel)</span></span>
<span id="cb13-14"><a></a>    unique_docs <span class="op">=</span> parallel_filter(is_unique, documents, lsh)</span>
<span id="cb13-15"><a></a>    <span class="cf">return</span> unique_docs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="synthetic-data-generation" class="slide level2">
<h2>Synthetic Data Generation</h2>
<h3 id="parallel-synthetic-data-creation">Parallel Synthetic Data Creation</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a><span class="co"># Generate training data using LLMs</span></span>
<span id="cb14-2"><a></a>prompts <span class="op">=</span> [</span>
<span id="cb14-3"><a></a>    <span class="st">"Generate a customer service dialogue about..."</span>,</span>
<span id="cb14-4"><a></a>    <span class="st">"Create a technical documentation for..."</span>,</span>
<span id="cb14-5"><a></a>    <span class="st">"Write a product review for..."</span></span>
<span id="cb14-6"><a></a>]</span>
<span id="cb14-7"><a></a></span>
<span id="cb14-8"><a></a><span class="co"># Parallel generation across multiple API calls</span></span>
<span id="cb14-9"><a></a><span class="cf">with</span> ThreadPoolExecutor(max_workers<span class="op">=</span><span class="dv">100</span>) <span class="im">as</span> executor:</span>
<span id="cb14-10"><a></a>    futures <span class="op">=</span> []</span>
<span id="cb14-11"><a></a>    <span class="cf">for</span> prompt <span class="kw">in</span> prompts:</span>
<span id="cb14-12"><a></a>        future <span class="op">=</span> executor.submit(generate_with_llm, prompt)</span>
<span id="cb14-13"><a></a>        futures.append(future)</span>
<span id="cb14-14"><a></a>    </span>
<span id="cb14-15"><a></a>    synthetic_data <span class="op">=</span> [f.result() <span class="cf">for</span> f <span class="kw">in</span> futures]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="fragment">
<h3 id="use-cases">Use Cases</h3>
<ul>
<li>Augmenting limited datasets</li>
<li>Creating instruction-tuning data</li>
<li>Generating test cases</li>
<li>Privacy-preserving alternatives</li>
</ul>
</div>
</section>
<section id="best-practices-for-parallel-data-processing" class="slide level2">
<h2>Best Practices for Parallel Data Processing</h2>
<ol type="1">
<li class="fragment"><strong>Profile First</strong>: Identify bottlenecks before parallelizing</li>
<li class="fragment"><strong>Chunk Appropriately</strong>: Balance chunk size with overhead</li>
<li class="fragment"><strong>Handle Failures</strong>: Implement retry logic and checkpointing</li>
<li class="fragment"><strong>Monitor Progress</strong>: Use progress bars and logging</li>
<li class="fragment"><strong>Validate Output</strong>: Always verify data quality post-processing</li>
<li class="fragment"><strong>Version Everything</strong>: Track data lineage and transformations</li>
</ol>
</section>
<section id="tools-comparison-for-ai-data-processing" class="slide level2">
<h2>Tools Comparison for AI Data Processing</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Tool</th>
<th>Best For</th>
<th>Scalability</th>
<th>Learning Curve</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>multiprocessing</strong></td>
<td>Single machine</td>
<td>10s of cores</td>
<td>Low</td>
</tr>
<tr class="even">
<td><strong>Spark</strong></td>
<td>Distributed batch</td>
<td>1000s of nodes</td>
<td>Medium</td>
</tr>
<tr class="odd">
<td><strong>Ray</strong></td>
<td>ML workloads</td>
<td>100s of nodes</td>
<td>Low</td>
</tr>
<tr class="even">
<td><strong>Dask</strong></td>
<td>Python-native</td>
<td>100s of nodes</td>
<td>Low</td>
</tr>
<tr class="odd">
<td><strong>Beam</strong></td>
<td>Stream + batch</td>
<td>1000s of nodes</td>
<td>High</td>
</tr>
</tbody>
</table>
</section>
<section id="case-study-building-a-rag-dataset" class="slide level2 hidden">
<h2>Case Study: Building a RAG Dataset</h2>
<h3 id="requirements">Requirements</h3>
<ul>
<li>Process 10M documents</li>
<li>Generate embeddings for each</li>
<li>Build vector index</li>
<li>Ensure quality and deduplication</li>
</ul>
<h3 id="parallel-solution">Parallel Solution</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a><span class="co"># Stage 1: Parallel document processing</span></span>
<span id="cb15-2"><a></a>processed_docs <span class="op">=</span> ray.data.read_parquet(<span class="st">"raw_docs/"</span>) <span class="op">\</span></span>
<span id="cb15-3"><a></a>    .map_batches(extract_and_clean) <span class="op">\</span></span>
<span id="cb15-4"><a></a>    .map_batches(chunk_documents)</span>
<span id="cb15-5"><a></a></span>
<span id="cb15-6"><a></a><span class="co"># Stage 2: Distributed embedding generation  </span></span>
<span id="cb15-7"><a></a>embeddings <span class="op">=</span> processed_docs.map_batches(</span>
<span id="cb15-8"><a></a>    generate_embeddings,</span>
<span id="cb15-9"><a></a>    num_gpus<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb15-10"><a></a>    batch_size<span class="op">=</span><span class="dv">100</span></span>
<span id="cb15-11"><a></a>)</span>
<span id="cb15-12"><a></a></span>
<span id="cb15-13"><a></a><span class="co"># Stage 3: Build index (using FAISS)</span></span>
<span id="cb15-14"><a></a>index <span class="op">=</span> build_distributed_index(embeddings)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="key-takeaways" class="slide level2">
<h2>Key Takeaways</h2>
<ul>
<li class="fragment"><strong>Data is the bottleneck</strong> in modern AI systems</li>
<li class="fragment"><strong>Parallel processing is essential</strong> for AI-scale data</li>
<li class="fragment"><strong>Choose the right tool</strong> for your scale and use case</li>
<li class="fragment"><strong>Quality over quantity</strong> - parallel quality checks are crucial</li>
<li class="fragment"><strong>Monitor and validate</strong> throughout the pipeline</li>
<li class="fragment"><strong>Modern frameworks</strong> (Spark, Ray, Dask) simplify distributed processing</li>
</ul>


<script type="ojs-module-contents">
eyJjb250ZW50cyI6W119
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section></section>

    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="ojs-module-contents">
    eyJjb250ZW50cyI6W119
    </script>
    <script type="module">
    if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
    window._ojs.paths.runtimeToDoc = "../../slides";
    window._ojs.paths.runtimeToRoot = "../..";
    window._ojs.paths.docToRoot = "..";
    window._ojs.selfContained = false;
    window._ojs.runtime.interpretFromScriptTags();
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/gwu-dats6450-13\.github\.io\/6450-spring-2026\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
                // target, if specified
                link.setAttribute("target", "_blank");
                if (link.getAttribute("rel") === null) {
                  link.setAttribute("rel", "noopener");
                }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
    (function() {
      let previousOnload = window.onload;
      window.onload = () => {
        if (previousOnload) {
          previousOnload();
        }
        lightboxQuarto.on('slide_before_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          const href = trigger.getAttribute('href');
          if (href !== null) {
            const imgEl = window.document.querySelector(`a[href="${href}"] img`);
            if (imgEl !== null) {
              const srcAttr = imgEl.getAttribute("src");
              if (srcAttr && srcAttr.startsWith("data:")) {
                slideConfig.href = srcAttr;
              }
            }
          } 
        });
      
        lightboxQuarto.on('slide_after_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(slideNode);
          }
        });
      
      };
      
    })();
              </script>
    

</body></html>