[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Course: DATS 6450.13 (Applied Big Data Analytics) Semester: Spring, 2026 Meeting time: Thursdays 6:10-8:40pm Location: Philips 416\n\n\nName: Abhijit Dasgupta, PhD\nPhone: 240-813-8458\nGW E-mail: abhijit.dasgupta@gwu.edu\nOffice hours:\nIn-person: Thursdays 5:10-6:10pm, Philips 416\nRemote: By appointment (via link)\nCommunications: via Slack\n\n\n\n\n\n\nImportantCommunication policy\n\n\n\nAll general class-related issues, including discussion of material, should be done on Slack. Any private issue, including absences, illness, issues around grades, etc. should be done via e-mail\n\n\n\nCourse prerequisites: DATS 6101, 6102, Python\nStudents completing the course will be able to:\n\nBuild data pipelines and transformations using DuckDB and PySpark.\nUse concepts of parallelization, embarrasingly parallel problems, and applications in Python to process large data\nCompare computational efficiency, memory management, and scalability between single-node, cluster and distributed frameworks.\nOptimize queries, caching, and partitioning strategies across engines.\nApply datashader or equivalent tools to visualize large datasets efficiently.\nDevelop end-to-end reproducible analytics workflows integrating both systems.\n\nAverage amount of direct instruction or guided interaction with the instructor and average minimum amount of independent (out-of-class) learning expected per week:\nThis class will meet for 2.5 hours of in-person learning per week. It is expected that homework, project and independent study will take an average of 8-10 hours weekly. We note here that classes are in-person only and no remote option will be available during the semester. It will be the student’s responsibility to make up work during an absence. Class attendance will be noted and class participation is encouraged to enable active learning of the material.\nRequired textbooks and/or other materials and recommended readings: There are no required textbooks for this class. Relevant readings will be assigned on a weekly basis\n\n\n\nGenerative Artificial Intelligence (GAI) tools such as ChatGPT are becoming important resources in many fields and industries. Accordingly, you are permitted to use such tools to generate content submitted for evaluation in this course, including assignments, with the limitation that no more than 50% of your submitted code can be generated using AI tools. You remain responsible for all content you submit for evaluation, and to ensure that the material submitted runs and produces correct results. Material for final presentation and reports must be substantially your own work, with GAI tools available to help with brainstorming, editing, organization, polish.\nYou may use GAI tools to help generate ideas and brainstorm. However, you should note that the material generated by these tools may be inaccurate, incomplete, or otherwise problematic. Beware that use may also stifle your own independent thinking and creativity.\nIf you include content (e.g., ideas, text, code, images) that was generated, in whole or in part, by Generative Artificial Intelligence tools (including, but not limited to, ChatGPT and other large language models) in work submitted for evaluation in this course, you must document and credit your source. For example, text generated using ChatGPT-4 should include a citation such as: “ChatGPT-4. (YYYY, Month DD of query). ‘Text of your query.’ Generated using OpenAI. https://chat.openai.com/.” Material generated using other tools should be cited accordingly. Failure to do so in this course constitutes failure to attribute under the George Washington University Code of Academic Integrity.\n\n\n\nWe will check for plagiarism of submitted code between students. We understand that there are common templates and structures in code that will naturally be common between students, but everyone does typically have their own coding style, naming conventions, comments and approaches. Any pair of submissions that appear to have more than 70% code in common will be subject to further review. Code that is over 90% common will be considered a potential violation of the Honor Code.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nLab\n\n\n1\n2026-01-15\nCourse overview\nBash shell\n\n\n3\n2026-01-22\nParallel and distributed computing\nUsing multiprocesing and asyncio\n\n\n2\n2026-01-29\nIntroduction to Cloud Computing\nAWS setup\n\n\n4\n2026-02-05\nintroduction to DuckDB and Polars\n\n\n\n5\n2026-02-12\nAdvanced DuckDB operations\n\n\n\n6\n2026-02-19\nPython tools for larger data\npandas, dask, Ray, RAPIDS\n\n\n7\n2026-02-26\nDeveloping workflows for big data (Midterm 1)\n\n\n\n8\n2026-03-05\nApache Spark Fundamentals\nSpark RDD\n\n\n9\n2026-03-12\nSpring Break\n\n\n\n10\n2026-03-19\nSpark DataFrames and Spark SQL\n\n\n\n11\n2026-03-26\nSpark ML and Streaming\nMLib\n\n\n12\n2026-04-02\nSpark NLP\nJohn Snow Labs\n\n\n13\n2026-04-09\nVisualization and reporting large data (Midterm 2)\ndatashader\n\n\n14\n2026-04-16\nMachine Learning at Scale\nTensorflow, TF probability, PyMC\n\n\n15\n2026-04-23\nFinal project presentations\n\n\n\n\nScheduling of final examinations: There will be no final exams for this class. There will be a final group project which will be an implementation of a data science analytic pipeline for a large dataset that will run on the cloud.\n\n\n\nEach week we will have several short assessment activities to build on class materials. These will include:\n\nShort weekly homework that will be graded for correctness. You will have 7-10 days to complete each assignment. Assignments will be provided and submissions made via Github Classroom\nLabs will be started in class and will need to be completed and submitted via Github Classroom. These will be graded for completion\nYou will be given weekly readings that introduce the material for the following week. There will be a short quiz based on the readings that must be completed before class each week.\nThere will be two midterm evaluations. These will be analytic and coding exercises to be done in class without the help of any generative AI or LLM coding assistance. These will typically be 45-60 minutes long and will also be submitted via Github Classroom.\nYou will form groups of 2-3 students and do an analytic group project on a Big dataset. This project will involve an end-to-end analytic pipeline and report based on either a data set you choose or a dataset provided to you (which is TBD). You will have required ungraded check-ins with the professor per schedule to show progress, and a presentation on the final day of class. The project report and code must be submitted per the calendar\n\n\n\n\nQuizzes, labs, midterms and projects cannot be late; a late submission will be entered as a 0.\n\nFor quizzes and labs, the lowest score for each type will be removed from calculation of the final grade at the end of the semester\n\nHomework will have the following late policy:\n\nThere will be a penalty of 10% of the total points per day for each day late, for a maximum of 20% penalty\nAny homework can be submitted by the last day of class for a penalty of 40% of the total points for that homework\n\n\n\n\n\n\n\nhomework (25%)\nlab completions (10%)\nquizzes (10%)\nmidterm exams (15%)\nfinal group project (30%)\nclass participation/attendance (10%)\n\n\n\n\nAcademic integrity is an essential part of the educational process, and all members of the GW community take these matters very seriously. As the instructor of record for this course, my role is to provide clear expectations and uphold them in all assessments. Violations of academic integrity occur when students fail to cite research sources properly, engage in unauthorized collaboration, falsify data, and otherwise violate the Code of Academic Integrity. If you have any questions about whether particular academic practices or resources are permitted, you should ask me for clarification. If you are reported for an academic integrity violation, you should contact Conflict Education and Student Accountability (CESA), formerly known as Student Rights and Responsibilities (SRR), to learn more about your rights and options in the process. Consequences can range from failure of assignment to expulsion from the University and may include a transcript notation. For more information, refer to the CESA website at students.gwu.edu/code-academic-integrity or contact CESA by email cesa@gwu.edu or phone 202-994-6757.\n\n\n\nStudents must notify faculty during the first week of the semester in which they are enrolled in the course, or as early as possible, but no later than three weeks prior to the absence, of their intention to be absent from class on their day(s) of religious observance. If the holiday falls within the first three weeks of class, the student must inform faculty in the first week of the semester. For details and policy, see provost.gwu.edu/policies-procedures-and-guidelines.\n\n\n\nStudents are encouraged to use electronic course materials, including recorded class sessions, for private personal use in connection with their academic program of study. Electronic course materials and recorded class sessions should not be shared or used for non-course related purposes unless express permission has been granted by the instructor. Students who impermissibly share any electronic course materials are subject to discipline under the Student Code of Conduct. Contact the instructor if you have questions regarding what constitutes permissible or impermissible use of electronic course materials and/or recorded class sessions. Contact Disability Support Services at disabilitysupport.gwu.edu if you have questions or need assistance in accessing electronic course materials."
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus",
    "section": "",
    "text": "Name: Abhijit Dasgupta, PhD\nPhone: 240-813-8458\nGW E-mail: abhijit.dasgupta@gwu.edu\nOffice hours:\nIn-person: Thursdays 5:10-6:10pm, Philips 416\nRemote: By appointment (via link)\nCommunications: via Slack\n\n\n\n\n\n\nImportantCommunication policy\n\n\n\nAll general class-related issues, including discussion of material, should be done on Slack. Any private issue, including absences, illness, issues around grades, etc. should be done via e-mail\n\n\n\nCourse prerequisites: DATS 6101, 6102, Python\nStudents completing the course will be able to:\n\nBuild data pipelines and transformations using DuckDB and PySpark.\nUse concepts of parallelization, embarrasingly parallel problems, and applications in Python to process large data\nCompare computational efficiency, memory management, and scalability between single-node, cluster and distributed frameworks.\nOptimize queries, caching, and partitioning strategies across engines.\nApply datashader or equivalent tools to visualize large datasets efficiently.\nDevelop end-to-end reproducible analytics workflows integrating both systems.\n\nAverage amount of direct instruction or guided interaction with the instructor and average minimum amount of independent (out-of-class) learning expected per week:\nThis class will meet for 2.5 hours of in-person learning per week. It is expected that homework, project and independent study will take an average of 8-10 hours weekly. We note here that classes are in-person only and no remote option will be available during the semester. It will be the student’s responsibility to make up work during an absence. Class attendance will be noted and class participation is encouraged to enable active learning of the material.\nRequired textbooks and/or other materials and recommended readings: There are no required textbooks for this class. Relevant readings will be assigned on a weekly basis"
  },
  {
    "objectID": "syllabus.html#class-policy-on-the-use-of-ai",
    "href": "syllabus.html#class-policy-on-the-use-of-ai",
    "title": "Syllabus",
    "section": "",
    "text": "Generative Artificial Intelligence (GAI) tools such as ChatGPT are becoming important resources in many fields and industries. Accordingly, you are permitted to use such tools to generate content submitted for evaluation in this course, including assignments, with the limitation that no more than 50% of your submitted code can be generated using AI tools. You remain responsible for all content you submit for evaluation, and to ensure that the material submitted runs and produces correct results. Material for final presentation and reports must be substantially your own work, with GAI tools available to help with brainstorming, editing, organization, polish.\nYou may use GAI tools to help generate ideas and brainstorm. However, you should note that the material generated by these tools may be inaccurate, incomplete, or otherwise problematic. Beware that use may also stifle your own independent thinking and creativity.\nIf you include content (e.g., ideas, text, code, images) that was generated, in whole or in part, by Generative Artificial Intelligence tools (including, but not limited to, ChatGPT and other large language models) in work submitted for evaluation in this course, you must document and credit your source. For example, text generated using ChatGPT-4 should include a citation such as: “ChatGPT-4. (YYYY, Month DD of query). ‘Text of your query.’ Generated using OpenAI. https://chat.openai.com/.” Material generated using other tools should be cited accordingly. Failure to do so in this course constitutes failure to attribute under the George Washington University Code of Academic Integrity."
  },
  {
    "objectID": "syllabus.html#note-on-code-plagiarism",
    "href": "syllabus.html#note-on-code-plagiarism",
    "title": "Syllabus",
    "section": "",
    "text": "We will check for plagiarism of submitted code between students. We understand that there are common templates and structures in code that will naturally be common between students, but everyone does typically have their own coding style, naming conventions, comments and approaches. Any pair of submissions that appear to have more than 70% code in common will be subject to further review. Code that is over 90% common will be considered a potential violation of the Honor Code."
  },
  {
    "objectID": "syllabus.html#schedule-of-topics",
    "href": "syllabus.html#schedule-of-topics",
    "title": "Syllabus",
    "section": "",
    "text": "Week\nDate\nTopic\nLab\n\n\n1\n2026-01-15\nCourse overview\nBash shell\n\n\n3\n2026-01-22\nParallel and distributed computing\nUsing multiprocesing and asyncio\n\n\n2\n2026-01-29\nIntroduction to Cloud Computing\nAWS setup\n\n\n4\n2026-02-05\nintroduction to DuckDB and Polars\n\n\n\n5\n2026-02-12\nAdvanced DuckDB operations\n\n\n\n6\n2026-02-19\nPython tools for larger data\npandas, dask, Ray, RAPIDS\n\n\n7\n2026-02-26\nDeveloping workflows for big data (Midterm 1)\n\n\n\n8\n2026-03-05\nApache Spark Fundamentals\nSpark RDD\n\n\n9\n2026-03-12\nSpring Break\n\n\n\n10\n2026-03-19\nSpark DataFrames and Spark SQL\n\n\n\n11\n2026-03-26\nSpark ML and Streaming\nMLib\n\n\n12\n2026-04-02\nSpark NLP\nJohn Snow Labs\n\n\n13\n2026-04-09\nVisualization and reporting large data (Midterm 2)\ndatashader\n\n\n14\n2026-04-16\nMachine Learning at Scale\nTensorflow, TF probability, PyMC\n\n\n15\n2026-04-23\nFinal project presentations\n\n\n\n\nScheduling of final examinations: There will be no final exams for this class. There will be a final group project which will be an implementation of a data science analytic pipeline for a large dataset that will run on the cloud."
  },
  {
    "objectID": "syllabus.html#assignments-and-evaluations",
    "href": "syllabus.html#assignments-and-evaluations",
    "title": "Syllabus",
    "section": "",
    "text": "Each week we will have several short assessment activities to build on class materials. These will include:\n\nShort weekly homework that will be graded for correctness. You will have 7-10 days to complete each assignment. Assignments will be provided and submissions made via Github Classroom\nLabs will be started in class and will need to be completed and submitted via Github Classroom. These will be graded for completion\nYou will be given weekly readings that introduce the material for the following week. There will be a short quiz based on the readings that must be completed before class each week.\nThere will be two midterm evaluations. These will be analytic and coding exercises to be done in class without the help of any generative AI or LLM coding assistance. These will typically be 45-60 minutes long and will also be submitted via Github Classroom.\nYou will form groups of 2-3 students and do an analytic group project on a Big dataset. This project will involve an end-to-end analytic pipeline and report based on either a data set you choose or a dataset provided to you (which is TBD). You will have required ungraded check-ins with the professor per schedule to show progress, and a presentation on the final day of class. The project report and code must be submitted per the calendar\n\n\n\n\nQuizzes, labs, midterms and projects cannot be late; a late submission will be entered as a 0.\n\nFor quizzes and labs, the lowest score for each type will be removed from calculation of the final grade at the end of the semester\n\nHomework will have the following late policy:\n\nThere will be a penalty of 10% of the total points per day for each day late, for a maximum of 20% penalty\nAny homework can be submitted by the last day of class for a penalty of 40% of the total points for that homework"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "",
    "text": "homework (25%)\nlab completions (10%)\nquizzes (10%)\nmidterm exams (15%)\nfinal group project (30%)\nclass participation/attendance (10%)"
  },
  {
    "objectID": "syllabus.html#academic-integrity-code",
    "href": "syllabus.html#academic-integrity-code",
    "title": "Syllabus",
    "section": "",
    "text": "Academic integrity is an essential part of the educational process, and all members of the GW community take these matters very seriously. As the instructor of record for this course, my role is to provide clear expectations and uphold them in all assessments. Violations of academic integrity occur when students fail to cite research sources properly, engage in unauthorized collaboration, falsify data, and otherwise violate the Code of Academic Integrity. If you have any questions about whether particular academic practices or resources are permitted, you should ask me for clarification. If you are reported for an academic integrity violation, you should contact Conflict Education and Student Accountability (CESA), formerly known as Student Rights and Responsibilities (SRR), to learn more about your rights and options in the process. Consequences can range from failure of assignment to expulsion from the University and may include a transcript notation. For more information, refer to the CESA website at students.gwu.edu/code-academic-integrity or contact CESA by email cesa@gwu.edu or phone 202-994-6757."
  },
  {
    "objectID": "syllabus.html#university-policy-on-observance-of-religious-holidays",
    "href": "syllabus.html#university-policy-on-observance-of-religious-holidays",
    "title": "Syllabus",
    "section": "",
    "text": "Students must notify faculty during the first week of the semester in which they are enrolled in the course, or as early as possible, but no later than three weeks prior to the absence, of their intention to be absent from class on their day(s) of religious observance. If the holiday falls within the first three weeks of class, the student must inform faculty in the first week of the semester. For details and policy, see provost.gwu.edu/policies-procedures-and-guidelines."
  },
  {
    "objectID": "syllabus.html#use-of-electronic-course-materials-and-class-recordings",
    "href": "syllabus.html#use-of-electronic-course-materials-and-class-recordings",
    "title": "Syllabus",
    "section": "",
    "text": "Students are encouraged to use electronic course materials, including recorded class sessions, for private personal use in connection with their academic program of study. Electronic course materials and recorded class sessions should not be shared or used for non-course related purposes unless express permission has been granted by the instructor. Students who impermissibly share any electronic course materials are subject to discipline under the Student Code of Conduct. Contact the instructor if you have questions regarding what constitutes permissible or impermissible use of electronic course materials and/or recorded class sessions. Contact Disability Support Services at disabilitysupport.gwu.edu if you have questions or need assistance in accessing electronic course materials."
  },
  {
    "objectID": "syllabus.html#academic-commons",
    "href": "syllabus.html#academic-commons",
    "title": "Syllabus",
    "section": "Academic Commons",
    "text": "Academic Commons\nAcademic Commons is the central location for academic support resources for GW students. To schedule a peer tutoring session for a variety of courses visit go.gwu.edu/tutoring. Visit academiccommons.gwu.edu for study skills tips, finding help with research, and connecting with other campus resources. For questions email academiccommons@gwu.edu."
  },
  {
    "objectID": "syllabus.html#gw-writing-center",
    "href": "syllabus.html#gw-writing-center",
    "title": "Syllabus",
    "section": "GW Writing Center",
    "text": "GW Writing Center\nGW Writing Center cultivates confident writers in the University community by facilitating collaborative, critical, and inclusive conversations at all stages of the writing process. Working alongside peer mentors, writers develop strategies to write independently in academic and public settings. Appointments can be booked online at gwu.mywconline."
  },
  {
    "objectID": "syllabus.html#disability-support-services-dss-202-994-8250",
    "href": "syllabus.html#disability-support-services-dss-202-994-8250",
    "title": "Syllabus",
    "section": "Disability Support Services (DSS) 202-994-8250",
    "text": "Disability Support Services (DSS) 202-994-8250\nAny student who may need an accommodation based on the potential impact of a disability should contact Disability Support Services at disabilitysupport.gwu.edu to establish eligibility and to coordinate reasonable accommodation."
  },
  {
    "objectID": "syllabus.html#student-health-center-202-994-5300-247",
    "href": "syllabus.html#student-health-center-202-994-5300-247",
    "title": "Syllabus",
    "section": "Student Health Center 202-994-5300, 24/7",
    "text": "Student Health Center 202-994-5300, 24/7\nThe Student Health Center (SHC) offers medical, counseling/psychological, and psychiatric services to GW students. More information about the SHC is available at healthcenter.gwu.edu. Students experiencing a medical or mental health emergency on campus should contact GW Emergency Services at 202-994-6111, or off campus at 911."
  },
  {
    "objectID": "syllabus.html#gw-emergency-services-202-994-6111",
    "href": "syllabus.html#gw-emergency-services-202-994-6111",
    "title": "Syllabus",
    "section": "GW Emergency Services: 202-994-6111",
    "text": "GW Emergency Services: 202-994-6111\nFor situation-specific instructions, refer to GW’s Emergency Procedures guide."
  },
  {
    "objectID": "syllabus.html#gw-alert",
    "href": "syllabus.html#gw-alert",
    "title": "Syllabus",
    "section": "GW Alert",
    "text": "GW Alert\nGW Alert is an emergency notification system that sends alerts to the GW community. GW requests students, faculty, and staff maintain current contact information by logging on to alert.gwu.edu. Alerts are sent via email, text, social media, and other means, including the Guardian app. The Guardian app is a safety app that allows you to communicate quickly with GW Emergency Services, 911, and other resources. Learn more at safety.gwu.edu."
  },
  {
    "objectID": "syllabus.html#protective-actions",
    "href": "syllabus.html#protective-actions",
    "title": "Syllabus",
    "section": "Protective Actions",
    "text": "Protective Actions\nGW prescribes four protective actions that can be issued by university officials depending on the type of emergency. All GW community members are expected to follow directions according to the specified protective action. The protective actions are Shelter, Evacuate, Secure, and Lockdown (details below). Learn more at safety.gwu.edu/gw-standard-emergency-statuses.\n\nShelter\n\nProtection from a specific hazard\nThe hazard could be a tornado, earthquake, hazardous material spill, or other environmental emergency.\nSpecific safety guidance will be shared on a case-by-case basis.\n\n\n\nAction:\n\nFollow safety guidance for the hazard.\n\n\n\nEvacuate\n\nNeed to move people from one location to another.\nStudents and staff should be prepared to follow specific instructions given by first responders and University officials.\n\n\n\nAction:\n\nEvacuate to a designated location.\nLeave belongings behind.\nFollow additional instructions from first responders.\n\n\n\nSecure\n\nThreat or hazard outside of buildings or around campus.\nncreased security, secured building perimeter, increased situational awareness, and restricted access to entry doors.\n\n\n\nAction:\n\nGo inside and stay inside.\nActivities inside may continue.\n\n\n\nLockdown\n\nThreat or hazard with the potential to impact individuals inside buildings.\nRoom-based protocol that requires locking interior doors, turning off lights, and staying out of sight of corridor window.\n\n\n\nAction:\n\nLocks, lights, out of sight\nConsider Run, Hide, Fight\n\n\n\nClassroom Emergency Lockdown Buttons\nSome classrooms have been equipped with classroom emergency lockdown buttons. If the button is pushed, GWorld Card access to the room will be disabled, and GW Dispatch will be alerted. The door must be manually closed if it is not closed when the button is pushed. Anyone in the classroom will be able to exit, but no one will be able to get in."
  },
  {
    "objectID": "slides/01-slides.html#agenda-for-todays-session",
    "href": "slides/01-slides.html#agenda-for-todays-session",
    "title": "Lecture 1",
    "section": "Agenda for today’s session",
    "text": "Agenda for today’s session\n\nCourse and syllabus overview\nBig Data Concepts\n\nDefinition\nChallenges\nApproaches\n\nData Engineering\nIntroduction to bash\n\nLab: Linux command line"
  },
  {
    "objectID": "slides/01-slides.html#bookmark-these-links",
    "href": "slides/01-slides.html#bookmark-these-links",
    "title": "Lecture 1",
    "section": "Bookmark these links!",
    "text": "Bookmark these links!\n\nCourse website: https://gwu-dats6450-13.github.io/6450-spring-2026\nGitHub Organization for your deliverables: https://github.com/gwu-dats6450-13/ \nSlack Workspace: DATS 6450.13 Spring 2026 - 6450-spring-2026.slack.com\n\nJoin link: https://join.slack.com/t/6450-spring-2026/shared_invite/zt-3nh9da585-T0CBYJd4vJwm7mLCEpnrKg"
  },
  {
    "objectID": "slides/01-slides.html#instructors",
    "href": "slides/01-slides.html#instructors",
    "title": "Lecture 1",
    "section": "Instructors",
    "text": "Instructors\n\nAbhijit Dasgupta, abhijit.dasgupta@gwu.edu\nTA: Vishal Fulsundar vishal.fulsundar@gwmail.gwu.edu"
  },
  {
    "objectID": "slides/01-slides.html#abhijit-dasgupta",
    "href": "slides/01-slides.html#abhijit-dasgupta",
    "title": "Lecture 1",
    "section": "Abhijit Dasgupta",
    "text": "Abhijit Dasgupta\n\n“AI” is the new line of makeup in data science; putting it on makes even BS attractive\n\n\n\n\nData Science Director at AstraZeneca supporting Oncology R&D (lung, breast, bladder, GI, ovarian)\n\nbiomarkers, clinical studies, strategy, decision-making, education\nBayesian, survival, visualization, high-dimensional, causality\n\nAdjunct Professor at Georgetown since 2020, GMU since 2018, GWU since 2026\nData science consultant in a wide variety of domains, from the USPS to telecom to environmental health and epidemiology\nR and reproducible research evangelist\n\nPython is cool too!!\n\n\nFun facts\n\nAikido instructor (5th degree black belt), teaching since 1994\n\n\n Community theater actor with Ebong Theatrix (Bengali group in DC)"
  },
  {
    "objectID": "slides/01-slides.html#course-description",
    "href": "slides/01-slides.html#course-description",
    "title": "Lecture 1",
    "section": "Course Description",
    "text": "Course Description\nData is everywhere! Many times, it’s just too big to work with traditional tools. This is a hands-on, practical workshop style course about using cloud computing resources to do analysis and manipulation of datasets that are too large to fit on a single machine and/or analyzed with traditional tools. The course will focus on Spark, MapReduce, the Hadoop Ecosystem and other tools.\nYou will understand how to acquire and/or ingest the data, and then massage, clean, transform, analyze, and model it within the context of big data analytics. You will be able to think more programmatically and logically about your big data needs, tools and issues.\n\nAlways refer to the syllabus in the course website for class policies."
  },
  {
    "objectID": "slides/01-slides.html#learning-objectives",
    "href": "slides/01-slides.html#learning-objectives",
    "title": "Lecture 1",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nBuild data pipelines and transformations using DuckDB and PySpark.\nUse concepts of parallelization, embarrasingly parallel problems, and applications in Python to process large data\nCompare computational efficiency, memory management, and scalability between single-node, cluster and distributed frameworks.\nOptimize queries, caching, and partitioning strategies across engines.\nApply datashader or equivalent tools to visualize large datasets efficiently.\nDevelop end-to-end reproducible analytics workflows integrating both systems."
  },
  {
    "objectID": "slides/01-slides.html#evaluation",
    "href": "slides/01-slides.html#evaluation",
    "title": "Lecture 1",
    "section": "Evaluation",
    "text": "Evaluation\n\nhomework (25%)\nlab completions (10%)\nquizzes (10%)\nmidterm exams (15%)\nfinal group project (30%)\nclass participation/attendance (10%)"
  },
  {
    "objectID": "slides/01-slides.html#course-materials",
    "href": "slides/01-slides.html#course-materials",
    "title": "Lecture 1",
    "section": "Course Materials",
    "text": "Course Materials\n\nSlides/labs/assignment on Website/GitHub\nQuizzes and readings in Blackboard\nNo required textbook. Recommended readings will be provided."
  },
  {
    "objectID": "slides/01-slides.html#communication",
    "href": "slides/01-slides.html#communication",
    "title": "Lecture 1",
    "section": "Communication",
    "text": "Communication\n\nSlack is the primary form of communication for this course. Please join the Slack workspace for announcements, questions, and discussions.\nEmail is reserved for private matters only. Please allow 24-48 hours for a response\nIn-person office hours will be held the hour before class in this classroom\nOnline office hours are by appointment (at this link)"
  },
  {
    "objectID": "slides/01-slides.html#slack-rules",
    "href": "slides/01-slides.html#slack-rules",
    "title": "Lecture 1",
    "section": "Slack rules:",
    "text": "Slack rules:\n\nPost any question/comment about the course, assignments or any technical issue.\nDMs are to be used sparingly\nYou may not DM multiple people in the instructional team at the same time for the same issue\nKeep an eye on the questions posted in Slack. Use the search function. It’s very possible that we have already answered a questions\nYou may DM us back only if we DM you first on a given issue\nLab/assignment/project questions will only be answered up to 6 hours before something is due"
  },
  {
    "objectID": "slides/01-slides.html#midterms",
    "href": "slides/01-slides.html#midterms",
    "title": "Lecture 1",
    "section": "Midterms",
    "text": "Midterms\nWe will have 2 midterms during the semester. They will be held during regular class hours. The midterms will be a short coding project that will be based on the previous labs and assignments. No GAI (chatGPT etc) will be allowed for the midterms\nDetails: TBD (will be announced by week 3)"
  },
  {
    "objectID": "slides/01-slides.html#final-project",
    "href": "slides/01-slides.html#final-project",
    "title": "Lecture 1",
    "section": "Final Project",
    "text": "Final Project\n\nGroups of 2-3 students\nUse an archive of Reddit data, augmented with external data\nExploratory analysis\nNLP\nMachine Learning\nWriteup\n\nData sourcing and ingesting\nExploratory analysis\nModeling\nChallenges and Learnings\nConclusions\nFuture work\n\nPresentation\n\n15 minute presentation\nDemo of code/workflow\n\nDue date: Finals week (date TBD)"
  },
  {
    "objectID": "slides/01-slides.html#where-does-it-come-from-how-is-it-being-created",
    "href": "slides/01-slides.html#where-does-it-come-from-how-is-it-being-created",
    "title": "Lecture 1",
    "section": "Where does it come from?How is it being created?",
    "text": "Where does it come from?How is it being created?"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2018",
    "href": "slides/01-slides.html#in-one-minute-of-time-2018",
    "title": "Lecture 1",
    "section": "In one minute of time (2018)",
    "text": "In one minute of time (2018)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2019",
    "href": "slides/01-slides.html#in-one-minute-of-time-2019",
    "title": "Lecture 1",
    "section": "In one minute of time (2019)",
    "text": "In one minute of time (2019)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2020",
    "href": "slides/01-slides.html#in-one-minute-of-time-2020",
    "title": "Lecture 1",
    "section": "In one minute of time (2020)",
    "text": "In one minute of time (2020)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2021",
    "href": "slides/01-slides.html#in-one-minute-of-time-2021",
    "title": "Lecture 1",
    "section": "In one minute of time (2021)",
    "text": "In one minute of time (2021)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2025",
    "href": "slides/01-slides.html#in-one-minute-of-time-2025",
    "title": "Lecture 1",
    "section": "In one minute of time (2025)",
    "text": "In one minute of time (2025)\nEvery 60 seconds in 2025: * ChatGPT serves millions of requests (exact numbers proprietary) * 500 hours of video uploaded to YouTube * 1.04 million Slack messages sent * 362,000 hours watched on Netflix * 5.9-11.4 million Google searches * $443,000 spent on Amazon * AI-generated images created at massive scale (metrics not publicly available) * 347,200 posts on X (formerly Twitter) * 231-250 million emails sent"
  },
  {
    "objectID": "slides/01-slides.html#a-lot-of-it-is-hapenning-online.",
    "href": "slides/01-slides.html#a-lot-of-it-is-hapenning-online.",
    "title": "Lecture 1",
    "section": "A lot of it is hapenning online.",
    "text": "A lot of it is hapenning online.\nWe can record every: * click * ad impression * billing event * video interaction * server request * transaction * network message * fault * …"
  },
  {
    "objectID": "slides/01-slides.html#it-can-also-be-user-generated-content-e.g.",
    "href": "slides/01-slides.html#it-can-also-be-user-generated-content-e.g.",
    "title": "Lecture 1",
    "section": "It can also be user-generated content, e.g.:",
    "text": "It can also be user-generated content, e.g.:\n\nInstagram posts & Reels\nX (Twitter) posts & Threads\nTikTok videos\nYouTube Shorts\nReddit discussions\nDiscord conversations\nAI-generated content (text, images, code)\n…"
  },
  {
    "objectID": "slides/01-slides.html#but-health-and-scientific-computing-create-a-lot-too",
    "href": "slides/01-slides.html#but-health-and-scientific-computing-create-a-lot-too",
    "title": "Lecture 1",
    "section": "But health and scientific computing create a lot too!",
    "text": "But health and scientific computing create a lot too!\n\n???"
  },
  {
    "objectID": "slides/01-slides.html#theres-lots-of-graph-data-too",
    "href": "slides/01-slides.html#theres-lots-of-graph-data-too",
    "title": "Lecture 1",
    "section": "There’s lots of graph data too",
    "text": "There’s lots of graph data too\nMany interesting datasets have a graph structure:\n\nSocial networks\nGoogle’s knowledge graph\nTelecom networks\nComputer networks\nRoad networks\nCollaboration/relationships\n\nSome of these are HUGE"
  },
  {
    "objectID": "slides/01-slides.html#apache-web-server-log-files",
    "href": "slides/01-slides.html#apache-web-server-log-files",
    "title": "Lecture 1",
    "section": "Apache (web server) log files",
    "text": "Apache (web server) log files"
  },
  {
    "objectID": "slides/01-slides.html#system-log-files",
    "href": "slides/01-slides.html#system-log-files",
    "title": "Lecture 1",
    "section": "System log files",
    "text": "System log files"
  },
  {
    "objectID": "slides/01-slides.html#internet-of-things-iot-in-2025",
    "href": "slides/01-slides.html#internet-of-things-iot-in-2025",
    "title": "Lecture 1",
    "section": "Internet of Things (IoT) in 2025",
    "text": "Internet of Things (IoT) in 2025\n75 billion connected devices generating data: * Smart home devices (Alexa, Google Home, Apple HomePod) * Wearables (Apple Watch, Fitbit, Oura rings) * Connected vehicles & autonomous driving systems * Industrial IoT sensors * Smart city infrastructure * Medical devices & remote patient monitoring"
  },
  {
    "objectID": "slides/01-slides.html#smartphones-collecting-our-information",
    "href": "slides/01-slides.html#smartphones-collecting-our-information",
    "title": "Lecture 1",
    "section": "Smartphones collecting our information",
    "text": "Smartphones collecting our information"
  },
  {
    "objectID": "slides/01-slides.html#where-else",
    "href": "slides/01-slides.html#where-else",
    "title": "Lecture 1",
    "section": "Where else?",
    "text": "Where else?\n\nThe Internet\nTransactions\nDatabases\nExcel\nPDF Files\nAnything digital (music, movies, apps)\nSome old floppy disk lying around the house"
  },
  {
    "objectID": "slides/01-slides.html#typical-real-world-scenarios-2026",
    "href": "slides/01-slides.html#typical-real-world-scenarios-2026",
    "title": "Lecture 1",
    "section": "Typical real world scenarios (2026)",
    "text": "Typical real world scenarios (2026)\nScenario 1: Traditional Big Data\nYou have a laptop with 16GB of RAM and a 256GB SSD. You are given a 1TB dataset in text files. What do you do?\nScenario 2: AI/ML Pipeline\nYour company wants to build a RAG system using 10TB of internal documents. You need sub-second query response times. How do you architect this?\nScenario 3: Real-time Analytics\nYou need to process 1 million events per second from IoT devices and provide real-time dashboards with &lt;1 second latency. What’s your stack?"
  },
  {
    "objectID": "slides/01-slides.html#lets-discuss",
    "href": "slides/01-slides.html#lets-discuss",
    "title": "Lecture 1",
    "section": "Let’s discuss!",
    "text": "Let’s discuss!\n\n\n\nExponential data growth"
  },
  {
    "objectID": "slides/01-slides.html#big-data-definitions",
    "href": "slides/01-slides.html#big-data-definitions",
    "title": "Lecture 1",
    "section": "Big Data Definitions",
    "text": "Big Data Definitions\nWikipedia\n“In essence, is a term for a collection of datasets so large and complex that it becomes difficult to process using traditional tools and applications. Big Data technologies describe a new generation of technologies and architectures designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discover and/or analysis”\nO’Reilly\n“Big data is when the size of the data itself becomes part of the problem”\nEMC/IDC\n“Big data technologies describe a new generation of technologies and architectures, designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discovery, and/or analysis.”"
  },
  {
    "objectID": "slides/01-slides.html#frameworks-for-thinking-about-big-data",
    "href": "slides/01-slides.html#frameworks-for-thinking-about-big-data",
    "title": "Lecture 1",
    "section": "Frameworks for thinking about Big Data",
    "text": "Frameworks for thinking about Big Data\nIBM: (The famous 3-V’s definition)\n\nVolume (Gigabytes -&gt; Exabytes -&gt; Zettabytes)\nVelocity (Batch -&gt; Streaming -&gt; Real-time AI inference)\nVariety (Structured, Semi-structured, Unstructured, Embeddings)\n\nAdditional V’s for 2025\n\nVariability\nVeracity\nVisualization\nValue\nVectors (embeddings for AI/ML)\nVersatility (multi-modal data)"
  },
  {
    "objectID": "slides/01-slides.html#think-of-data-size-as-a-function-of-processing-and-storage",
    "href": "slides/01-slides.html#think-of-data-size-as-a-function-of-processing-and-storage",
    "title": "Lecture 1",
    "section": "Think of data size as a function of processing and storage",
    "text": "Think of data size as a function of processing and storage\n\nCan you analyze/process your data on a single machine?\nCan you store (or is it stored) on a single machine?\nCan you serve it fast enough for real-time AI applications?\n\nIf any of of the answers is no then you have a big-ish data problem!"
  },
  {
    "objectID": "slides/01-slides.html#the-new-data-landscape-2025",
    "href": "slides/01-slides.html#the-new-data-landscape-2025",
    "title": "Lecture 1",
    "section": "The New Data Landscape (2025)",
    "text": "The New Data Landscape (2025)\nTraining Foundation Models\n\nGPT-4: Trained on ~13 trillion tokens\nLlama 3: 15 trillion tokens\nGoogle’s Gemini: Multi-modal training on text, images, video\nEach iteration requires petabytes of curated data\n\nData Requirements Have Exploded\n\n2020: BERT trained on 3.3 billion words\n2023: GPT-4 trained on ~13 trillion tokens\n2024: Llama 3 trained on 15+ trillion tokens\nFuture: Approaching the limits of human-generated text"
  },
  {
    "objectID": "slides/01-slides.html#big-data-infrastructure-for-ai",
    "href": "slides/01-slides.html#big-data-infrastructure-for-ai",
    "title": "Lecture 1",
    "section": "Big Data Infrastructure for AI",
    "text": "Big Data Infrastructure for AI\nData Lakes & Warehouses for AI\nTraditional Use Cases: * Business intelligence * Analytics & reporting * Historical data storage\nModern AI Use Cases: * Training data repositories * Vector embeddings storage * RAG (Retrieval-Augmented Generation) context * Fine-tuning datasets * Evaluation & benchmark data"
  },
  {
    "objectID": "slides/01-slides.html#rag-context-engineering",
    "href": "slides/01-slides.html#rag-context-engineering",
    "title": "Lecture 1",
    "section": "RAG & Context Engineering",
    "text": "RAG & Context Engineering\nThe New Data Pipeline\nRaw Data → Data Lake → Processing → Vector DB → LLM Context\nKey Components: * Data Lakes (S3, Azure Data Lake): Store massive unstructured data * Data Warehouses (Snowflake, BigQuery): Structured data for context * Vector Databases (Pinecone, Weaviate, Qdrant): Semantic search * Embedding Models: Convert data to vectors * Orchestration (Airflow, Prefect): Manage the pipeline"
  },
  {
    "objectID": "slides/01-slides.html#mcp-servers-agentic-ai",
    "href": "slides/01-slides.html#mcp-servers-agentic-ai",
    "title": "Lecture 1",
    "section": "MCP Servers & Agentic AI",
    "text": "MCP Servers & Agentic AI\nModel Context Protocol (MCP)\nWhat is MCP? * Open protocol for connecting AI assistants to data sources * Standardized way to expose tools and data to LLMs * Enables “agentic” behavior - AI that can act autonomously\nMCP in Production\nData Warehouse → MCP Server → AI Agent → Action\nExamples: * AI agents querying Snowflake for real-time analytics * Autonomous systems updating data lakes based on predictions * Multi-agent systems coordinating through shared data contexts"
  },
  {
    "objectID": "slides/01-slides.html#data-quality-for-ai",
    "href": "slides/01-slides.html#data-quality-for-ai",
    "title": "Lecture 1",
    "section": "Data Quality for AI",
    "text": "Data Quality for AI\nWhy Data Quality Matters More Than Ever\nGarbage In, Garbage Out - Amplified: * Bad training data → Biased models * Incorrect RAG data → Hallucinations * Poor data governance → Compliance issues\nData Quality Challenges in 2025\n\nScale: Validating trillions of tokens\nDiversity: Multi-modal, multi-lingual data\nVelocity: Real-time data for online learning\nVeracity: Detecting AI-generated synthetic data"
  },
  {
    "objectID": "slides/01-slides.html#real-world-big-data-ai-examples",
    "href": "slides/01-slides.html#real-world-big-data-ai-examples",
    "title": "Lecture 1",
    "section": "Real-World Big Data + AI Examples",
    "text": "Real-World Big Data + AI Examples\nNetflix\n\nData Scale: 260+ million subscribers generating 100+ billion events/day\nAI Use: Personalization, content recommendations, thumbnail generation\nStack: S3 → Spark → Iceberg → ML models → Real-time serving\n\nUber\n\nData Scale: 35+ million trips per day, petabytes of location data\nAI Use: ETA prediction, surge pricing, driver-rider matching\nStack: Kafka → Spark Streaming → Feature Store → ML Platform\n\nOpenAI\n\nData Scale: Trillions of tokens for training, millions of queries/day\nAI Use: GPT models, DALL-E, embeddings\nStack: Distributed training → Vector DBs → Inference clusters"
  },
  {
    "objectID": "slides/01-slides.html#the-future-big-data-ai-convergence",
    "href": "slides/01-slides.html#the-future-big-data-ai-convergence",
    "title": "Lecture 1",
    "section": "The Future: Big Data + AI Convergence",
    "text": "The Future: Big Data + AI Convergence\nEmerging Trends (2025-2027)\nUnified Platforms: * Data lakes becoming “AI lakes” * Integrated vector + relational databases * One-stop shops for data + AI (Databricks, Snowflake Cortex)\nEdge Computing + AI: * Processing at the data source * Federated learning across devices * 5G enabling real-time edge AI\nSynthetic Data: * AI generating training data for AI * Privacy-preserving synthetic datasets * Infinite data generation loops"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes",
    "href": "slides/01-slides.html#relative-data-sizes",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-1",
    "href": "slides/01-slides.html#relative-data-sizes-1",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-2",
    "href": "slides/01-slides.html#relative-data-sizes-2",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-3",
    "href": "slides/01-slides.html#relative-data-sizes-3",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-4",
    "href": "slides/01-slides.html#relative-data-sizes-4",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-5",
    "href": "slides/01-slides.html#relative-data-sizes-5",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#what-youll-learn-in-this-course",
    "href": "slides/01-slides.html#what-youll-learn-in-this-course",
    "title": "Lecture 1",
    "section": "What You’ll Learn in This Course",
    "text": "What You’ll Learn in This Course\nBig Data-driven Analytics\nQuery Engines: * DuckDB - In-process analytical database * Polars - Lightning-fast DataFrame library\n* Spark - Distributed processing at scale\nData Warehouses & Lakes: * Snowflake - Cloud-native data warehouse * Athena - Serverless SQL on S3\nOrchestration: * Serverless with AWS Lambda * Airflow for pipeline management (?)"
  },
  {
    "objectID": "slides/01-slides.html#data-types",
    "href": "slides/01-slides.html#data-types",
    "title": "Lecture 1",
    "section": "Data Types",
    "text": "Data Types\n\nStructured\nUnstructured\nNatural language\nMachine-generated\nGraph-based\nAudio, video, and images\nStreaming"
  },
  {
    "objectID": "slides/01-slides.html#big-data-vs.-small-data",
    "href": "slides/01-slides.html#big-data-vs.-small-data",
    "title": "Lecture 1",
    "section": "Big Data vs. Small Data",
    "text": "Big Data vs. Small Data\n\n\n\n\n\n\n\n\n\nSmall Data is usually…\nOn the other hand, Big Data…\n\n\n\n\nGoals\ngathered for a specific goal\nmay have a goal in mind when it’s first started, but things can evolve or take unexpected directions\n\n\nLocation\nin one place, and often in a single computer file\ncan be in multiple files in multiple servers on computers in different geographic locations\n\n\nStructure/Contents\nhighly structured like an Excel spreadsheet, and it’s got rows and columns of data\ncan be unstructured, it can have many formats in files involved across disciplines, and may link to other resources\n\n\nPreparation\nprepared by the end user for their own purposes\nis often prepared by one group of people, analyzed by a second group of people, and then used by a third group of people, and they may have different purposes, and they may have different disciplines"
  },
  {
    "objectID": "slides/01-slides.html#big-data-vs.-small-data-1",
    "href": "slides/01-slides.html#big-data-vs.-small-data-1",
    "title": "Lecture 1",
    "section": "Big Data vs. Small Data",
    "text": "Big Data vs. Small Data\n\n\n\n\n\n\n\n\n\nSmall Data is usually…\nOn the other hand, Big Data…\n\n\n\n\nLongevity\nkept for a specific amount of time after the project is over because there’s a clear ending point. In the academic world it’s maybe five or seven years and then you can throw it away\ncontains data that must be stored in perpetuity. Many big data projects extend into the past and future\n\n\nMeasurements\nmeasured with a single protocol using set units and it’s usually done at the same time\nis collected and measured using many sources, protocols, units, etc\n\n\nReproducibility\nbe reproduced in their entirety if something goes wrong in the process\nreplication is seldom feasible\n\n\nStakes\nif things go wrong the costs are limited, it’s not an enormous problem\ncan have high costs of failure in terms of money, time and labor\n\n\nAccess\nidentified by a location specified in a row/column\nunless it is exceptionally well designed, the organization can be inscrutable\n\n\nAnalysis\nanalyzed together, all at once\nis ordinarily analyzed in incremental steps"
  },
  {
    "objectID": "slides/01-slides.html#traditional-data-analysis-tools-like-r-and-python-are-single-threaded",
    "href": "slides/01-slides.html#traditional-data-analysis-tools-like-r-and-python-are-single-threaded",
    "title": "Lecture 1",
    "section": "Traditional data analysis tools like R and Python are single threaded",
    "text": "Traditional data analysis tools like R and Python are single threaded"
  },
  {
    "objectID": "slides/01-slides.html#tools-at-a-glance",
    "href": "slides/01-slides.html#tools-at-a-glance",
    "title": "Lecture 1",
    "section": "Tools at-a-glance",
    "text": "Tools at-a-glance\n\n\nLanguages, libraries, and projects\n\nPython\n\npandas\npolars\nPySpark\nduckdb\ndask\nray\n\nApache Arrow\nApache Spark\nSQL\n\nWe’ll talk briefly about Apache Hadoop today but we will not cover it in this course.\n\nCloud Services\n\nAmazon Web Services (AWS)\n\nAWS Sagemaker\nAmazon S3\n\nAzure\n\nAzure Blob\nAzure Machine Learning\n\n\nOther:\n\nAWS Elastic MapReduce (EMR)"
  },
  {
    "objectID": "slides/01-slides.html#additional-links-of-interest",
    "href": "slides/01-slides.html#additional-links-of-interest",
    "title": "Lecture 1",
    "section": "Additional links of interest",
    "text": "Additional links of interest\n\nMatt Turck’s Machine Learning, Artificial Intelligence & Data Landscape (MAD)\n\nArticle\nInteractive Landscape\n\nIs there life after Hadoop?\n10 Best Big Data Tools for 2023"
  },
  {
    "objectID": "slides/01-slides.html#difference-between-data-scientist-and-data-engineer",
    "href": "slides/01-slides.html#difference-between-data-scientist-and-data-engineer",
    "title": "Lecture 1",
    "section": "Difference between Data Scientist and Data Engineer",
    "text": "Difference between Data Scientist and Data Engineer\nIn this course, you’ll be doing a little data engineering!"
  },
  {
    "objectID": "slides/01-slides.html#responsibilities",
    "href": "slides/01-slides.html#responsibilities",
    "title": "Lecture 1",
    "section": "Responsibilities",
    "text": "Responsibilities"
  },
  {
    "objectID": "slides/01-slides.html#data-engineering-falls-into-levels-2-and-3-primarily",
    "href": "slides/01-slides.html#data-engineering-falls-into-levels-2-and-3-primarily",
    "title": "Lecture 1",
    "section": "Data Engineering falls into levels 2 and 3 primarily",
    "text": "Data Engineering falls into levels 2 and 3 primarily"
  },
  {
    "objectID": "slides/01-slides.html#as-an-analystdata-scientist-you-really-need-both",
    "href": "slides/01-slides.html#as-an-analystdata-scientist-you-really-need-both",
    "title": "Lecture 1",
    "section": "As an analyst/data scientist, you really need both",
    "text": "As an analyst/data scientist, you really need both"
  },
  {
    "objectID": "slides/01-slides.html#architecture",
    "href": "slides/01-slides.html#architecture",
    "title": "Lecture 1",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "slides/01-slides.html#storage",
    "href": "slides/01-slides.html#storage",
    "title": "Lecture 1",
    "section": "Storage",
    "text": "Storage"
  },
  {
    "objectID": "slides/01-slides.html#source-control",
    "href": "slides/01-slides.html#source-control",
    "title": "Lecture 1",
    "section": "Source control",
    "text": "Source control"
  },
  {
    "objectID": "slides/01-slides.html#orchestration",
    "href": "slides/01-slides.html#orchestration",
    "title": "Lecture 1",
    "section": "Orchestration",
    "text": "Orchestration"
  },
  {
    "objectID": "slides/01-slides.html#processing",
    "href": "slides/01-slides.html#processing",
    "title": "Lecture 1",
    "section": "Processing",
    "text": "Processing"
  },
  {
    "objectID": "slides/01-slides.html#analytics",
    "href": "slides/01-slides.html#analytics",
    "title": "Lecture 1",
    "section": "Analytics",
    "text": "Analytics"
  },
  {
    "objectID": "slides/01-slides.html#machine-learning",
    "href": "slides/01-slides.html#machine-learning",
    "title": "Lecture 1",
    "section": "Machine Learning",
    "text": "Machine Learning"
  },
  {
    "objectID": "slides/01-slides.html#governance",
    "href": "slides/01-slides.html#governance",
    "title": "Lecture 1",
    "section": "Governance",
    "text": "Governance"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-1",
    "href": "slides/01-slides.html#linux-command-line-1",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nTerminal\n\n\nTerminal access was THE ONLY way to do programming\nNo GUIs! No Spyder, Jupyter, RStudio, etc.\nCoding is still more powerful than graphical interfaces for complex jobs\nCoding makes work repeatable"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-2",
    "href": "slides/01-slides.html#linux-command-line-2",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nBASH\n\n\nCreated in 1989 by Brian Fox\nBrian Fox also built the first online interactive banking software\nBASH is a command processor\nConnection between you and the machine language and hardware"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-3",
    "href": "slides/01-slides.html#linux-command-line-3",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nThe Prompt\nusername@hostname:current_directory $\nWhat do we learn from the prompt?\n\nWho you are - username\nThe machine where your code is running - hostname\nThe directory where your code is running - current_directory\nThe shell type - $ - this symbol means BASH"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-4",
    "href": "slides/01-slides.html#linux-command-line-4",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nSyntax\nCOMMAND -F --FLAG * COMMAND is the program * Everything after that are arguments * F is a single letter flag * FLAG is a single word or words connected by dashes flag. A space breaks things into a new argument. + Sometimes single letter and long form flags (e.g. F and FLAG) can refer to the same argument\nCOMMAND -F --FILE file1\nHere we pass an text argument “file1” into the FILE flag\nThe -h flag is usually to get help. You can also run the man command and pass the name of the program as the argument to get the help page.\nLet’s try basic commands:\n\ndate to get the current date\nwhoami to get your user name\necho \"Hello World\" to print to the console"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-5",
    "href": "slides/01-slides.html#linux-command-line-5",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nExamining Files\nFind out your Present Working Directory pwd\nExamine the contents of files and folders using the ls command\nMake new files from scratch using the touch command\nGlobbing - how to select files in a general way\n\n\\* for wild card any number of characters\n\\? for wild card for a single character\n[] for one of many character options\n! for exclusion\nspecial options [:alpha:], [:alnum:], [:digit:], [:lower:], [:upper:]\n\nReference material Reference material: Shell Lesson 1,2,4,5"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-6",
    "href": "slides/01-slides.html#linux-command-line-6",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nNavigating Directories\nKnowing where your terminal is executing code ensures you are working with the right inputs and making the right outputs.\nUse the command pwd to determine the Present Working Directory.\nLet’s say you need to change to a folder called “git-repo”. To change directories you can use a command like cd git-repo.\n\n. refers to the current directory, such as ./git-repo\n.. can be used to move up one folder, use cd .., and can be combined to move up multiple levels ../../my_folder\n/ is the root of the Linux OS, where there are core folders, such as system, users, etc.\n~ is the home directory. Move to folders referenced relative to this path by including it at the start of your path, for example ~/projects.\n\nTo view the structure of directories from your present working directory, use the tree command\nReference link"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-7",
    "href": "slides/01-slides.html#linux-command-line-7",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nInteracting with Files\nNow that we know how to navigate through directories, we need to learn the commands for interacting with files\n\nmv to move files from one location to another\n\nCan use file globbing here - ?, *, [], …\n\ncp to copy files instead of moving\n\nCan use file globbing here - ?, *, [], …\n\nmkdir to make a directory\nrm to remove files\nrmdir to remove directories\nrm -rf to blast everything! WARNING!!! DO NOT USE UNLESS YOU KNOW WHAT YOU ARE DOING"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-8",
    "href": "slides/01-slides.html#linux-command-line-8",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nUsing BASH for Data Exploration\nCommands:\n\nhead FILENAME / tail FILENAME - glimpsing the first / last few rows of data\nmore FILENAME / less FILENAME - viewing the data with basic up / (up & down) controls\ncat FILENAME - print entire file contents into terminal\nvim FILENAME - open (or edit!) the file in vim editor\ngrep FILENAME - search for lines within a file that match a regex expression\nwc FILENAME - count the number of lines (-l flag) or number of words (-w flag)\n\nReference link Reference material: Text Lesson 8,9,15,16"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-9",
    "href": "slides/01-slides.html#linux-command-line-9",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nPipes and Arrows\n\n| sends the stdout to another command (is the most powerful symbol in BASH!)\n&gt; sends stdout to a file and overwrites anything that was there before\n&gt;&gt; appends the stdout to the end of a file (or starts a new file from scratch if one does not exist yet)\n&lt; sends stdin into the command on the left\n\nTo-dos:\n\necho Hello World\nCounting rows of data with certain attributes\n\nReference material: Text Lesson 1,2,3,4,5"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-10",
    "href": "slides/01-slides.html#linux-command-line-10",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nAlias and User Files\n.bashrc is where your shell settings are located\nIf we wanted a shortcut to find out the number of our running processes, we would write a commmand like whoami | xargs ps -u | wc -l.\nWe don’t want to write out this full command every time! Let’s make an alias.\nalias alias_name=\"command_to_run\"\nalias nproc=\"whoami | xargs ps -u | wc -l\"\nNow we need to put this alias into the .bashrc\nalias nproc=\"whoami | xargs ps -u | wc -l\" &gt;&gt; ~/.bashrc\nWhat happened??\necho alias nproc=\"whoami | xargs ps -u | wc -l\" &gt;&gt; ~/.bashrc\nYour commands get saved in ~/.bash_history"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-11",
    "href": "slides/01-slides.html#linux-command-line-11",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nProcess Managment\nUse the command ps to see your running processes.\nUse the command top or even better htop to see all the running processes on the machine.\nInstall the program htop using the command sudo yum install htop -y\nFind the process ID (PID) so you can kill a broken process.\nUse the command kill [PID NUM] to signal the process to terminate. If things get really bad, then use the command kill -9 [PID NUM]\nTo kill a command in the terminal window it is running in, try using Ctrl + C or Ctrl + /\nRun the cat command on its own to let it stay open. Now open a new terminal to examine the processes and find the cat process.\nReference material: Text Lesson 1,2,3,7,9,10\nTry playing a Linux game!\nhttps://gitlab.com/slackermedia/bashcrawl is a game to help you practice your navigation and file access skills. Click on the binder link in this repo to launch a jupyter lab session and explore!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Big Data Analytics",
    "section": "",
    "text": "NoteCourse Information\n\n\n\nCourse Code: DATS 6450.13\nCredits: 3\nSchedule: Thursdays 6:10 PM - 8:40 PM\nLocation: Phillips 416"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Applied Big Data Analytics",
    "section": "Course Description",
    "text": "Course Description\nApplied Big Data Analytics (DATS 6450.13) teaches practical, hands‑on methods for building scalable analytics pipelines that start on a single machine and scale to distributed clusters. Students learn to develop local analytical workflows with DuckDB, translate and scale them with Apache Spark, and evaluate performance tradeoffs through comparative benchmarking, query tuning, partitioning and memory strategies. The course covers modern tooling (Polars, Ray, RAPIDS), Spark SQL/DataFrame APIs, Spark NLP and MLlib, and efficient visualization of very large datasets (Datashader), with emphasis on reproducible end‑to‑end workflows and a final project demonstrating design and performance decisions."
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "DATS 6450.13",
    "section": "",
    "text": "Course overview, big data concepts, cloud computing and evolution of cloud technologies.",
    "crumbs": [
      "Home",
      "Course content",
      "1. Course overview"
    ]
  },
  {
    "objectID": "content/01-content.html#about-todays-class",
    "href": "content/01-content.html#about-todays-class",
    "title": "DATS 6450.13",
    "section": "",
    "text": "Course overview, big data concepts, cloud computing and evolution of cloud technologies.",
    "crumbs": [
      "Home",
      "Course content",
      "1. Course overview"
    ]
  },
  {
    "objectID": "content/01-content.html#readings",
    "href": "content/01-content.html#readings",
    "title": "DATS 6450.13",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture",
    "crumbs": [
      "Home",
      "Course content",
      "1. Course overview"
    ]
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "DATS 6450.13",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Home",
      "Course content",
      "1. Course overview"
    ]
  },
  {
    "objectID": "resources/uv.html",
    "href": "resources/uv.html",
    "title": "Using uv to manage Python environments",
    "section": "",
    "text": "uv is an extremely fast Python package and project manager. uv can both create virtual environments directly and manage them implicitly per project.",
    "crumbs": [
      "Home",
      "Resources",
      "Using uv to manage Python environments"
    ]
  },
  {
    "objectID": "resources/uv.html#install-uv",
    "href": "resources/uv.html#install-uv",
    "title": "Using uv to manage Python environments",
    "section": "1. Install uv",
    "text": "1. Install uv\n\nmacOS / Linux (shell):\n\nRun:\n\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n\nThis downloads the latest standalone uv binary and adds it to your PATH (usually ~/.local/bin).\n\nWindows (PowerShell):\n\nRun:\n\nirm https://astral.sh/uv/install.ps1 | iex s\n\nThis installs uv as a standalone executable similarly to the Unix script.\n\nVerify installation:\n\nRun uv --version to confirm that the command is available.",
    "crumbs": [
      "Home",
      "Resources",
      "Using uv to manage Python environments"
    ]
  },
  {
    "objectID": "resources/uv.html#quick-mental-model",
    "href": "resources/uv.html#quick-mental-model",
    "title": "Using uv to manage Python environments",
    "section": "2. Quick mental model",
    "text": "2. Quick mental model\n\nuv is an all‑in‑one tool that can:\n\nCreate and manage virtual environments (uv venv, uv run). docs.astral\nInstall and resolve packages with a pip‑compatible interface (uv add, uv pip). docs.astral\nManage Python versions if needed (uv python, uv venv --python ...). docs.astral\n\nYou can either:\n\nUse explicit virtualenv commands (uv venv, then uv pip install ...), or\nLet uv manage a per‑project environment implicitly via uv init and uv add. aeturrell.github",
    "crumbs": [
      "Home",
      "Resources",
      "Using uv to manage Python environments"
    ]
  },
  {
    "objectID": "resources/uv.html#explicit-classic-venv-workflow-with-uv",
    "href": "resources/uv.html#explicit-classic-venv-workflow-with-uv",
    "title": "Using uv to manage Python environments",
    "section": "3. Explicit: “classic” venv workflow with uv",
    "text": "3. Explicit: “classic” venv workflow with uv\nThis mirrors the traditional python -m venv + pip model but faster and more ergonomic. docs.astral\n\nCreate a virtual environment in the current directory:\n\nuv venv\n\nBy default, this creates .venv using the system’s default Python or a uv‑managed Python. docs.astral\nTo specify Python explicitly:\n\nuv venv --python 3.11\n\nIf 3.11 is not installed, uv can download and use it automatically. docs.astral\n\n\nActivate the environment:\n\nmacOS / Linux (bash/zsh/fish-like shells):\n\nsource .venv/bin/activate docs.astral\n\nWindows (PowerShell):\n\n.venv\\Scripts\\Activate.ps1 fastapi.tiangolo\n\n\nInstall packages into that environment:\n\nuv pip install numpy pandas\n\nThis is a drop‑in replacement for pip install ... but uses uv’s fast resolver and installer. docs.astral\n\nRun Python inside the environment:\n\nOnce activated, use python, ipython, or any tooling as usual. fastapi.tiangolo\n\nDeactivate:\n\nRun deactivate when done, just like with any venv. fastapi.tiangolo",
    "crumbs": [
      "Home",
      "Resources",
      "Using uv to manage Python environments"
    ]
  },
  {
    "objectID": "resources/uv.html#projectcentric-uv-manages-the-env-for-you",
    "href": "resources/uv.html#projectcentric-uv-manages-the-env-for-you",
    "title": "Using uv to manage Python environments",
    "section": "4. Project‑centric: uv manages the env for you",
    "text": "4. Project‑centric: uv manages the env for you\nThis is often simpler for teaching, because the same commands set up both project metadata and environments. aeturrell.github\n\nInitialize a new project directory:\n\nCreate and enter a folder, e.g.:\n\nmkdir my-project && cd my-project\n\nRun:\n\nuv init\n\nThis creates:\n\nA pyproject.toml file with basic project metadata and dependencies section. aeturrell.github\nA dedicated virtual environment for this project (usually .venv). aeturrell.github\n\n\nAdd dependencies:\n\nuv add numpy pandas\n\nThe first call both creates/uses the project env and installs these packages, updating pyproject.toml and a lock file. datacamp\n\nRun code with the project environment:\n\nuv run python main.py\n\nuv run pytest\nuv run ensures that the command uses the project’s virtual environment, even without manually activating it. docs.astral\n\nSync an existing project (e.g., after pulling from git):\n\nIf pyproject.toml and lock file exist, run:\n\nuv sync\n\nThis creates (or updates) the project env to match the lock file. datacamp",
    "crumbs": [
      "Home",
      "Resources",
      "Using uv to manage Python environments"
    ]
  },
  {
    "objectID": "resources/uv.html#teachingfriendly-patterns-and-tips",
    "href": "resources/uv.html#teachingfriendly-patterns-and-tips",
    "title": "Using uv to manage Python environments",
    "section": "5. Teaching‑friendly patterns and tips",
    "text": "5. Teaching‑friendly patterns and tips\n\nOne‑liner for students to get started in a new repo:\n\nuv sync && uv run python main.py\n\nThis handles “create env + install deps + run code” in two commands. datacamp\n\nSwitching Python versions in a project:\n\nuv python pin 3.11 writes .python-version with 3.11, then:\n\nuv sync recreates/updates the env with that version. github\n\n\nUsing tools without polluting the project env:\n\nuvx ruff . or uv tool run black app.py run tools in ephemeral or tool‑specific environments instead of the project venv. docs.astral\n\nGuideline to give students:\n\n“Each project lives in its own folder. In that folder, run uv init once, then always use uv add, uv sync, and uv run from there.” aeturrell.github",
    "crumbs": [
      "Home",
      "Resources",
      "Using uv to manage Python environments"
    ]
  },
  {
    "objectID": "labs/02-labs.html",
    "href": "labs/02-labs.html",
    "title": "Lab 2",
    "section": "",
    "text": "GitHub Classroom Link"
  },
  {
    "objectID": "labs/02-labs.html#github-classroom",
    "href": "labs/02-labs.html#github-classroom",
    "title": "Lab 2",
    "section": "",
    "text": "GitHub Classroom Link"
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Labs",
    "section": "",
    "text": "Lab\nThe lab for the class would involve a hands-on coding assignment provided through GitHub Classroom. You will start the lab in-class, myself and the TAs would be helping you with any questions with the lab and then you would need to turn in the lab by checking in your code and results in the GitHub repo (you will have until next class for this, but usually you would be able to do this much sooner)."
  },
  {
    "objectID": "assignments/a02-parallelization.html",
    "href": "assignments/a02-parallelization.html",
    "title": "Assignment 2: Parallelization",
    "section": "",
    "text": "Assignment 2 is available on Github Classroom and is due February 1, 11:59pm"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Lectures and labs",
    "section": "",
    "text": "Each class is divided into two parts: a lecture followed by a lab. Some class optionally have a reading component also that needs to be completed before class.\n\nLecture\nDuring the lecture, I will be going over the slides for that lecture, this would usually include a quick review of the previous class, feedback on a recently graded assignment and then cover the topic of the day.\nThe lecture would usually last about 90 minutes, followed by 5 to 10 minutes break.\n\n\nLab\nThe lab for the class would involve a hands-on coding assignment provided through GitHub Classroom. You will start the lab in-class, myself and the TAs would be helping you with any questions with the lab and then you would need to turn in the lab by checking in your code and results in the GitHub repo (you will have until next class for this, but usually you would be able to do this much sooner)."
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "DATS 6450.13",
    "section": "",
    "text": "Scaling up on a single machine with Python multiprocessing.",
    "crumbs": [
      "Home",
      "Course content",
      "2. Parallel processing"
    ]
  },
  {
    "objectID": "content/02-content.html#about-todays-class",
    "href": "content/02-content.html#about-todays-class",
    "title": "DATS 6450.13",
    "section": "",
    "text": "Scaling up on a single machine with Python multiprocessing.",
    "crumbs": [
      "Home",
      "Course content",
      "2. Parallel processing"
    ]
  },
  {
    "objectID": "content/02-content.html#readings",
    "href": "content/02-content.html#readings",
    "title": "DATS 6450.13",
    "section": "Readings",
    "text": "Readings\nReadings for this lecture (to be completed before this class):\nWolohan Ch. 1,2 (available on Blackboard)",
    "crumbs": [
      "Home",
      "Course content",
      "2. Parallel processing"
    ]
  },
  {
    "objectID": "content/02-content.html#slides",
    "href": "content/02-content.html#slides",
    "title": "DATS 6450.13",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Home",
      "Course content",
      "2. Parallel processing"
    ]
  },
  {
    "objectID": "content/02-content.html#lab",
    "href": "content/02-content.html#lab",
    "title": "DATS 6450.13",
    "section": "Lab",
    "text": "Lab\nParallelization with Python and multiprocessing. The lab for today’s lesson is available online as an HTML file.",
    "crumbs": [
      "Home",
      "Course content",
      "2. Parallel processing"
    ]
  },
  {
    "objectID": "content/02-content.html#assignment",
    "href": "content/02-content.html#assignment",
    "title": "DATS 6450.13",
    "section": "Assignment",
    "text": "Assignment\nThe assignment for this week is available here",
    "crumbs": [
      "Home",
      "Course content",
      "2. Parallel processing"
    ]
  },
  {
    "objectID": "slides/02-slides.html#agenda-and-goals-for-today",
    "href": "slides/02-slides.html#agenda-and-goals-for-today",
    "title": "Lecture 2",
    "section": "Agenda and Goals for Today",
    "text": "Agenda and Goals for Today\n\nScaling up and scaling out for AI data pipelines\nParallelization fundamentals for data processing\nMap and Reduce functions\nParallel data preparation for AI/ML workloads\nDistributed data processing frameworks (Spark, Ray, Dask)\nLab Preview: Parallelization with Python\n\nUse the multiprocessing module\nImplement synchronous and asynchronous processing\nParallel data preprocessing examples\n\nHomework Preview: Parallelization with Python\n\nParallel data processing\nBuilding scalable data pipelines"
  },
  {
    "objectID": "slides/02-slides.html#looking-back",
    "href": "slides/02-slides.html#looking-back",
    "title": "Lecture 2",
    "section": "Looking back",
    "text": "Looking back\n\nDue date reminders:\n\nAssignment 2: January 30\nLab 3: January 30"
  },
  {
    "objectID": "slides/02-slides.html#glossary",
    "href": "slides/02-slides.html#glossary",
    "title": "Lecture 2",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nLocal\nYour current workstation (laptop, desktop, etc.), wherever you start the terminal/console application.\n\n\nRemote\nAny machine you connect to via ssh or other means.\n\n\nEC2\nSingle virtual machine in the cloud where you can run computation (ephemeral)\n\n\nSageMaker\nIntegrated Developer Environment where you can conduct data science on single machines or distributed training\n\n\nGPU\nGraphics Processing Unit - specialized hardware for parallel computation, essential for AI/ML\n\n\nTPU\nTensor Processing Unit - Google’s custom AI accelerator chips\n\n\nEphemeral\nLasting for a short time - any machine that will get turned off or place you will lose data\n\n\nPersistent\nLasting for a long time - any environment where your work is NOT lost when the timer goes off"
  },
  {
    "objectID": "slides/02-slides.html#typical-real-world-scenarios",
    "href": "slides/02-slides.html#typical-real-world-scenarios",
    "title": "Lecture 2",
    "section": "Typical real world scenarios",
    "text": "Typical real world scenarios\n\nYou need to prepare training data for LLMs by cleaning and deduplicating 100TB of web-scraped text\nYou are building a RAG system that requires embedding and indexing millions of documents in parallel\nYou need to extract structured data from millions of PDFs using vision models for document AI\nYou are preprocessing multimodal datasets with billions of image-text pairs for foundation model training\nYou need to run quality filtering on petabytes of Common Crawl data for training dataset curation\nYou are generating synthetic training data using LLMs to augment limited real-world datasets\nYou need to transform and tokenize text corpora across 100+ languages for multilingual AI\nYou are building real-time data pipelines that process streaming data for online learning systems"
  },
  {
    "objectID": "slides/02-slides.html#data-for-ai-parallel-scenarios",
    "href": "slides/02-slides.html#data-for-ai-parallel-scenarios",
    "title": "Lecture 2",
    "section": "Data-for-AI Parallel Scenarios",
    "text": "Data-for-AI Parallel Scenarios\n\nWeb-scale data processing: Filtering 45TB of Common Crawl data for high-quality training examples\nEmbedding generation: Creating vector embeddings for 100M documents using sentence transformers\nData deduplication: Finding near-duplicates in billion-scale datasets using MinHash and LSH\nSynthetic data generation: Using LLMs to generate millions of instruction-following examples\nData quality assessment: Running parallel quality checks on training data (toxicity, bias, factuality)\nFeature extraction: Extracting features from millions of images, videos, or audio files for AI training"
  },
  {
    "objectID": "slides/02-slides.html#linear-vs.-parallel",
    "href": "slides/02-slides.html#linear-vs.-parallel",
    "title": "Lecture 2",
    "section": "Linear vs. Parallel",
    "text": "Linear vs. Parallel\n\n\nLinear/Sequential\n\nA program starts to run\nThe program issues an instruction\nThe instruction is executed\nSteps 2 and 3 are repeated\nThe program finishes running\n\n\nParallel\n\nA program starts to run\nThe program divides up the work into chunks of instructions and data\nEach chunk of work is executed independently\nThe chunks of work are reassembled\nThe program finishes running"
  },
  {
    "objectID": "slides/02-slides.html#linear-vs.-parallel-1",
    "href": "slides/02-slides.html#linear-vs.-parallel-1",
    "title": "Lecture 2",
    "section": "Linear vs. Parallel",
    "text": "Linear vs. Parallel"
  },
  {
    "objectID": "slides/02-slides.html#linear-vs.-parallel-2",
    "href": "slides/02-slides.html#linear-vs.-parallel-2",
    "title": "Lecture 2",
    "section": "Linear vs. Parallel",
    "text": "Linear vs. Parallel\nFrom a data engineering for AI perspective\n\n\nLinear\n\nThe data remains monolithic\nProcedures act on the data sequentially\n\nEach procedure has to complete before the next procedure can start\n\nYou can think of this as a single pipeline\n\n\nParallel\n\nThe data can be split up into chunks\nThe same procedures can be run on each chunk at the same time\nOr, independent procedures can run on different chunks at the same time\nNeed to bring things back together at the end\n\n\n\nWhat are some examples of linear and parallel data science workflows?"
  },
  {
    "objectID": "slides/02-slides.html#embarrasingly-parallel-in-data-for-ai",
    "href": "slides/02-slides.html#embarrasingly-parallel-in-data-for-ai",
    "title": "Lecture 2",
    "section": "Embarrasingly Parallel in Data for AI",
    "text": "Embarrasingly Parallel in Data for AI\nIt’s easy to speed things up when:\n\nProcessing millions of documents independently for text extraction\nGenerating embeddings for each document in a corpus\nApplying the same preprocessing to each image in a dataset\nRunning quality filters on individual data samples\nTokenizing text files independently\nConverting file formats (PDF to text, audio to features)\nValidating and cleaning individual records\n\nJust run multiple data processing tasks at the same time"
  },
  {
    "objectID": "slides/02-slides.html#modern-data-processing-tools-for-ai",
    "href": "slides/02-slides.html#modern-data-processing-tools-for-ai",
    "title": "Lecture 2",
    "section": "Modern Data Processing Tools for AI:",
    "text": "Modern Data Processing Tools for AI:\n\nApache Spark: Distributed data processing at scale\nRay Data: Scalable data preprocessing for ML\nDask: Parallel computing with Python APIs\nPolars: Fast DataFrame library with parallel execution"
  },
  {
    "objectID": "slides/02-slides.html#embarrasingly-parallel",
    "href": "slides/02-slides.html#embarrasingly-parallel",
    "title": "Lecture 2",
    "section": "Embarrasingly Parallel",
    "text": "Embarrasingly Parallel\nThe concept is based on the old middle/high school math problem:\n\nIf 5 people can shovel a parking lot in 6 hours, how long will it take 100 people to shovel the same parking lot?\n\nBasic idea is that many hands (cores/instances) make lighter (faster/more efficient) work of the same problem, as long as the effort can be split up appropriately into nearly equal parcels\n\n\nThe classical answer to the problem is 18 minutes"
  },
  {
    "objectID": "slides/02-slides.html#embarassingly-parallel",
    "href": "slides/02-slides.html#embarassingly-parallel",
    "title": "Lecture 2",
    "section": "Embarassingly parallel",
    "text": "Embarassingly parallel\n\nIf you can truly split up your problem into multiple independent parts, then you can often get linear speedups with the number of parallel components (to a limit)\n\nThe more cores you use and the more you parallelize, the more you incur communication overhead and decrease available RAM, so the speedup is almost certainly sub-linear, i.e. for a 4-core machine you’ll probably get a 3-4x speedup, but rarely a full 4x speedup1\n\nThe question often is, which part of your problem is embarassingly parallel?\nAmdahl’s law (which we’ll see in a few slides) shows how parallelization can benefit overall if a large proportion of the problem is parallelizable\nIt’s not all milk and honey. Setting up, programming, evaluating, debugging parallel computations requires better infrastructure and more expertise.\n\nGorelick & Ozsvald, 2020. High Performance Python, O’Reilly"
  },
  {
    "objectID": "slides/02-slides.html#some-limitations-in-data-processing-for-ai",
    "href": "slides/02-slides.html#some-limitations-in-data-processing-for-ai",
    "title": "Lecture 2",
    "section": "Some limitations in Data Processing for AI",
    "text": "Some limitations in Data Processing for AI\nYou can get speedups by parallelizing data operations, but\n\nData skew: Uneven distribution of data can create bottlenecks (e.g., one partition has 90% of data)\nI/O bound operations: Reading/writing to storage can limit parallelization benefits\nMemory overhead: Loading large models (like BERT) on each worker for feature extraction\nDeduplication challenges: Some operations like global deduplication require data shuffling\nOrdering dependencies: Maintaining sequence order in time-series or conversational data\n\n\nData-Specific Challenges\n\nData quality issues: Bad data in parallel pipelines can be hard to trace\nResource management: Large models (BERT, GPT) need significant memory per worker\nConsistency: Ensuring all workers use same preprocessing versions\n\n\n\nMaking sure that we can get back all the pieces needs monitoring\n\nFailure tolerance and protections (Spark checkpointing)\nProper collection and aggregation of the processed data\nData validation after parallel processing"
  },
  {
    "objectID": "slides/02-slides.html#amdahls-law-in-data-processing",
    "href": "slides/02-slides.html#amdahls-law-in-data-processing",
    "title": "Lecture 2",
    "section": "Amdahl’s Law in Data Processing",
    "text": "Amdahl’s Law in Data Processing\n\n\n\n\n\n\\[\n\\lim_{s\\rightarrow\\infty} S_{latency} = \\frac{1}{1-p}\n\\]\nData Pipeline Examples:\n\nIf 80% is parallel (processing) and 20% is sequential (I/O), max speedup = 5x\nIf 95% is parallel (embedding generation), max speedup = 20x\n\n\nReal Data Processing Speedups:\n\nDocument processing: Near-linear up to 100s of workers\nEmbedding generation: Linear with number of GPUs\nData deduplication: Sub-linear due to shuffling overhead"
  },
  {
    "objectID": "slides/02-slides.html#pros-and-cons-of-parallelization-for-ai-data",
    "href": "slides/02-slides.html#pros-and-cons-of-parallelization-for-ai-data",
    "title": "Lecture 2",
    "section": "Pros and cons of parallelization for AI Data",
    "text": "Pros and cons of parallelization for AI Data\n\nYes - Parallelize These\nData Preparation:\n\nText extraction from documents (PDFs, HTML)\nTokenization of text corpora\nImage preprocessing and augmentation\nEmbedding generation for documents\nData quality filtering and validation\nFormat conversions (audio → features)\nWeb scraping and data collection\nSynthetic data generation\n\nData Processing:\n\nBatch inference on datasets\nFeature extraction at scale\nData deduplication (local)\nParallel chunk processing\n\n\n\nNo - Keep Sequential\nOrder-Dependent:\n\nConversation threading\nTime-series preprocessing\nSequential data validation\nCumulative statistics\n\nGlobal Operations:\n\nGlobal deduplication\nCross-dataset joins\nSorting entire datasets\nComputing exact quantiles\n\nSmall Data:\n\nConfig file processing\nMetadata operations\nSingle document processing\n\n\n\n\n\n\n\n\n\nFor data operations in the “No” column, they often require global coordination or maintain strict ordering. However, many can be approximated with parallel algorithms (like approximate deduplication with locality-sensitive hashing )"
  },
  {
    "objectID": "slides/02-slides.html#pros-and-cons-of-parallelization",
    "href": "slides/02-slides.html#pros-and-cons-of-parallelization",
    "title": "Lecture 2",
    "section": "Pros and cons of parallelization",
    "text": "Pros and cons of parallelization\n\n\nPros\n\nHigher efficiency\nUsing modern infrastructure\nScalable to larger data, more complex procedures\n\nproviso procedures are embarassingly parallel\n\n\n\n\nCons\n\nHigher programming complexity\nNeed proper software infrastructure (MPI, Hadoop, etc)\nNeed to ensure right packages/modules are distributed across processors\nNeed to account for a proportion of jobs failing, and recovering from them\nHence, Hadoop/Spark and other technologies\nHigher setup cost in terms of time/expertise/money\n\n\n\n\nThere are good solutions today for most of the cons, so the pros have it and so this paradigm is widely accepted and implemented"
  },
  {
    "objectID": "slides/02-slides.html#distributed-memory-message-passing-model",
    "href": "slides/02-slides.html#distributed-memory-message-passing-model",
    "title": "Lecture 2",
    "section": "Distributed memory / Message Passing Model",
    "text": "Distributed memory / Message Passing Model"
  },
  {
    "objectID": "slides/02-slides.html#data-parallel-model",
    "href": "slides/02-slides.html#data-parallel-model",
    "title": "Lecture 2",
    "section": "Data parallel model",
    "text": "Data parallel model"
  },
  {
    "objectID": "slides/02-slides.html#hybrid-model",
    "href": "slides/02-slides.html#hybrid-model",
    "title": "Lecture 2",
    "section": "Hybrid model",
    "text": "Hybrid model"
  },
  {
    "objectID": "slides/02-slides.html#partitioning-data",
    "href": "slides/02-slides.html#partitioning-data",
    "title": "Lecture 2",
    "section": "Partitioning data",
    "text": "Partitioning data"
  },
  {
    "objectID": "slides/02-slides.html#designing-parallel-programs",
    "href": "slides/02-slides.html#designing-parallel-programs",
    "title": "Lecture 2",
    "section": "Designing parallel programs",
    "text": "Designing parallel programs\n\nData partitioning\nCommunication\nSynchronization / Orchestration\nData dependencies\nLoad balancing\nInput and Output (I/O)\nDebugging\n\n\nA lot of these components are data engineering and DevOps issues\n\n\nInfrastructures have standardized many of these and have helped data scientists implement parallel programming much more easily\n\n\nWe’ll see in the lab how the multiprocessing module in Python makes parallel processing on a machine quite easy to implement"
  },
  {
    "objectID": "slides/02-slides.html#parallel-data-processing-for-ai",
    "href": "slides/02-slides.html#parallel-data-processing-for-ai",
    "title": "Lecture 2",
    "section": "Parallel Data Processing for AI",
    "text": "Parallel Data Processing for AI\nCommon AI Data Workloads\n\nText preprocessing: Tokenization, cleaning for LLM training\nEmbedding generation: Converting documents to vectors\nData quality filtering: Removing low-quality training samples\nFormat conversion: PDF to text, audio to features\nSynthetic data generation: Using LLMs to create training data\nDeduplication: Finding and removing duplicate content\n\n\nThese operations are embarrassingly parallel - each document/sample can be processed independently"
  },
  {
    "objectID": "slides/02-slides.html#components-of-a-parallel-programming-workflow",
    "href": "slides/02-slides.html#components-of-a-parallel-programming-workflow",
    "title": "Lecture 2",
    "section": "Components of a parallel programming workflow",
    "text": "Components of a parallel programming workflow\n\nDivide the work into chunks\nWork on each chunk separately\nReassemble the work\n\nThis paradigm is often referred to as a map-reduce framework, or, more descriptively, the split-apply-combine paradigm"
  },
  {
    "objectID": "slides/02-slides.html#map-1",
    "href": "slides/02-slides.html#map-1",
    "title": "Lecture 2",
    "section": "Map",
    "text": "Map\nThe map operation is a 1-1 operation that takes each split and processes it\nThe map operation keeps the same number of objects in its output that were present in its input"
  },
  {
    "objectID": "slides/02-slides.html#map-2",
    "href": "slides/02-slides.html#map-2",
    "title": "Lecture 2",
    "section": "Map",
    "text": "Map\n\n\n\nThe operations included in a particular map can be quite complex, involving multiple steps. In fact, you can implement a pipeline of procedures within the map step to process each data object.\nThe main point is that the same operations will be run on each data object in the map implementation"
  },
  {
    "objectID": "slides/02-slides.html#map-3",
    "href": "slides/02-slides.html#map-3",
    "title": "Lecture 2",
    "section": "Map",
    "text": "Map\nSome examples of map operations are:\nTraditional Data Analytics:\n\nExtracting a standard table from online reports from multiple years\nExtracting particular records from multiple JSON objects\nTransforming data (as opposed to summarizing it)\nRun a normalization script on each transcript in a GWAS dataset\nStandardizing demographic data for each of the last 20 years against the 2000 US population"
  },
  {
    "objectID": "slides/02-slides.html#map-4",
    "href": "slides/02-slides.html#map-4",
    "title": "Lecture 2",
    "section": "Map",
    "text": "Map\nSome examples of map operations are:\nAI Data Processing (2025):\n\nText processing: Tokenizing millions of documents for LLM training\nEmbedding generation: Converting each document to a vector representation\nData extraction: Extracting text from millions of PDFs using OCR\nQuality filtering: Applying toxicity/bias filters to each text sample\nImage preprocessing: Resizing and normalizing images for vision models\nSynthetic data: Generating training examples from prompts using LLMs"
  },
  {
    "objectID": "slides/02-slides.html#map-5",
    "href": "slides/02-slides.html#map-5",
    "title": "Lecture 2",
    "section": "Map",
    "text": "Map"
  },
  {
    "objectID": "slides/02-slides.html#reduce-1",
    "href": "slides/02-slides.html#reduce-1",
    "title": "Lecture 2",
    "section": "Reduce",
    "text": "Reduce\nThe reduce operation takes multiple objects and reduces them to a (perhaps) smaller number of objects using transformations that aren’t amenable to the map paradigm.\nThese transformations are often serial/linear in nature\nThe reduce transformation is usually the last, not-so-elegant transformation needed after most of the other transformations have been efficiently handled in a parallel fashion by map"
  },
  {
    "objectID": "slides/02-slides.html#reduce-2",
    "href": "slides/02-slides.html#reduce-2",
    "title": "Lecture 2",
    "section": "Reduce",
    "text": "Reduce\nThe reduce operation requires\n\n\nAn accumulator function, that will update serially as new data is fed into it\n\nA sequence of objects to run through the accumulator function\n\nA starting value from which the accumulator function starts\n\nProgrammatically, this can be written as"
  },
  {
    "objectID": "slides/02-slides.html#reduce-3",
    "href": "slides/02-slides.html#reduce-3",
    "title": "Lecture 2",
    "section": "Reduce",
    "text": "Reduce\nThe reduce operation works serially from “left” to “right”, passing each object successively through the accumulator function.\nFor example, if we were to add successive numbers with a function called add…"
  },
  {
    "objectID": "slides/02-slides.html#reduce-4",
    "href": "slides/02-slides.html#reduce-4",
    "title": "Lecture 2",
    "section": "Reduce",
    "text": "Reduce\nSome examples:\nTraditional Analytics:\n\nFinding the common elements (intersection) of a large number of sets\nComputing a table of group-wise summaries\nFiltering\nTabulating\n\nAI Data Processing (2025):\n\nDeduplication: Finding unique documents across billions of samples\nAggregating statistics: Computing dataset quality metrics\nVocabulary building: Creating token vocabularies from text corpus\nIndex building: Merging vector embeddings into searchable indices\nData validation: Aggregating quality scores across partitions"
  },
  {
    "objectID": "slides/02-slides.html#map-reduce-1",
    "href": "slides/02-slides.html#map-reduce-1",
    "title": "Lecture 2",
    "section": "map-reduce",
    "text": "map-reduce\nCombining the map and reduce operations creates a powerful pipeline that can handle a diverse range of problems in the Big Data context"
  },
  {
    "objectID": "slides/02-slides.html#parallelization-and-map-reduce-work-hand-in-hand",
    "href": "slides/02-slides.html#parallelization-and-map-reduce-work-hand-in-hand",
    "title": "Lecture 2",
    "section": "Parallelization and map-reduce work hand-in-hand",
    "text": "Parallelization and map-reduce work hand-in-hand\n\n\n\n\nOne of the issues here is, how to split the data in a “good” manner so that the map-reduce framework works well"
  },
  {
    "objectID": "slides/02-slides.html#the-multiprocessing-module",
    "href": "slides/02-slides.html#the-multiprocessing-module",
    "title": "Lecture 2",
    "section": "The multiprocessing module",
    "text": "The multiprocessing module\n\nFocused on single-machine multicore parallelism\nFacilitates:\n\nprocess- and thread-based parallel processing\nsharing work over queues\nsharing data among processes"
  },
  {
    "objectID": "slides/02-slides.html#processes-and-threads",
    "href": "slides/02-slides.html#processes-and-threads",
    "title": "Lecture 2",
    "section": "Processes and threads",
    "text": "Processes and threads\n\nA process is an executing program, that is self-contained and has dedicated runtime and memory\nA thread is the basic unit to which the operating system allocates processor time. It is an entity within a process. A thread can execute any part of the process code, including parts currently being executed by another thread.\nA thread will often be faster to spin up and terminate than a full process\nThreads can share memory and data with each other\n\n\n\n\n\n\n\n\nPython has the Global Interpretor Lock (GIL) which only allows only one thread to interact with Python objects at a time. So the way to parallel process in Python is to do multi-processor parallelization, where we run multiple Python interpretors across multiple processes, each with its own private memory space and GIL."
  },
  {
    "objectID": "slides/02-slides.html#some-concepts-in-multiprocessing",
    "href": "slides/02-slides.html#some-concepts-in-multiprocessing",
    "title": "Lecture 2",
    "section": "Some concepts in multiprocessing1",
    "text": "Some concepts in multiprocessing1\nProcess\nA forked copy of the current process; this creates a new process identifier, and the task runs as an independent child process in the operating system\nPool\nWraps the Process into a convenient pool of workers that share a chunk of work and return an aggregated result\nGorelick & Ozsvald, 2020. High Performance Python, O’Reilly"
  },
  {
    "objectID": "slides/02-slides.html#other-methods-of-parallel-processing-in-python",
    "href": "slides/02-slides.html#other-methods-of-parallel-processing-in-python",
    "title": "Lecture 2",
    "section": "Other methods of parallel processing in Python",
    "text": "Other methods of parallel processing in Python\n\nThe joblib module\nMost scikit-learn functions have implicit parallelization baked in through the n_jobs parameter\nRay for distributed AI/ML workloads\nDask for scaling pandas/numpy operations\nApache Beam for data pipelines\n\nFor example\nfrom sklearn.ensembles import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators = 100, random_state = 124, n_jobs=-1)\nuses the joblib module to use all available processors (n_jobs=-1) to do the bootstrapping\n\n\n\n\n\nSee here for a description of parallel processing in the scikit-learn module"
  },
  {
    "objectID": "slides/02-slides.html#ai-data-processing-example",
    "href": "slides/02-slides.html#ai-data-processing-example",
    "title": "Lecture 2",
    "section": "AI Data Processing Example",
    "text": "AI Data Processing Example\nfrom multiprocessing import Pool\nfrom transformers import AutoTokenizer\n\n# Parallel tokenization for LLM training\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef process_document(text):\n    # Clean and tokenize text\n    tokens = tokenizer(text, truncation=True, max_length=512)\n    return tokens\n\n# Process millions of documents in parallel\nwith Pool(processes=32) as pool:\n    tokenized_docs = pool.map(process_document, documents)"
  },
  {
    "objectID": "slides/02-slides.html#the-need-for-async-io-in-ai-data-processing",
    "href": "slides/02-slides.html#the-need-for-async-io-in-ai-data-processing",
    "title": "Lecture 2",
    "section": "The need for Async I/O in AI Data Processing",
    "text": "The need for Async I/O in AI Data Processing\n\n\n\nWhen talking to external systems (databases, APIs, LLM services) the bottleneck is not local CPU/memory but rather the time it takes to receive a response from the external system.\nThe Async I/O model addresses this by allowing to send multiple request in parallel without having to wait for a response.\nAI Use Cases:\n\nCalling LLM APIs (OpenAI, Anthropic, Bedrock)\nFetching data from multiple sources\nParallel web scraping for training data\nDistributed vector database queries\n\nReferences: asyncio — Asynchronous I/O, Async IO in Python: A Complete Walkthrough\n\n\n\n\n\nAsync I/O"
  },
  {
    "objectID": "slides/02-slides.html#concurrency-and-parallelism-in-python3",
    "href": "slides/02-slides.html#concurrency-and-parallelism-in-python3",
    "title": "Lecture 2",
    "section": "Concurrency and parallelism in Python3",
    "text": "Concurrency and parallelism in Python3\n\n\n\nParallelism: multiple tasks are running in parallel, each on a different processors. This is done through the multiprocessing module.\nConcurrency: multiple tasks are taking turns to run on the same processor. Another task can be scheduled while the current one is blocked on I/O.\nThreading: multiple threads take turns executing tasks. One process can contain multiple threads. Similar to concurrency but within the context of a single process.\n\n\n\n\n\nConcurrency in Python"
  },
  {
    "objectID": "slides/02-slides.html#how-to-implement-concurrency-with-asyncio",
    "href": "slides/02-slides.html#how-to-implement-concurrency-with-asyncio",
    "title": "Lecture 2",
    "section": "How to implement concurrency with asyncio",
    "text": "How to implement concurrency with asyncio\n\n\n\nasyncio is a library to write concurrent code using the async/await syntax.\nUse of async and await keywords. You can call an async function multiple times while you await the result of a previous invocation.\nawait the result of multiple async tasks using gather.\nThe main function in this example is called a coroutine. Multiple coroutines can be run concurrnetly as awaitable tasks.\n\n\nimport asyncio\n\nasync def main():\n    print('Hello ...')\n    await asyncio.sleep(1)\n    print('... World!')\n\nasyncio.run(main())"
  },
  {
    "objectID": "slides/02-slides.html#anatomy-of-an-asyncio-python-program",
    "href": "slides/02-slides.html#anatomy-of-an-asyncio-python-program",
    "title": "Lecture 2",
    "section": "Anatomy of an asyncio Python program",
    "text": "Anatomy of an asyncio Python program\n\n\n\nWrite a regular Python function that makes a call to a database, an API or any other blocking functionality as you normally would.\nCreate a coroutine i.e. an async wrapper function to the blocking function using async and await, with the call to blocking function made using the asyncio.to_thread function. This enables the coroutine execution in a separate thread.\nCreate another coroutine that makes multiple calls (in a loop, list comprehension) to the async wrapper created in the previous step and awaits completion of all of the invocations using the asyncio.gather function.\nCall the coroutine created in the previous step from another function using the asyncio.run function.\n\n\nimport time\nimport asyncio\n\ndef my_blocking_func(i):\n    # some blocking code such as an api call\n    print(f\"{i}, entry\")\n    time.sleep(1)\n    print(f\"{i}, exiting\")\n    return None\n  \nasync def async_my_blocking_func(i: int):\n    return await asyncio.to_thread(my_blocking_func, i)\n\nasync def async_my_blocking_func_for_multiple(n: int):\n    return await asyncio.gather(*[async_my_blocking_func(i) for i in range(n)])\n\nif __name__ == \"__main__\":\n    # async version\n    s = time.perf_counter()\n    n = 20\n    brewery_counts = asyncio.run(async_my_blocking_func_for_multiple(n))\n    elapsed_async = time.perf_counter() - s\n    print(f\"{__file__}, async_my_blocking_func_for_multiple finished in {elapsed_async:0.2f} seconds\")"
  },
  {
    "objectID": "slides/02-slides.html#ai-example-parallel-llm-api-calls",
    "href": "slides/02-slides.html#ai-example-parallel-llm-api-calls",
    "title": "Lecture 2",
    "section": "AI Example: Parallel LLM API Calls",
    "text": "AI Example: Parallel LLM API Calls\nimport asyncio\nimport aiohttp\n\nasync def call_llm_api(prompt, session):\n    \"\"\"Make async call to LLM API\"\"\"\n    async with session.post(\n        \"https://api.example.com/generate\",\n        json={\"prompt\": prompt, \"max_tokens\": 100}\n    ) as response:\n        return await response.json()\n\nasync def process_prompts(prompts):\n    \"\"\"Process multiple prompts in parallel\"\"\"\n    async with aiohttp.ClientSession() as session:\n        tasks = [call_llm_api(prompt, session) for prompt in prompts]\n        results = await asyncio.gather(*tasks)\n    return results\n\n# Generate synthetic data using parallel API calls\nprompts = [\"Generate a customer review for...\", \n           \"Create technical documentation for...\",\n           \"Write a dialogue about...\"] * 100\n\nresults = asyncio.run(process_prompts(prompts))\n\n\n\n\n\n\nTip\n\n\nThis approach can speed up synthetic data generation by 10-100x compared to sequential API calls"
  },
  {
    "objectID": "slides/02-slides.html#the-ai-data-challenge",
    "href": "slides/02-slides.html#the-ai-data-challenge",
    "title": "Lecture 2",
    "section": "The AI Data Challenge",
    "text": "The AI Data Challenge\n\nVolume: LLMs trained on trillions of tokens (petabytes of text)\nVariety: Multimodal data (text, images, audio, video, structured data)\nVelocity: Real-time data pipelines for online learning\nVeracity: Data quality is crucial for model performance\nValue: High-quality data is more important than model architecture\n\n\n\n“Data is the new oil, but it needs refining” - especially for AI"
  },
  {
    "objectID": "slides/02-slides.html#parallel-data-processing-pipeline-for-ai",
    "href": "slides/02-slides.html#parallel-data-processing-pipeline-for-ai",
    "title": "Lecture 2",
    "section": "Parallel Data Processing Pipeline for AI",
    "text": "Parallel Data Processing Pipeline for AI\n# Example: Parallel document processing for RAG system\nfrom multiprocessing import Pool\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\n\ndef process_document(doc):\n    # Extract text\n    text = extract_text(doc)\n    # Clean and normalize\n    cleaned = clean_text(text)\n    # Chunk into passages\n    chunks = chunk_text(cleaned, max_length=512)\n    # Generate embeddings\n    embeddings = model.encode(chunks)\n    return {'doc_id': doc.id, 'chunks': chunks, 'embeddings': embeddings}\n\n# Process millions of documents in parallel\nwith Pool(processes=32) as pool:\n    results = pool.map(process_document, documents)"
  },
  {
    "objectID": "slides/02-slides.html#common-data-parallel-patterns-for-ai",
    "href": "slides/02-slides.html#common-data-parallel-patterns-for-ai",
    "title": "Lecture 2",
    "section": "Common Data Parallel Patterns for AI",
    "text": "Common Data Parallel Patterns for AI\n1. Embarrassingly Parallel\n# Process each item independently\nresults = parallel_map(transform_function, data_items)\n2. Map-Reduce Pattern\n# Map: Extract features from each document\n# Reduce: Aggregate statistics across corpus\nword_counts = data.map(tokenize).reduce(aggregate_counts)\n3. Pipeline Pattern\n# Stage 1: Download → Stage 2: Extract → Stage 3: Transform → Stage 4: Load\npipeline = Download() | Extract() | Transform() | Load()"
  },
  {
    "objectID": "slides/02-slides.html#data-quality-at-scale",
    "href": "slides/02-slides.html#data-quality-at-scale",
    "title": "Lecture 2",
    "section": "Data Quality at Scale",
    "text": "Data Quality at Scale\n\n\nQuality Checks to Parallelize\n\nDuplicate detection (MinHash)\nLanguage identification\nToxicity filtering\nPII detection and removal\nFormat validation\nSchema compliance\nStatistical outlier detection\n\n\nQuality Metrics for AI Data\n\nToken distribution analysis\nVocabulary coverage\nDomain representation\nBias measurement\nData freshness\nAnnotation agreement\nSynthetic data detection"
  },
  {
    "objectID": "slides/02-slides.html#distributed-frameworks-for-ai-data",
    "href": "slides/02-slides.html#distributed-frameworks-for-ai-data",
    "title": "Lecture 2",
    "section": "Distributed Frameworks for AI Data",
    "text": "Distributed Frameworks for AI Data\nApache Spark for Large-Scale Processing\n# Process TB-scale datasets\ndf = spark.read.parquet(\"s3://bucket/training_data/\")\ncleaned = df.filter(col(\"quality_score\") &gt; 0.8) \\\n           .select(clean_text_udf(\"text\").alias(\"cleaned_text\"))\ncleaned.write.parquet(\"s3://bucket/cleaned_data/\")\nRay Data for ML Pipelines\nimport ray.data\n\n# Distributed preprocessing with Ray\nds = ray.data.read_parquet(\"s3://bucket/images/\")\nprocessed = ds.map_batches(preprocess_images, batch_size=100)\n              .map_batches(extract_features, num_gpus=1)"
  },
  {
    "objectID": "slides/02-slides.html#embedding-generation-at-scale",
    "href": "slides/02-slides.html#embedding-generation-at-scale",
    "title": "Lecture 2",
    "section": "Embedding Generation at Scale",
    "text": "Embedding Generation at Scale\n\nChallenge: Generate embeddings for 100M+ documents\nSolution: Distributed GPU processing\n\n# Distributed embedding generation\nfrom transformers import AutoModel\nimport torch.distributed as dist\n\ndef generate_embeddings_distributed(documents, rank, world_size):\n    # Each GPU processes a subset\n    subset = documents[rank::world_size]\n    model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n    model = model.to(f\"cuda:{rank}\")\n    \n    embeddings = []\n    for batch in batch_iterator(subset, batch_size=256):\n        with torch.no_grad():\n            emb = model.encode(batch)\n        embeddings.extend(emb)\n    return embeddings"
  },
  {
    "objectID": "slides/02-slides.html#data-deduplication-for-ai",
    "href": "slides/02-slides.html#data-deduplication-for-ai",
    "title": "Lecture 2",
    "section": "Data Deduplication for AI",
    "text": "Data Deduplication for AI\nWhy Deduplication Matters\n\nTraining on duplicates wastes compute\nCan cause overfitting and memorization\nCritical for web-scale datasets\n\nParallel Deduplication Strategies\n# MinHash for approximate deduplication\nfrom datasketch import MinHash, MinHashLSH\n\ndef parallel_dedup(documents):\n    # Step 1: Generate MinHash signatures (parallel)\n    signatures = parallel_map(generate_minhash, documents)\n    \n    # Step 2: LSH for finding near-duplicates\n    lsh = MinHashLSH(threshold=0.9)\n    for doc_id, sig in signatures:\n        lsh.insert(doc_id, sig)\n    \n    # Step 3: Filter duplicates (parallel)\n    unique_docs = parallel_filter(is_unique, documents, lsh)\n    return unique_docs"
  },
  {
    "objectID": "slides/02-slides.html#synthetic-data-generation",
    "href": "slides/02-slides.html#synthetic-data-generation",
    "title": "Lecture 2",
    "section": "Synthetic Data Generation",
    "text": "Synthetic Data Generation\nParallel Synthetic Data Creation\n# Generate training data using LLMs\nprompts = [\n    \"Generate a customer service dialogue about...\",\n    \"Create a technical documentation for...\",\n    \"Write a product review for...\"\n]\n\n# Parallel generation across multiple API calls\nwith ThreadPoolExecutor(max_workers=100) as executor:\n    futures = []\n    for prompt in prompts:\n        future = executor.submit(generate_with_llm, prompt)\n        futures.append(future)\n    \n    synthetic_data = [f.result() for f in futures]\n\nUse Cases\n\nAugmenting limited datasets\nCreating instruction-tuning data\nGenerating test cases\nPrivacy-preserving alternatives"
  },
  {
    "objectID": "slides/02-slides.html#best-practices-for-parallel-data-processing",
    "href": "slides/02-slides.html#best-practices-for-parallel-data-processing",
    "title": "Lecture 2",
    "section": "Best Practices for Parallel Data Processing",
    "text": "Best Practices for Parallel Data Processing\n\nProfile First: Identify bottlenecks before parallelizing\nChunk Appropriately: Balance chunk size with overhead\nHandle Failures: Implement retry logic and checkpointing\nMonitor Progress: Use progress bars and logging\nValidate Output: Always verify data quality post-processing\nVersion Everything: Track data lineage and transformations"
  },
  {
    "objectID": "slides/02-slides.html#tools-comparison-for-ai-data-processing",
    "href": "slides/02-slides.html#tools-comparison-for-ai-data-processing",
    "title": "Lecture 2",
    "section": "Tools Comparison for AI Data Processing",
    "text": "Tools Comparison for AI Data Processing\n\n\n\nTool\nBest For\nScalability\nLearning Curve\n\n\n\n\nmultiprocessing\nSingle machine\n10s of cores\nLow\n\n\nSpark\nDistributed batch\n1000s of nodes\nMedium\n\n\nRay\nML workloads\n100s of nodes\nLow\n\n\nDask\nPython-native\n100s of nodes\nLow\n\n\nBeam\nStream + batch\n1000s of nodes\nHigh"
  },
  {
    "objectID": "slides/02-slides.html#case-study-building-a-rag-dataset",
    "href": "slides/02-slides.html#case-study-building-a-rag-dataset",
    "title": "Lecture 2",
    "section": "Case Study: Building a RAG Dataset",
    "text": "Case Study: Building a RAG Dataset\nRequirements\n\nProcess 10M documents\nGenerate embeddings for each\nBuild vector index\nEnsure quality and deduplication\n\nParallel Solution\n# Stage 1: Parallel document processing\nprocessed_docs = ray.data.read_parquet(\"raw_docs/\") \\\n    .map_batches(extract_and_clean) \\\n    .map_batches(chunk_documents)\n\n# Stage 2: Distributed embedding generation  \nembeddings = processed_docs.map_batches(\n    generate_embeddings,\n    num_gpus=1,\n    batch_size=100\n)\n\n# Stage 3: Build index (using FAISS)\nindex = build_distributed_index(embeddings)"
  },
  {
    "objectID": "slides/02-slides.html#key-takeaways",
    "href": "slides/02-slides.html#key-takeaways",
    "title": "Lecture 2",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nData is the bottleneck in modern AI systems\nParallel processing is essential for AI-scale data\nChoose the right tool for your scale and use case\nQuality over quantity - parallel quality checks are crucial\nMonitor and validate throughout the pipeline\nModern frameworks (Spark, Ray, Dask) simplify distributed processing"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Abhijit Das Gupta",
    "section": "",
    "text": "Abhijit Das Gupta is a Data Science Director at AstraZeneca. He works at the interface of statistics and machine learning, with broad experience in statistical modeling and machine learning, bioinformatics, optimization, signal processing, and data visualization. He holds a PhD in Biostatistics with earlier training in Mathematical Statistics. He has been using R for over 25 years for a wide variety of applications in biomedical research, as well as for data-driven reports and presentations.\nAbhijit is adjunct faculty at George Washington University, where he currently teaches courses in Applied Big Data Analysis . He is also adjunct faculty at George Mason University and Georgetown University. He is a certified Carpentries instructor. He co-founded and organized the Statistical Programming DC meetup (formerly R Users Group DC) for several years and got to know several luminaries in the field, including DJ Patil, Hadley Wickham, Max Kuhn, Frank Harrell and others.\nOutside of work, Abhijit is an aikido instructor holding a 5th degree black belt, having practiced the art since 1994. He teaches in Frederick County, MD. He is also active in community theater productions with Ebong Theatrix, a Bengali theater group in the DC area.\nAbhijit lives in Germantown, MD with his wife, children and golden retriever Jingle."
  }
]