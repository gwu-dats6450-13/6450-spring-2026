[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Course: DATS 6450.13 (Applied Big Data Analytics) Semester: Spring, 2026 Meeting time: Thursdays 6:10-8:40pm Location: Philips 416\n\n\nName: Abhijit Dasgupta, PhD\nPhone: 240-813-8458\nGW E-mail: abhijit.dasgupta@gwu.edu\nOffice hours:\nIn-person: Thursdays 5:10-6:10pm, Philips 416\nRemote: By appointment (via link)\nCommunications: via Slack\n\n\n\n\n\n\nImportantCommunication policy\n\n\n\nAll general class-related issues, including discussion of material, should be done on Slack. Any private issue, including absences, illness, issues around grades, etc. should be done via e-mail\n\n\n\nCourse prerequisites: DATS 6101, 6102, Python\nStudents completing the course will be able to:\n\nBuild data pipelines and transformations using DuckDB and PySpark.\nUse concepts of parallelization, embarrasingly parallel problems, and applications in Python to process large data\nCompare computational efficiency, memory management, and scalability between single-node, cluster and distributed frameworks.\nOptimize queries, caching, and partitioning strategies across engines.\nApply datashader or equivalent tools to visualize large datasets efficiently.\nDevelop end-to-end reproducible analytics workflows integrating both systems.\n\nAverage amount of direct instruction or guided interaction with the instructor and average minimum amount of independent (out-of-class) learning expected per week:\nThis class will meet for 2.5 hours of in-person learning per week. It is expected that homework, project and independent study will take an average of 8-10 hours weekly. We note here that classes are in-person only and no remote option will be available during the semester. It will be the student’s responsibility to make up work during an absence. Class attendance will be noted and class participation is encouraged to enable active learning of the material.\nRequired textbooks and/or other materials and recommended readings: There are no required textbooks for this class. Relevant readings will be assigned on a weekly basis\n\n\n\nGenerative Artificial Intelligence (GAI) tools such as ChatGPT are becoming important resources in many fields and industries. Accordingly, you are permitted to use such tools to generate content submitted for evaluation in this course, including assignments, with the limitation that no more than 50% of your submitted code can be generated using AI tools. You remain responsible for all content you submit for evaluation, and to ensure that the material submitted runs and produces correct results. Material for final presentation and reports must be substantially your own work, with GAI tools available to help with brainstorming, editing, organization, polish.\nYou may use GAI tools to help generate ideas and brainstorm. However, you should note that the material generated by these tools may be inaccurate, incomplete, or otherwise problematic. Beware that use may also stifle your own independent thinking and creativity.\nIf you include content (e.g., ideas, text, code, images) that was generated, in whole or in part, by Generative Artificial Intelligence tools (including, but not limited to, ChatGPT and other large language models) in work submitted for evaluation in this course, you must document and credit your source. For example, text generated using ChatGPT-4 should include a citation such as: “ChatGPT-4. (YYYY, Month DD of query). ‘Text of your query.’ Generated using OpenAI. https://chat.openai.com/.” Material generated using other tools should be cited accordingly. Failure to do so in this course constitutes failure to attribute under the George Washington University Code of Academic Integrity.\n\n\n\nWe will check for plagiarism of submitted code between students. We understand that there are common templates and structures in code that will naturally be common between students, but everyone does typically have their own coding style, naming conventions, comments and approaches. Any pair of submissions that appear to have more than 70% code in common will be subject to further review. Code that is over 90% common will be considered a potential violation of the Honor Code.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nLab\n\n\n1\n2026-01-15\nCourse overview\nBash shell\n\n\n2\n2026-01-22\nIntroduction to Cloud Computing\nAWS setup\n\n\n3\n2026-01-29\nParallel and distributed computing\nUsing parquet\n\n\n4\n2026-02-05\nintroduction to DuckDB and Polars\n\n\n\n5\n2026-02-12\nAdvanced DuckDB operations\n\n\n\n6\n2026-02-19\nPython tools for larger data\npandas, dask, Ray, RAPIDS\n\n\n7\n2026-02-26\nDeveloping workflows for big data (Midterm 1)\n\n\n\n8\n2026-03-05\nApache Spark Fundamentals\nSpark RDD\n\n\n9\n2026-03-12\nSpring Break\n\n\n\n10\n2026-03-19\nSpark DataFrames and Spark SQL\n\n\n\n11\n2026-03-26\nSpark ML and Streaming\nMLib\n\n\n12\n2026-04-02\nSpark NLP\nJohn Snow Labs\n\n\n13\n2026-04-09\nVisualization and reporting large data (Midterm 2)\ndatashader\n\n\n14\n2026-04-16\nMachine Learning at Scale\nTensorflow, TF probability, PyMC\n\n\n15\n2026-04-23\nFinal project presentations\n\n\n\n\nScheduling of final examinations: There will be no final exams for this class. There will be a final group project which will be an implementation of a data science analytic pipeline for a large dataset that will run on the cloud.\n\n\n\nEach week we will have several short assessment activities to build on class materials. These will include:\n\nShort weekly homework that will be graded for correctness. You will have 7-10 days to complete each assignment. Assignments will be provided and submissions made via Github Classroom\nLabs will be started in class and will need to be completed and submitted via Github Classroom. These will be graded for completion\nYou will be given weekly readings that introduce the material for the following week. There will be a short quiz based on the readings that must be completed before class each week.\nThere will be two midterm evaluations. These will be analytic and coding exercises to be done in class without the help of any generative AI or LLM coding assistance. These will typically be 45-60 minutes long and will also be submitted via Github Classroom.\nYou will form groups of 2-3 students and do an analytic group project on a Big dataset. This project will involve an end-to-end analytic pipeline and report based on either a data set you choose or a dataset provided to you (which is TBD). You will have required ungraded check-ins with the professor per schedule to show progress, and a presentation on the final day of class. The project report and code must be submitted per the calendar\n\n\n\n\nQuizzes, labs, midterms and projects cannot be late; a late submission will be entered as a 0.\n\nFor quizzes and labs, the lowest score for each type will be removed from calculation of the final grade at the end of the semester\n\nHomework will have the following late policy:\n\nThere will be a penalty of 10% of the total points per day for each day late, for a maximum of 20% penalty\nAny homework can be submitted by the last day of class for a penalty of 40% of the total points for that homework\n\n\n\n\n\n\n\nhomework (25%)\nlab completions (10%)\nquizzes (10%)\nmidterm exams (15%)\nfinal group project (30%)\nclass participation/attendance (10%)\n\n\n\n\nAcademic integrity is an essential part of the educational process, and all members of the GW community take these matters very seriously. As the instructor of record for this course, my role is to provide clear expectations and uphold them in all assessments. Violations of academic integrity occur when students fail to cite research sources properly, engage in unauthorized collaboration, falsify data, and otherwise violate the Code of Academic Integrity. If you have any questions about whether particular academic practices or resources are permitted, you should ask me for clarification. If you are reported for an academic integrity violation, you should contact Conflict Education and Student Accountability (CESA), formerly known as Student Rights and Responsibilities (SRR), to learn more about your rights and options in the process. Consequences can range from failure of assignment to expulsion from the University and may include a transcript notation. For more information, refer to the CESA website at students.gwu.edu/code-academic-integrity or contact CESA by email cesa@gwu.edu or phone 202-994-6757.\n\n\n\nStudents must notify faculty during the first week of the semester in which they are enrolled in the course, or as early as possible, but no later than three weeks prior to the absence, of their intention to be absent from class on their day(s) of religious observance. If the holiday falls within the first three weeks of class, the student must inform faculty in the first week of the semester. For details and policy, see provost.gwu.edu/policies-procedures-and-guidelines.\n\n\n\nStudents are encouraged to use electronic course materials, including recorded class sessions, for private personal use in connection with their academic program of study. Electronic course materials and recorded class sessions should not be shared or used for non-course related purposes unless express permission has been granted by the instructor. Students who impermissibly share any electronic course materials are subject to discipline under the Student Code of Conduct. Contact the instructor if you have questions regarding what constitutes permissible or impermissible use of electronic course materials and/or recorded class sessions. Contact Disability Support Services at disabilitysupport.gwu.edu if you have questions or need assistance in accessing electronic course materials."
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus",
    "section": "",
    "text": "Name: Abhijit Dasgupta, PhD\nPhone: 240-813-8458\nGW E-mail: abhijit.dasgupta@gwu.edu\nOffice hours:\nIn-person: Thursdays 5:10-6:10pm, Philips 416\nRemote: By appointment (via link)\nCommunications: via Slack\n\n\n\n\n\n\nImportantCommunication policy\n\n\n\nAll general class-related issues, including discussion of material, should be done on Slack. Any private issue, including absences, illness, issues around grades, etc. should be done via e-mail\n\n\n\nCourse prerequisites: DATS 6101, 6102, Python\nStudents completing the course will be able to:\n\nBuild data pipelines and transformations using DuckDB and PySpark.\nUse concepts of parallelization, embarrasingly parallel problems, and applications in Python to process large data\nCompare computational efficiency, memory management, and scalability between single-node, cluster and distributed frameworks.\nOptimize queries, caching, and partitioning strategies across engines.\nApply datashader or equivalent tools to visualize large datasets efficiently.\nDevelop end-to-end reproducible analytics workflows integrating both systems.\n\nAverage amount of direct instruction or guided interaction with the instructor and average minimum amount of independent (out-of-class) learning expected per week:\nThis class will meet for 2.5 hours of in-person learning per week. It is expected that homework, project and independent study will take an average of 8-10 hours weekly. We note here that classes are in-person only and no remote option will be available during the semester. It will be the student’s responsibility to make up work during an absence. Class attendance will be noted and class participation is encouraged to enable active learning of the material.\nRequired textbooks and/or other materials and recommended readings: There are no required textbooks for this class. Relevant readings will be assigned on a weekly basis"
  },
  {
    "objectID": "syllabus.html#class-policy-on-the-use-of-ai",
    "href": "syllabus.html#class-policy-on-the-use-of-ai",
    "title": "Syllabus",
    "section": "",
    "text": "Generative Artificial Intelligence (GAI) tools such as ChatGPT are becoming important resources in many fields and industries. Accordingly, you are permitted to use such tools to generate content submitted for evaluation in this course, including assignments, with the limitation that no more than 50% of your submitted code can be generated using AI tools. You remain responsible for all content you submit for evaluation, and to ensure that the material submitted runs and produces correct results. Material for final presentation and reports must be substantially your own work, with GAI tools available to help with brainstorming, editing, organization, polish.\nYou may use GAI tools to help generate ideas and brainstorm. However, you should note that the material generated by these tools may be inaccurate, incomplete, or otherwise problematic. Beware that use may also stifle your own independent thinking and creativity.\nIf you include content (e.g., ideas, text, code, images) that was generated, in whole or in part, by Generative Artificial Intelligence tools (including, but not limited to, ChatGPT and other large language models) in work submitted for evaluation in this course, you must document and credit your source. For example, text generated using ChatGPT-4 should include a citation such as: “ChatGPT-4. (YYYY, Month DD of query). ‘Text of your query.’ Generated using OpenAI. https://chat.openai.com/.” Material generated using other tools should be cited accordingly. Failure to do so in this course constitutes failure to attribute under the George Washington University Code of Academic Integrity."
  },
  {
    "objectID": "syllabus.html#note-on-code-plagiarism",
    "href": "syllabus.html#note-on-code-plagiarism",
    "title": "Syllabus",
    "section": "",
    "text": "We will check for plagiarism of submitted code between students. We understand that there are common templates and structures in code that will naturally be common between students, but everyone does typically have their own coding style, naming conventions, comments and approaches. Any pair of submissions that appear to have more than 70% code in common will be subject to further review. Code that is over 90% common will be considered a potential violation of the Honor Code."
  },
  {
    "objectID": "syllabus.html#schedule-of-topics",
    "href": "syllabus.html#schedule-of-topics",
    "title": "Syllabus",
    "section": "",
    "text": "Week\nDate\nTopic\nLab\n\n\n1\n2026-01-15\nCourse overview\nBash shell\n\n\n2\n2026-01-22\nIntroduction to Cloud Computing\nAWS setup\n\n\n3\n2026-01-29\nParallel and distributed computing\nUsing parquet\n\n\n4\n2026-02-05\nintroduction to DuckDB and Polars\n\n\n\n5\n2026-02-12\nAdvanced DuckDB operations\n\n\n\n6\n2026-02-19\nPython tools for larger data\npandas, dask, Ray, RAPIDS\n\n\n7\n2026-02-26\nDeveloping workflows for big data (Midterm 1)\n\n\n\n8\n2026-03-05\nApache Spark Fundamentals\nSpark RDD\n\n\n9\n2026-03-12\nSpring Break\n\n\n\n10\n2026-03-19\nSpark DataFrames and Spark SQL\n\n\n\n11\n2026-03-26\nSpark ML and Streaming\nMLib\n\n\n12\n2026-04-02\nSpark NLP\nJohn Snow Labs\n\n\n13\n2026-04-09\nVisualization and reporting large data (Midterm 2)\ndatashader\n\n\n14\n2026-04-16\nMachine Learning at Scale\nTensorflow, TF probability, PyMC\n\n\n15\n2026-04-23\nFinal project presentations\n\n\n\n\nScheduling of final examinations: There will be no final exams for this class. There will be a final group project which will be an implementation of a data science analytic pipeline for a large dataset that will run on the cloud."
  },
  {
    "objectID": "syllabus.html#assignments-and-evaluations",
    "href": "syllabus.html#assignments-and-evaluations",
    "title": "Syllabus",
    "section": "",
    "text": "Each week we will have several short assessment activities to build on class materials. These will include:\n\nShort weekly homework that will be graded for correctness. You will have 7-10 days to complete each assignment. Assignments will be provided and submissions made via Github Classroom\nLabs will be started in class and will need to be completed and submitted via Github Classroom. These will be graded for completion\nYou will be given weekly readings that introduce the material for the following week. There will be a short quiz based on the readings that must be completed before class each week.\nThere will be two midterm evaluations. These will be analytic and coding exercises to be done in class without the help of any generative AI or LLM coding assistance. These will typically be 45-60 minutes long and will also be submitted via Github Classroom.\nYou will form groups of 2-3 students and do an analytic group project on a Big dataset. This project will involve an end-to-end analytic pipeline and report based on either a data set you choose or a dataset provided to you (which is TBD). You will have required ungraded check-ins with the professor per schedule to show progress, and a presentation on the final day of class. The project report and code must be submitted per the calendar\n\n\n\n\nQuizzes, labs, midterms and projects cannot be late; a late submission will be entered as a 0.\n\nFor quizzes and labs, the lowest score for each type will be removed from calculation of the final grade at the end of the semester\n\nHomework will have the following late policy:\n\nThere will be a penalty of 10% of the total points per day for each day late, for a maximum of 20% penalty\nAny homework can be submitted by the last day of class for a penalty of 40% of the total points for that homework"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "",
    "text": "homework (25%)\nlab completions (10%)\nquizzes (10%)\nmidterm exams (15%)\nfinal group project (30%)\nclass participation/attendance (10%)"
  },
  {
    "objectID": "syllabus.html#academic-integrity-code",
    "href": "syllabus.html#academic-integrity-code",
    "title": "Syllabus",
    "section": "",
    "text": "Academic integrity is an essential part of the educational process, and all members of the GW community take these matters very seriously. As the instructor of record for this course, my role is to provide clear expectations and uphold them in all assessments. Violations of academic integrity occur when students fail to cite research sources properly, engage in unauthorized collaboration, falsify data, and otherwise violate the Code of Academic Integrity. If you have any questions about whether particular academic practices or resources are permitted, you should ask me for clarification. If you are reported for an academic integrity violation, you should contact Conflict Education and Student Accountability (CESA), formerly known as Student Rights and Responsibilities (SRR), to learn more about your rights and options in the process. Consequences can range from failure of assignment to expulsion from the University and may include a transcript notation. For more information, refer to the CESA website at students.gwu.edu/code-academic-integrity or contact CESA by email cesa@gwu.edu or phone 202-994-6757."
  },
  {
    "objectID": "syllabus.html#university-policy-on-observance-of-religious-holidays",
    "href": "syllabus.html#university-policy-on-observance-of-religious-holidays",
    "title": "Syllabus",
    "section": "",
    "text": "Students must notify faculty during the first week of the semester in which they are enrolled in the course, or as early as possible, but no later than three weeks prior to the absence, of their intention to be absent from class on their day(s) of religious observance. If the holiday falls within the first three weeks of class, the student must inform faculty in the first week of the semester. For details and policy, see provost.gwu.edu/policies-procedures-and-guidelines."
  },
  {
    "objectID": "syllabus.html#use-of-electronic-course-materials-and-class-recordings",
    "href": "syllabus.html#use-of-electronic-course-materials-and-class-recordings",
    "title": "Syllabus",
    "section": "",
    "text": "Students are encouraged to use electronic course materials, including recorded class sessions, for private personal use in connection with their academic program of study. Electronic course materials and recorded class sessions should not be shared or used for non-course related purposes unless express permission has been granted by the instructor. Students who impermissibly share any electronic course materials are subject to discipline under the Student Code of Conduct. Contact the instructor if you have questions regarding what constitutes permissible or impermissible use of electronic course materials and/or recorded class sessions. Contact Disability Support Services at disabilitysupport.gwu.edu if you have questions or need assistance in accessing electronic course materials."
  },
  {
    "objectID": "syllabus.html#academic-commons",
    "href": "syllabus.html#academic-commons",
    "title": "Syllabus",
    "section": "Academic Commons",
    "text": "Academic Commons\nAcademic Commons is the central location for academic support resources for GW students. To schedule a peer tutoring session for a variety of courses visit go.gwu.edu/tutoring. Visit academiccommons.gwu.edu for study skills tips, finding help with research, and connecting with other campus resources. For questions email academiccommons@gwu.edu."
  },
  {
    "objectID": "syllabus.html#gw-writing-center",
    "href": "syllabus.html#gw-writing-center",
    "title": "Syllabus",
    "section": "GW Writing Center",
    "text": "GW Writing Center\nGW Writing Center cultivates confident writers in the University community by facilitating collaborative, critical, and inclusive conversations at all stages of the writing process. Working alongside peer mentors, writers develop strategies to write independently in academic and public settings. Appointments can be booked online at gwu.mywconline."
  },
  {
    "objectID": "syllabus.html#disability-support-services-dss-202-994-8250",
    "href": "syllabus.html#disability-support-services-dss-202-994-8250",
    "title": "Syllabus",
    "section": "Disability Support Services (DSS) 202-994-8250",
    "text": "Disability Support Services (DSS) 202-994-8250\nAny student who may need an accommodation based on the potential impact of a disability should contact Disability Support Services at disabilitysupport.gwu.edu to establish eligibility and to coordinate reasonable accommodation."
  },
  {
    "objectID": "syllabus.html#student-health-center-202-994-5300-247",
    "href": "syllabus.html#student-health-center-202-994-5300-247",
    "title": "Syllabus",
    "section": "Student Health Center 202-994-5300, 24/7",
    "text": "Student Health Center 202-994-5300, 24/7\nThe Student Health Center (SHC) offers medical, counseling/psychological, and psychiatric services to GW students. More information about the SHC is available at healthcenter.gwu.edu. Students experiencing a medical or mental health emergency on campus should contact GW Emergency Services at 202-994-6111, or off campus at 911."
  },
  {
    "objectID": "syllabus.html#gw-emergency-services-202-994-6111",
    "href": "syllabus.html#gw-emergency-services-202-994-6111",
    "title": "Syllabus",
    "section": "GW Emergency Services: 202-994-6111",
    "text": "GW Emergency Services: 202-994-6111\nFor situation-specific instructions, refer to GW’s Emergency Procedures guide."
  },
  {
    "objectID": "syllabus.html#gw-alert",
    "href": "syllabus.html#gw-alert",
    "title": "Syllabus",
    "section": "GW Alert",
    "text": "GW Alert\nGW Alert is an emergency notification system that sends alerts to the GW community. GW requests students, faculty, and staff maintain current contact information by logging on to alert.gwu.edu. Alerts are sent via email, text, social media, and other means, including the Guardian app. The Guardian app is a safety app that allows you to communicate quickly with GW Emergency Services, 911, and other resources. Learn more at safety.gwu.edu."
  },
  {
    "objectID": "syllabus.html#protective-actions",
    "href": "syllabus.html#protective-actions",
    "title": "Syllabus",
    "section": "Protective Actions",
    "text": "Protective Actions\nGW prescribes four protective actions that can be issued by university officials depending on the type of emergency. All GW community members are expected to follow directions according to the specified protective action. The protective actions are Shelter, Evacuate, Secure, and Lockdown (details below). Learn more at safety.gwu.edu/gw-standard-emergency-statuses.\n\nShelter\n\nProtection from a specific hazard\nThe hazard could be a tornado, earthquake, hazardous material spill, or other environmental emergency.\nSpecific safety guidance will be shared on a case-by-case basis.\n\n\n\nAction:\n\nFollow safety guidance for the hazard.\n\n\n\nEvacuate\n\nNeed to move people from one location to another.\nStudents and staff should be prepared to follow specific instructions given by first responders and University officials.\n\n\n\nAction:\n\nEvacuate to a designated location.\nLeave belongings behind.\nFollow additional instructions from first responders.\n\n\n\nSecure\n\nThreat or hazard outside of buildings or around campus.\nncreased security, secured building perimeter, increased situational awareness, and restricted access to entry doors.\n\n\n\nAction:\n\nGo inside and stay inside.\nActivities inside may continue.\n\n\n\nLockdown\n\nThreat or hazard with the potential to impact individuals inside buildings.\nRoom-based protocol that requires locking interior doors, turning off lights, and staying out of sight of corridor window.\n\n\n\nAction:\n\nLocks, lights, out of sight\nConsider Run, Hide, Fight\n\n\n\nClassroom Emergency Lockdown Buttons\nSome classrooms have been equipped with classroom emergency lockdown buttons. If the button is pushed, GWorld Card access to the room will be disabled, and GW Dispatch will be alerted. The door must be manually closed if it is not closed when the button is pushed. Anyone in the classroom will be able to exit, but no one will be able to get in."
  },
  {
    "objectID": "slides/01-slides.html#agenda-for-todays-session",
    "href": "slides/01-slides.html#agenda-for-todays-session",
    "title": "Lecture 1",
    "section": "Agenda for today’s session",
    "text": "Agenda for today’s session\n\nCourse and syllabus overview\nBig Data Concepts\n\nDefinition\nChallenges\nApproaches\n\nData Engineering\nIntroduction to bash\n\nLab: Linux command line"
  },
  {
    "objectID": "slides/01-slides.html#bookmark-these-links",
    "href": "slides/01-slides.html#bookmark-these-links",
    "title": "Lecture 1",
    "section": "Bookmark these links!",
    "text": "Bookmark these links!\n\nCourse website: https://gwu-dats6450-13.github.io/6450-spring-2026\nGitHub Organization for your deliverables: https://github.com/gwu-dats6450-13/ \nSlack Workspace: DATS 6450.13 Spring 2026 - 6450-spring-2026.slack.com\n\nJoin link: https://join.slack.com/t/6450-spring-2026/shared_invite/zt-3nh9da585-T0CBYJd4vJwm7mLCEpnrKg"
  },
  {
    "objectID": "slides/01-slides.html#instructors",
    "href": "slides/01-slides.html#instructors",
    "title": "Lecture 1",
    "section": "Instructors",
    "text": "Instructors\n\nAbhijit Dasgupta, abhijit.dasgupta@gwu.edu\nTA: Vishal Fulsundar vishal.fulsundar@gwmail.gwu.edu"
  },
  {
    "objectID": "slides/01-slides.html#abhijit-dasgupta",
    "href": "slides/01-slides.html#abhijit-dasgupta",
    "title": "Lecture 1",
    "section": "Abhijit Dasgupta",
    "text": "Abhijit Dasgupta\n\n“AI” is the new line of makeup in data science; putting it on makes even BS attractive\n\n\n\n\nData Science Director at AstraZeneca supporting Oncology R&D (lung, breast, bladder, GI, ovarian)\n\nbiomarkers, clinical studies, strategy, decision-making, education\nBayesian, survival, visualization, high-dimensional, causality\n\nAdjunct Professor at Georgetown since 2020, GMU since 2018, GWU since 2026\nData science consultant in a wide variety of domains, from the USPS to telecom to environmental health and epidemiology\nR and reproducible research evangelist\n\nPython is cool too!!\n\n\nFun facts\n\nAikido instructor (5th degree black belt), teaching since 1994\n\n\n Community theater actor with Ebong Theatrix (Bengali group in DC)"
  },
  {
    "objectID": "slides/01-slides.html#course-description",
    "href": "slides/01-slides.html#course-description",
    "title": "Lecture 1",
    "section": "Course Description",
    "text": "Course Description\nData is everywhere! Many times, it’s just too big to work with traditional tools. This is a hands-on, practical workshop style course about using cloud computing resources to do analysis and manipulation of datasets that are too large to fit on a single machine and/or analyzed with traditional tools. The course will focus on Spark, MapReduce, the Hadoop Ecosystem and other tools.\nYou will understand how to acquire and/or ingest the data, and then massage, clean, transform, analyze, and model it within the context of big data analytics. You will be able to think more programmatically and logically about your big data needs, tools and issues.\n\nAlways refer to the syllabus in the course website for class policies."
  },
  {
    "objectID": "slides/01-slides.html#learning-objectives",
    "href": "slides/01-slides.html#learning-objectives",
    "title": "Lecture 1",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nBuild data pipelines and transformations using DuckDB and PySpark.\nUse concepts of parallelization, embarrasingly parallel problems, and applications in Python to process large data\nCompare computational efficiency, memory management, and scalability between single-node, cluster and distributed frameworks.\nOptimize queries, caching, and partitioning strategies across engines.\nApply datashader or equivalent tools to visualize large datasets efficiently.\nDevelop end-to-end reproducible analytics workflows integrating both systems."
  },
  {
    "objectID": "slides/01-slides.html#evaluation",
    "href": "slides/01-slides.html#evaluation",
    "title": "Lecture 1",
    "section": "Evaluation",
    "text": "Evaluation\n\nhomework (25%)\nlab completions (10%)\nquizzes (10%)\nmidterm exams (15%)\nfinal group project (30%)\nclass participation/attendance (10%)"
  },
  {
    "objectID": "slides/01-slides.html#course-materials",
    "href": "slides/01-slides.html#course-materials",
    "title": "Lecture 1",
    "section": "Course Materials",
    "text": "Course Materials\n\nSlides/labs/assignment on Website/GitHub\nQuizzes and readings in Blackboard\nNo required textbook. Recommended readings will be provided."
  },
  {
    "objectID": "slides/01-slides.html#communication",
    "href": "slides/01-slides.html#communication",
    "title": "Lecture 1",
    "section": "Communication",
    "text": "Communication\n\nSlack is the primary form of communication for this course. Please join the Slack workspace for announcements, questions, and discussions.\nEmail is reserved for private matters only. Please allow 24-48 hours for a response\nIn-person office hours will be held the hour before class in this classroom\nOnline office hours are by appointment (at this link)"
  },
  {
    "objectID": "slides/01-slides.html#slack-rules",
    "href": "slides/01-slides.html#slack-rules",
    "title": "Lecture 1",
    "section": "Slack rules:",
    "text": "Slack rules:\n\nPost any question/comment about the course, assignments or any technical issue.\nDMs are to be used sparingly\nYou may not DM multiple people in the instructional team at the same time for the same issue\nKeep an eye on the questions posted in Slack. Use the search function. It’s very possible that we have already answered a questions\nYou may DM us back only if we DM you first on a given issue\nLab/assignment/project questions will only be answered up to 6 hours before something is due"
  },
  {
    "objectID": "slides/01-slides.html#midterms",
    "href": "slides/01-slides.html#midterms",
    "title": "Lecture 1",
    "section": "Midterms",
    "text": "Midterms\nWe will have 2 midterms during the semester. They will be held during regular class hours. The midterms will be a short coding project that will be based on the previous labs and assignments. No GAI (chatGPT etc) will be allowed for the midterms\nDetails: TBD (will be announced by week 3)"
  },
  {
    "objectID": "slides/01-slides.html#final-project",
    "href": "slides/01-slides.html#final-project",
    "title": "Lecture 1",
    "section": "Final Project",
    "text": "Final Project\n\nGroups of 2-3 students\nUse an archive of Reddit data, augmented with external data\nExploratory analysis\nNLP\nMachine Learning\nWriteup\n\nData sourcing and ingesting\nExploratory analysis\nModeling\nChallenges and Learnings\nConclusions\nFuture work\n\nPresentation\n\n15 minute presentation\nDemo of code/workflow\n\nDue date: Finals week (date TBD)"
  },
  {
    "objectID": "slides/01-slides.html#where-does-it-come-from-how-is-it-being-created",
    "href": "slides/01-slides.html#where-does-it-come-from-how-is-it-being-created",
    "title": "Lecture 1",
    "section": "Where does it come from?How is it being created?",
    "text": "Where does it come from?How is it being created?"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2018",
    "href": "slides/01-slides.html#in-one-minute-of-time-2018",
    "title": "Lecture 1",
    "section": "In one minute of time (2018)",
    "text": "In one minute of time (2018)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2019",
    "href": "slides/01-slides.html#in-one-minute-of-time-2019",
    "title": "Lecture 1",
    "section": "In one minute of time (2019)",
    "text": "In one minute of time (2019)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2020",
    "href": "slides/01-slides.html#in-one-minute-of-time-2020",
    "title": "Lecture 1",
    "section": "In one minute of time (2020)",
    "text": "In one minute of time (2020)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2021",
    "href": "slides/01-slides.html#in-one-minute-of-time-2021",
    "title": "Lecture 1",
    "section": "In one minute of time (2021)",
    "text": "In one minute of time (2021)"
  },
  {
    "objectID": "slides/01-slides.html#in-one-minute-of-time-2025",
    "href": "slides/01-slides.html#in-one-minute-of-time-2025",
    "title": "Lecture 1",
    "section": "In one minute of time (2025)",
    "text": "In one minute of time (2025)\nEvery 60 seconds in 2025: * ChatGPT serves millions of requests (exact numbers proprietary) * 500 hours of video uploaded to YouTube * 1.04 million Slack messages sent * 362,000 hours watched on Netflix * 5.9-11.4 million Google searches * $443,000 spent on Amazon * AI-generated images created at massive scale (metrics not publicly available) * 347,200 posts on X (formerly Twitter) * 231-250 million emails sent"
  },
  {
    "objectID": "slides/01-slides.html#a-lot-of-it-is-hapenning-online.",
    "href": "slides/01-slides.html#a-lot-of-it-is-hapenning-online.",
    "title": "Lecture 1",
    "section": "A lot of it is hapenning online.",
    "text": "A lot of it is hapenning online.\nWe can record every: * click * ad impression * billing event * video interaction * server request * transaction * network message * fault * …"
  },
  {
    "objectID": "slides/01-slides.html#it-can-also-be-user-generated-content-e.g.",
    "href": "slides/01-slides.html#it-can-also-be-user-generated-content-e.g.",
    "title": "Lecture 1",
    "section": "It can also be user-generated content, e.g.:",
    "text": "It can also be user-generated content, e.g.:\n\nInstagram posts & Reels\nX (Twitter) posts & Threads\nTikTok videos\nYouTube Shorts\nReddit discussions\nDiscord conversations\nAI-generated content (text, images, code)\n…"
  },
  {
    "objectID": "slides/01-slides.html#but-health-and-scientific-computing-create-a-lot-too",
    "href": "slides/01-slides.html#but-health-and-scientific-computing-create-a-lot-too",
    "title": "Lecture 1",
    "section": "But health and scientific computing create a lot too!",
    "text": "But health and scientific computing create a lot too!\n\n???"
  },
  {
    "objectID": "slides/01-slides.html#theres-lots-of-graph-data-too",
    "href": "slides/01-slides.html#theres-lots-of-graph-data-too",
    "title": "Lecture 1",
    "section": "There’s lots of graph data too",
    "text": "There’s lots of graph data too\nMany interesting datasets have a graph structure:\n\nSocial networks\nGoogle’s knowledge graph\nTelecom networks\nComputer networks\nRoad networks\nCollaboration/relationships\n\nSome of these are HUGE"
  },
  {
    "objectID": "slides/01-slides.html#apache-web-server-log-files",
    "href": "slides/01-slides.html#apache-web-server-log-files",
    "title": "Lecture 1",
    "section": "Apache (web server) log files",
    "text": "Apache (web server) log files"
  },
  {
    "objectID": "slides/01-slides.html#system-log-files",
    "href": "slides/01-slides.html#system-log-files",
    "title": "Lecture 1",
    "section": "System log files",
    "text": "System log files"
  },
  {
    "objectID": "slides/01-slides.html#internet-of-things-iot-in-2025",
    "href": "slides/01-slides.html#internet-of-things-iot-in-2025",
    "title": "Lecture 1",
    "section": "Internet of Things (IoT) in 2025",
    "text": "Internet of Things (IoT) in 2025\n75 billion connected devices generating data: * Smart home devices (Alexa, Google Home, Apple HomePod) * Wearables (Apple Watch, Fitbit, Oura rings) * Connected vehicles & autonomous driving systems * Industrial IoT sensors * Smart city infrastructure * Medical devices & remote patient monitoring"
  },
  {
    "objectID": "slides/01-slides.html#smartphones-collecting-our-information",
    "href": "slides/01-slides.html#smartphones-collecting-our-information",
    "title": "Lecture 1",
    "section": "Smartphones collecting our information",
    "text": "Smartphones collecting our information"
  },
  {
    "objectID": "slides/01-slides.html#where-else",
    "href": "slides/01-slides.html#where-else",
    "title": "Lecture 1",
    "section": "Where else?",
    "text": "Where else?\n\nThe Internet\nTransactions\nDatabases\nExcel\nPDF Files\nAnything digital (music, movies, apps)\nSome old floppy disk lying around the house"
  },
  {
    "objectID": "slides/01-slides.html#typical-real-world-scenarios-2026",
    "href": "slides/01-slides.html#typical-real-world-scenarios-2026",
    "title": "Lecture 1",
    "section": "Typical real world scenarios (2026)",
    "text": "Typical real world scenarios (2026)\nScenario 1: Traditional Big Data\nYou have a laptop with 16GB of RAM and a 256GB SSD. You are given a 1TB dataset in text files. What do you do?\nScenario 2: AI/ML Pipeline\nYour company wants to build a RAG system using 10TB of internal documents. You need sub-second query response times. How do you architect this?\nScenario 3: Real-time Analytics\nYou need to process 1 million events per second from IoT devices and provide real-time dashboards with &lt;1 second latency. What’s your stack?"
  },
  {
    "objectID": "slides/01-slides.html#lets-discuss",
    "href": "slides/01-slides.html#lets-discuss",
    "title": "Lecture 1",
    "section": "Let’s discuss!",
    "text": "Let’s discuss!\n\n\n\nExponential data growth"
  },
  {
    "objectID": "slides/01-slides.html#big-data-definitions",
    "href": "slides/01-slides.html#big-data-definitions",
    "title": "Lecture 1",
    "section": "Big Data Definitions",
    "text": "Big Data Definitions\nWikipedia\n“In essence, is a term for a collection of datasets so large and complex that it becomes difficult to process using traditional tools and applications. Big Data technologies describe a new generation of technologies and architectures designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discover and/or analysis”\nO’Reilly\n“Big data is when the size of the data itself becomes part of the problem”\nEMC/IDC\n“Big data technologies describe a new generation of technologies and architectures, designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discovery, and/or analysis.”"
  },
  {
    "objectID": "slides/01-slides.html#frameworks-for-thinking-about-big-data",
    "href": "slides/01-slides.html#frameworks-for-thinking-about-big-data",
    "title": "Lecture 1",
    "section": "Frameworks for thinking about Big Data",
    "text": "Frameworks for thinking about Big Data\nIBM: (The famous 3-V’s definition)\n\nVolume (Gigabytes -&gt; Exabytes -&gt; Zettabytes)\nVelocity (Batch -&gt; Streaming -&gt; Real-time AI inference)\nVariety (Structured, Semi-structured, Unstructured, Embeddings)\n\nAdditional V’s for 2025\n\nVariability\nVeracity\nVisualization\nValue\nVectors (embeddings for AI/ML)\nVersatility (multi-modal data)"
  },
  {
    "objectID": "slides/01-slides.html#think-of-data-size-as-a-function-of-processing-and-storage",
    "href": "slides/01-slides.html#think-of-data-size-as-a-function-of-processing-and-storage",
    "title": "Lecture 1",
    "section": "Think of data size as a function of processing and storage",
    "text": "Think of data size as a function of processing and storage\n\nCan you analyze/process your data on a single machine?\nCan you store (or is it stored) on a single machine?\nCan you serve it fast enough for real-time AI applications?\n\nIf any of of the answers is no then you have a big-ish data problem!"
  },
  {
    "objectID": "slides/01-slides.html#the-new-data-landscape-2025",
    "href": "slides/01-slides.html#the-new-data-landscape-2025",
    "title": "Lecture 1",
    "section": "The New Data Landscape (2025)",
    "text": "The New Data Landscape (2025)\nTraining Foundation Models\n\nGPT-4: Trained on ~13 trillion tokens\nLlama 3: 15 trillion tokens\nGoogle’s Gemini: Multi-modal training on text, images, video\nEach iteration requires petabytes of curated data\n\nData Requirements Have Exploded\n\n2020: BERT trained on 3.3 billion words\n2023: GPT-4 trained on ~13 trillion tokens\n2024: Llama 3 trained on 15+ trillion tokens\nFuture: Approaching the limits of human-generated text"
  },
  {
    "objectID": "slides/01-slides.html#big-data-infrastructure-for-ai",
    "href": "slides/01-slides.html#big-data-infrastructure-for-ai",
    "title": "Lecture 1",
    "section": "Big Data Infrastructure for AI",
    "text": "Big Data Infrastructure for AI\nData Lakes & Warehouses for AI\nTraditional Use Cases: * Business intelligence * Analytics & reporting * Historical data storage\nModern AI Use Cases: * Training data repositories * Vector embeddings storage * RAG (Retrieval-Augmented Generation) context * Fine-tuning datasets * Evaluation & benchmark data"
  },
  {
    "objectID": "slides/01-slides.html#rag-context-engineering",
    "href": "slides/01-slides.html#rag-context-engineering",
    "title": "Lecture 1",
    "section": "RAG & Context Engineering",
    "text": "RAG & Context Engineering\nThe New Data Pipeline\nRaw Data → Data Lake → Processing → Vector DB → LLM Context\nKey Components: * Data Lakes (S3, Azure Data Lake): Store massive unstructured data * Data Warehouses (Snowflake, BigQuery): Structured data for context * Vector Databases (Pinecone, Weaviate, Qdrant): Semantic search * Embedding Models: Convert data to vectors * Orchestration (Airflow, Prefect): Manage the pipeline"
  },
  {
    "objectID": "slides/01-slides.html#mcp-servers-agentic-ai",
    "href": "slides/01-slides.html#mcp-servers-agentic-ai",
    "title": "Lecture 1",
    "section": "MCP Servers & Agentic AI",
    "text": "MCP Servers & Agentic AI\nModel Context Protocol (MCP)\nWhat is MCP? * Open protocol for connecting AI assistants to data sources * Standardized way to expose tools and data to LLMs * Enables “agentic” behavior - AI that can act autonomously\nMCP in Production\nData Warehouse → MCP Server → AI Agent → Action\nExamples: * AI agents querying Snowflake for real-time analytics * Autonomous systems updating data lakes based on predictions * Multi-agent systems coordinating through shared data contexts"
  },
  {
    "objectID": "slides/01-slides.html#data-quality-for-ai",
    "href": "slides/01-slides.html#data-quality-for-ai",
    "title": "Lecture 1",
    "section": "Data Quality for AI",
    "text": "Data Quality for AI\nWhy Data Quality Matters More Than Ever\nGarbage In, Garbage Out - Amplified: * Bad training data → Biased models * Incorrect RAG data → Hallucinations * Poor data governance → Compliance issues\nData Quality Challenges in 2025\n\nScale: Validating trillions of tokens\nDiversity: Multi-modal, multi-lingual data\nVelocity: Real-time data for online learning\nVeracity: Detecting AI-generated synthetic data"
  },
  {
    "objectID": "slides/01-slides.html#real-world-big-data-ai-examples",
    "href": "slides/01-slides.html#real-world-big-data-ai-examples",
    "title": "Lecture 1",
    "section": "Real-World Big Data + AI Examples",
    "text": "Real-World Big Data + AI Examples\nNetflix\n\nData Scale: 260+ million subscribers generating 100+ billion events/day\nAI Use: Personalization, content recommendations, thumbnail generation\nStack: S3 → Spark → Iceberg → ML models → Real-time serving\n\nUber\n\nData Scale: 35+ million trips per day, petabytes of location data\nAI Use: ETA prediction, surge pricing, driver-rider matching\nStack: Kafka → Spark Streaming → Feature Store → ML Platform\n\nOpenAI\n\nData Scale: Trillions of tokens for training, millions of queries/day\nAI Use: GPT models, DALL-E, embeddings\nStack: Distributed training → Vector DBs → Inference clusters"
  },
  {
    "objectID": "slides/01-slides.html#the-future-big-data-ai-convergence",
    "href": "slides/01-slides.html#the-future-big-data-ai-convergence",
    "title": "Lecture 1",
    "section": "The Future: Big Data + AI Convergence",
    "text": "The Future: Big Data + AI Convergence\nEmerging Trends (2025-2027)\nUnified Platforms: * Data lakes becoming “AI lakes” * Integrated vector + relational databases * One-stop shops for data + AI (Databricks, Snowflake Cortex)\nEdge Computing + AI: * Processing at the data source * Federated learning across devices * 5G enabling real-time edge AI\nSynthetic Data: * AI generating training data for AI * Privacy-preserving synthetic datasets * Infinite data generation loops"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes",
    "href": "slides/01-slides.html#relative-data-sizes",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-1",
    "href": "slides/01-slides.html#relative-data-sizes-1",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-2",
    "href": "slides/01-slides.html#relative-data-sizes-2",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-3",
    "href": "slides/01-slides.html#relative-data-sizes-3",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-4",
    "href": "slides/01-slides.html#relative-data-sizes-4",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#relative-data-sizes-5",
    "href": "slides/01-slides.html#relative-data-sizes-5",
    "title": "Lecture 1",
    "section": "Relative data sizes",
    "text": "Relative data sizes"
  },
  {
    "objectID": "slides/01-slides.html#what-youll-learn-in-this-course",
    "href": "slides/01-slides.html#what-youll-learn-in-this-course",
    "title": "Lecture 1",
    "section": "What You’ll Learn in This Course",
    "text": "What You’ll Learn in This Course\nBig Data-driven Analytics\nQuery Engines: * DuckDB - In-process analytical database * Polars - Lightning-fast DataFrame library\n* Spark - Distributed processing at scale\nData Warehouses & Lakes: * Snowflake - Cloud-native data warehouse * Athena - Serverless SQL on S3\nOrchestration: * Serverless with AWS Lambda * Airflow for pipeline management (?)"
  },
  {
    "objectID": "slides/01-slides.html#data-types",
    "href": "slides/01-slides.html#data-types",
    "title": "Lecture 1",
    "section": "Data Types",
    "text": "Data Types\n\nStructured\nUnstructured\nNatural language\nMachine-generated\nGraph-based\nAudio, video, and images\nStreaming"
  },
  {
    "objectID": "slides/01-slides.html#big-data-vs.-small-data",
    "href": "slides/01-slides.html#big-data-vs.-small-data",
    "title": "Lecture 1",
    "section": "Big Data vs. Small Data",
    "text": "Big Data vs. Small Data\n\n\n\n\n\n\n\n\n\nSmall Data is usually…\nOn the other hand, Big Data…\n\n\n\n\nGoals\ngathered for a specific goal\nmay have a goal in mind when it’s first started, but things can evolve or take unexpected directions\n\n\nLocation\nin one place, and often in a single computer file\ncan be in multiple files in multiple servers on computers in different geographic locations\n\n\nStructure/Contents\nhighly structured like an Excel spreadsheet, and it’s got rows and columns of data\ncan be unstructured, it can have many formats in files involved across disciplines, and may link to other resources\n\n\nPreparation\nprepared by the end user for their own purposes\nis often prepared by one group of people, analyzed by a second group of people, and then used by a third group of people, and they may have different purposes, and they may have different disciplines"
  },
  {
    "objectID": "slides/01-slides.html#big-data-vs.-small-data-1",
    "href": "slides/01-slides.html#big-data-vs.-small-data-1",
    "title": "Lecture 1",
    "section": "Big Data vs. Small Data",
    "text": "Big Data vs. Small Data\n\n\n\n\n\n\n\n\n\nSmall Data is usually…\nOn the other hand, Big Data…\n\n\n\n\nLongevity\nkept for a specific amount of time after the project is over because there’s a clear ending point. In the academic world it’s maybe five or seven years and then you can throw it away\ncontains data that must be stored in perpetuity. Many big data projects extend into the past and future\n\n\nMeasurements\nmeasured with a single protocol using set units and it’s usually done at the same time\nis collected and measured using many sources, protocols, units, etc\n\n\nReproducibility\nbe reproduced in their entirety if something goes wrong in the process\nreplication is seldom feasible\n\n\nStakes\nif things go wrong the costs are limited, it’s not an enormous problem\ncan have high costs of failure in terms of money, time and labor\n\n\nAccess\nidentified by a location specified in a row/column\nunless it is exceptionally well designed, the organization can be inscrutable\n\n\nAnalysis\nanalyzed together, all at once\nis ordinarily analyzed in incremental steps"
  },
  {
    "objectID": "slides/01-slides.html#traditional-data-analysis-tools-like-r-and-python-are-single-threaded",
    "href": "slides/01-slides.html#traditional-data-analysis-tools-like-r-and-python-are-single-threaded",
    "title": "Lecture 1",
    "section": "Traditional data analysis tools like R and Python are single threaded",
    "text": "Traditional data analysis tools like R and Python are single threaded"
  },
  {
    "objectID": "slides/01-slides.html#tools-at-a-glance",
    "href": "slides/01-slides.html#tools-at-a-glance",
    "title": "Lecture 1",
    "section": "Tools at-a-glance",
    "text": "Tools at-a-glance\n\n\nLanguages, libraries, and projects\n\nPython\n\npandas\npolars\nPySpark\nduckdb\ndask\nray\n\nApache Arrow\nApache Spark\nSQL\n\nWe’ll talk briefly about Apache Hadoop today but we will not cover it in this course.\n\nCloud Services\n\nAmazon Web Services (AWS)\n\nAWS Sagemaker\nAmazon S3\n\nAzure\n\nAzure Blob\nAzure Machine Learning\n\n\nOther:\n\nAWS Elastic MapReduce (EMR)"
  },
  {
    "objectID": "slides/01-slides.html#additional-links-of-interest",
    "href": "slides/01-slides.html#additional-links-of-interest",
    "title": "Lecture 1",
    "section": "Additional links of interest",
    "text": "Additional links of interest\n\nMatt Turck’s Machine Learning, Artificial Intelligence & Data Landscape (MAD)\n\nArticle\nInteractive Landscape\n\nIs there life after Hadoop?\n10 Best Big Data Tools for 2023"
  },
  {
    "objectID": "slides/01-slides.html#difference-between-data-scientist-and-data-engineer",
    "href": "slides/01-slides.html#difference-between-data-scientist-and-data-engineer",
    "title": "Lecture 1",
    "section": "Difference between Data Scientist and Data Engineer",
    "text": "Difference between Data Scientist and Data Engineer\nIn this course, you’ll be doing a little data engineering!"
  },
  {
    "objectID": "slides/01-slides.html#responsibilities",
    "href": "slides/01-slides.html#responsibilities",
    "title": "Lecture 1",
    "section": "Responsibilities",
    "text": "Responsibilities"
  },
  {
    "objectID": "slides/01-slides.html#data-engineering-falls-into-levels-2-and-3-primarily",
    "href": "slides/01-slides.html#data-engineering-falls-into-levels-2-and-3-primarily",
    "title": "Lecture 1",
    "section": "Data Engineering falls into levels 2 and 3 primarily",
    "text": "Data Engineering falls into levels 2 and 3 primarily"
  },
  {
    "objectID": "slides/01-slides.html#as-an-analystdata-scientist-you-really-need-both",
    "href": "slides/01-slides.html#as-an-analystdata-scientist-you-really-need-both",
    "title": "Lecture 1",
    "section": "As an analyst/data scientist, you really need both",
    "text": "As an analyst/data scientist, you really need both"
  },
  {
    "objectID": "slides/01-slides.html#architecture",
    "href": "slides/01-slides.html#architecture",
    "title": "Lecture 1",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "slides/01-slides.html#storage",
    "href": "slides/01-slides.html#storage",
    "title": "Lecture 1",
    "section": "Storage",
    "text": "Storage"
  },
  {
    "objectID": "slides/01-slides.html#source-control",
    "href": "slides/01-slides.html#source-control",
    "title": "Lecture 1",
    "section": "Source control",
    "text": "Source control"
  },
  {
    "objectID": "slides/01-slides.html#orchestration",
    "href": "slides/01-slides.html#orchestration",
    "title": "Lecture 1",
    "section": "Orchestration",
    "text": "Orchestration"
  },
  {
    "objectID": "slides/01-slides.html#processing",
    "href": "slides/01-slides.html#processing",
    "title": "Lecture 1",
    "section": "Processing",
    "text": "Processing"
  },
  {
    "objectID": "slides/01-slides.html#analytics",
    "href": "slides/01-slides.html#analytics",
    "title": "Lecture 1",
    "section": "Analytics",
    "text": "Analytics"
  },
  {
    "objectID": "slides/01-slides.html#machine-learning",
    "href": "slides/01-slides.html#machine-learning",
    "title": "Lecture 1",
    "section": "Machine Learning",
    "text": "Machine Learning"
  },
  {
    "objectID": "slides/01-slides.html#governance",
    "href": "slides/01-slides.html#governance",
    "title": "Lecture 1",
    "section": "Governance",
    "text": "Governance"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-1",
    "href": "slides/01-slides.html#linux-command-line-1",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nTerminal\n\n\nTerminal access was THE ONLY way to do programming\nNo GUIs! No Spyder, Jupyter, RStudio, etc.\nCoding is still more powerful than graphical interfaces for complex jobs\nCoding makes work repeatable"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-2",
    "href": "slides/01-slides.html#linux-command-line-2",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nBASH\n\n\nCreated in 1989 by Brian Fox\nBrian Fox also built the first online interactive banking software\nBASH is a command processor\nConnection between you and the machine language and hardware"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-3",
    "href": "slides/01-slides.html#linux-command-line-3",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nThe Prompt\nusername@hostname:current_directory $\nWhat do we learn from the prompt?\n\nWho you are - username\nThe machine where your code is running - hostname\nThe directory where your code is running - current_directory\nThe shell type - $ - this symbol means BASH"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-4",
    "href": "slides/01-slides.html#linux-command-line-4",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nSyntax\nCOMMAND -F --FLAG * COMMAND is the program * Everything after that are arguments * F is a single letter flag * FLAG is a single word or words connected by dashes flag. A space breaks things into a new argument. + Sometimes single letter and long form flags (e.g. F and FLAG) can refer to the same argument\nCOMMAND -F --FILE file1\nHere we pass an text argument “file1” into the FILE flag\nThe -h flag is usually to get help. You can also run the man command and pass the name of the program as the argument to get the help page.\nLet’s try basic commands:\n\ndate to get the current date\nwhoami to get your user name\necho \"Hello World\" to print to the console"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-5",
    "href": "slides/01-slides.html#linux-command-line-5",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nExamining Files\nFind out your Present Working Directory pwd\nExamine the contents of files and folders using the ls command\nMake new files from scratch using the touch command\nGlobbing - how to select files in a general way\n\n\\* for wild card any number of characters\n\\? for wild card for a single character\n[] for one of many character options\n! for exclusion\nspecial options [:alpha:], [:alnum:], [:digit:], [:lower:], [:upper:]\n\nReference material Reference material: Shell Lesson 1,2,4,5"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-6",
    "href": "slides/01-slides.html#linux-command-line-6",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nNavigating Directories\nKnowing where your terminal is executing code ensures you are working with the right inputs and making the right outputs.\nUse the command pwd to determine the Present Working Directory.\nLet’s say you need to change to a folder called “git-repo”. To change directories you can use a command like cd git-repo.\n\n. refers to the current directory, such as ./git-repo\n.. can be used to move up one folder, use cd .., and can be combined to move up multiple levels ../../my_folder\n/ is the root of the Linux OS, where there are core folders, such as system, users, etc.\n~ is the home directory. Move to folders referenced relative to this path by including it at the start of your path, for example ~/projects.\n\nTo view the structure of directories from your present working directory, use the tree command\nReference link"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-7",
    "href": "slides/01-slides.html#linux-command-line-7",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nInteracting with Files\nNow that we know how to navigate through directories, we need to learn the commands for interacting with files\n\nmv to move files from one location to another\n\nCan use file globbing here - ?, *, [], …\n\ncp to copy files instead of moving\n\nCan use file globbing here - ?, *, [], …\n\nmkdir to make a directory\nrm to remove files\nrmdir to remove directories\nrm -rf to blast everything! WARNING!!! DO NOT USE UNLESS YOU KNOW WHAT YOU ARE DOING"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-8",
    "href": "slides/01-slides.html#linux-command-line-8",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nUsing BASH for Data Exploration\nCommands:\n\nhead FILENAME / tail FILENAME - glimpsing the first / last few rows of data\nmore FILENAME / less FILENAME - viewing the data with basic up / (up & down) controls\ncat FILENAME - print entire file contents into terminal\nvim FILENAME - open (or edit!) the file in vim editor\ngrep FILENAME - search for lines within a file that match a regex expression\nwc FILENAME - count the number of lines (-l flag) or number of words (-w flag)\n\nReference link Reference material: Text Lesson 8,9,15,16"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-9",
    "href": "slides/01-slides.html#linux-command-line-9",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nPipes and Arrows\n\n| sends the stdout to another command (is the most powerful symbol in BASH!)\n&gt; sends stdout to a file and overwrites anything that was there before\n&gt;&gt; appends the stdout to the end of a file (or starts a new file from scratch if one does not exist yet)\n&lt; sends stdin into the command on the left\n\nTo-dos:\n\necho Hello World\nCounting rows of data with certain attributes\n\nReference material: Text Lesson 1,2,3,4,5"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-10",
    "href": "slides/01-slides.html#linux-command-line-10",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nAlias and User Files\n.bashrc is where your shell settings are located\nIf we wanted a shortcut to find out the number of our running processes, we would write a commmand like whoami | xargs ps -u | wc -l.\nWe don’t want to write out this full command every time! Let’s make an alias.\nalias alias_name=\"command_to_run\"\nalias nproc=\"whoami | xargs ps -u | wc -l\"\nNow we need to put this alias into the .bashrc\nalias nproc=\"whoami | xargs ps -u | wc -l\" &gt;&gt; ~/.bashrc\nWhat happened??\necho alias nproc=\"whoami | xargs ps -u | wc -l\" &gt;&gt; ~/.bashrc\nYour commands get saved in ~/.bash_history"
  },
  {
    "objectID": "slides/01-slides.html#linux-command-line-11",
    "href": "slides/01-slides.html#linux-command-line-11",
    "title": "Lecture 1",
    "section": "Linux Command Line",
    "text": "Linux Command Line\nProcess Managment\nUse the command ps to see your running processes.\nUse the command top or even better htop to see all the running processes on the machine.\nInstall the program htop using the command sudo yum install htop -y\nFind the process ID (PID) so you can kill a broken process.\nUse the command kill [PID NUM] to signal the process to terminate. If things get really bad, then use the command kill -9 [PID NUM]\nTo kill a command in the terminal window it is running in, try using Ctrl + C or Ctrl + /\nRun the cat command on its own to let it stay open. Now open a new terminal to examine the processes and find the cat process.\nReference material: Text Lesson 1,2,3,7,9,10\nTry playing a Linux game!\nhttps://gitlab.com/slackermedia/bashcrawl is a game to help you practice your navigation and file access skills. Click on the binder link in this repo to launch a jupyter lab session and explore!"
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "DATS 6450.13",
    "section": "",
    "text": "Course overview, big data concepts, cloud computing and evolution of cloud technologies.",
    "crumbs": [
      "Home",
      "Course content",
      "1. Course overview"
    ]
  },
  {
    "objectID": "content/01-content.html#about-todays-class",
    "href": "content/01-content.html#about-todays-class",
    "title": "DATS 6450.13",
    "section": "",
    "text": "Course overview, big data concepts, cloud computing and evolution of cloud technologies.",
    "crumbs": [
      "Home",
      "Course content",
      "1. Course overview"
    ]
  },
  {
    "objectID": "content/01-content.html#readings",
    "href": "content/01-content.html#readings",
    "title": "DATS 6450.13",
    "section": "Readings",
    "text": "Readings\nNo prescribed readings for this lecture",
    "crumbs": [
      "Home",
      "Course content",
      "1. Course overview"
    ]
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "DATS 6450.13",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. You can also click in the slides below and navigate through them with your left and right arrow keys.",
    "crumbs": [
      "Home",
      "Course content",
      "1. Course overview"
    ]
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Lectures and labs",
    "section": "",
    "text": "Each class is divided into two parts: a lecture followed by a lab. Some class optionally have a reading component also that needs to be completed before class.\n\nLecture\nDuring the lecture, I will be going over the slides for that lecture, this would usually include a quick review of the previous class, feedback on a recently graded assignment and then cover the topic of the day.\nThe lecture would usually last about 90 minutes, followed by 5 to 10 minutes break.\n\n\nLab\nThe lab for the class would involve a hands-on coding assignment provided through GitHub Classroom. You will start the lab in-class, myself and the TAs would be helping you with any questions with the lab and then you would need to turn in the lab by checking in your code and results in the GitHub repo (you will have until next class for this, but usually you would be able to do this much sooner)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Big Data Analytics",
    "section": "",
    "text": "NoteCourse Information\n\n\n\nCourse Code: DATS 6450.13\nCredits: 3\nSchedule: Thursdays 6:10 PM - 8:40 PM\nLocation: Phillips 416"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Applied Big Data Analytics",
    "section": "Course Description",
    "text": "Course Description\nApplied Big Data Analytics (DATS 6450.13) teaches practical, hands‑on methods for building scalable analytics pipelines that start on a single machine and scale to distributed clusters. Students learn to develop local analytical workflows with DuckDB, translate and scale them with Apache Spark, and evaluate performance tradeoffs through comparative benchmarking, query tuning, partitioning and memory strategies. The course covers modern tooling (Polars, Ray, RAPIDS), Spark SQL/DataFrame APIs, Spark NLP and MLlib, and efficient visualization of very large datasets (Datashader), with emphasis on reproducible end‑to‑end workflows and a final project demonstrating design and performance decisions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Abhijit Das Gupta",
    "section": "",
    "text": "Abhijit Das Gupta is a Data Science Director at AstraZeneca. He works at the interface of statistics and machine learning, with broad experience in statistical modeling and machine learning, bioinformatics, optimization, signal processing, and data visualization. He holds a PhD in Biostatistics with earlier training in Mathematical Statistics. He has been using R for over 25 years for a wide variety of applications in biomedical research, as well as for data-driven reports and presentations.\nAbhijit is adjunct faculty at George Washington University, where he currently teaches courses in Applied Big Data Analysis . He is also adjunct faculty at George Mason University and Georgetown University. He is a certified Carpentries instructor. He co-founded and organized the Statistical Programming DC meetup (formerly R Users Group DC) for several years and got to know several luminaries in the field, including DJ Patil, Hadley Wickham, Max Kuhn, Frank Harrell and others.\nOutside of work, Abhijit is an aikido instructor holding a 5th degree black belt, having practiced the art since 1994. He teaches in Frederick County, MD. He is also active in community theater productions with Ebong Theatrix, a Bengali theater group in the DC area.\nAbhijit lives in Germantown, MD with his wife, children and golden retriever Jingle."
  }
]